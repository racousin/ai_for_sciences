{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Optimization of Machine Learning Problems\n",
    "\n",
    "**Day 1 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day1/tp1.ipynb)\n",
    "\n",
    "## Objectives\n",
    "1. Understand gradient descent intuitively\n",
    "2. Apply gradient descent to linear regression with PyTorch\n",
    "3. Compare CPU vs GPU training performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to install and import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the aiforscience package from GitHub\n",
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from aiforscience import (\n",
    "    plot_gradient_descent_1d,\n",
    "    plot_loss_history,\n",
    "    plot_predictions,\n",
    "    plot_gradient_step,\n",
    "    print_model_params,\n",
    "    print_training_step,\n",
    "    print_gradient_info,\n",
    "    print_device_comparison,\n",
    "    generate_linear_data,\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Understanding Gradient Descent\n",
    "\n",
    "**Goal:** Find the value of $\\theta$ that minimizes a function $f(\\theta)$.\n",
    "\n",
    "## The Key Idea\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm:\n",
    "\n",
    "$$\\theta_{new} = \\theta_{old} - \\eta \\cdot \\nabla f(\\theta_{old})$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ is the parameter we want to optimize\n",
    "- $\\eta$ (eta) is the **learning rate** (step size)\n",
    "- $\\nabla f(\\theta)$ is the **gradient** (derivative) of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Function\n",
    "\n",
    "Let's minimize $f(\\theta) = (3\\theta - 7)^2$\n",
    "\n",
    "**Question:** What is the analytical minimum of this function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function and its gradient\n",
    "def f(theta):\n",
    "    \"\"\"Function to minimize: (3*theta - 7)^2\"\"\"\n",
    "    return (3 * theta - 7) ** 2\n",
    "\n",
    "def gradient_f(theta):\n",
    "    \"\"\"Gradient of f: d/d_theta [(3*theta - 7)^2] = 2*(3*theta - 7)*3 = 6*(3*theta - 7)\"\"\"\n",
    "    return 6 * (3 * theta - 7)\n",
    "\n",
    "# Visualize the function\n",
    "theta_values = np.linspace(-2, 5, 100)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(theta_values, [f(t) for t in theta_values], 'b-', linewidth=2)\n",
    "plt.xlabel('θ')\n",
    "plt.ylabel('f(θ)')\n",
    "plt.title('f(θ) = (3θ - 7)²')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Analytical minimum: θ = 7/3 ≈ {7/3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Step by Step\n",
    "\n",
    "Let's watch one gradient descent step in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start at theta = 0\n",
    "theta = 0.0\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Compute gradient at current position\n",
    "grad = gradient_f(theta)\n",
    "print(f\"Current θ = {theta}\")\n",
    "print(f\"Current f(θ) = {f(theta)}\")\n",
    "print(f\"Gradient at θ: {grad}\")\n",
    "print(f\"\\nUpdate: θ_new = θ - lr × gradient\")\n",
    "print(f\"        θ_new = {theta} - {learning_rate} × {grad}\")\n",
    "print(f\"        θ_new = {theta - learning_rate * grad}\")\n",
    "\n",
    "# Perform update\n",
    "theta_new = theta - learning_rate * grad\n",
    "\n",
    "# Visualize the step\n",
    "plot_gradient_step(theta, theta_new, grad, learning_rate, f, theta_range=(-2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Gradient Descent\n",
    "\n",
    "Now let's run multiple iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, gradient_f, theta_init, learning_rate, n_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent optimization.\n",
    "    \n",
    "    Args:\n",
    "        f: Function to minimize\n",
    "        gradient_f: Gradient of f\n",
    "        theta_init: Starting value\n",
    "        learning_rate: Step size\n",
    "        n_iterations: Number of steps\n",
    "    \n",
    "    Returns:\n",
    "        theta_history: List of theta values at each step\n",
    "    \"\"\"\n",
    "    theta = theta_init\n",
    "    theta_history = [theta]\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        grad = gradient_f(theta)\n",
    "        theta = theta - learning_rate * grad\n",
    "        theta_history.append(theta)\n",
    "        \n",
    "    return theta_history\n",
    "\n",
    "# Run gradient descent\n",
    "theta_history = gradient_descent(\n",
    "    f=f,\n",
    "    gradient_f=gradient_f,\n",
    "    theta_init=0.0,\n",
    "    learning_rate=0.05,\n",
    "    n_iterations=20\n",
    ")\n",
    "\n",
    "print(f\"Starting θ: {theta_history[0]:.4f}\")\n",
    "print(f\"Final θ: {theta_history[-1]:.4f}\")\n",
    "print(f\"True minimum: {7/3:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plot_gradient_descent_1d(f, theta_history, theta_range=(-2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Experiment with Learning Rate\n",
    "\n",
    "Try different learning rates and observe the behavior:\n",
    "- `learning_rate = 0.01` (small)\n",
    "- `learning_rate = 0.05` (medium)\n",
    "- `learning_rate = 0.15` (large)\n",
    "- `learning_rate = 0.35` (too large?)\n",
    "\n",
    "**Questions:**\n",
    "1. What happens with a very small learning rate?\n",
    "2. What happens with a very large learning rate?\n",
    "3. What is a good learning rate for this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different learning rates\n",
    "learning_rate = 0.05  # <-- Modify this value!\n",
    "\n",
    "theta_history = gradient_descent(\n",
    "    f=f,\n",
    "    gradient_f=gradient_f,\n",
    "    theta_init=0.0,\n",
    "    learning_rate=learning_rate,\n",
    "    n_iterations=20\n",
    ")\n",
    "\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Final θ: {theta_history[-1]:.4f} (target: {7/3:.4f})\")\n",
    "plot_gradient_descent_1d(f, theta_history, theta_range=(-2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Try a Different Function\n",
    "\n",
    "Modify the function to minimize. Try: $f(\\theta) = \\theta^2 + 5\\theta + 6$\n",
    "\n",
    "**Hint:** The gradient is $\\nabla f(\\theta) = 2\\theta + 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a new function and its gradient\n",
    "def f2(theta):\n",
    "    return theta**2 + 5*theta + 6  # Modify this!\n",
    "\n",
    "def gradient_f2(theta):\n",
    "    return 2*theta + 5  # Modify this!\n",
    "\n",
    "# What's the analytical minimum? (hint: set gradient to 0)\n",
    "analytical_min = -5/2  # theta where gradient = 0\n",
    "print(f\"Analytical minimum: θ = {analytical_min}\")\n",
    "\n",
    "# Run gradient descent\n",
    "theta_history = gradient_descent(\n",
    "    f=f2,\n",
    "    gradient_f=gradient_f2,\n",
    "    theta_init=5.0,\n",
    "    learning_rate=0.1,\n",
    "    n_iterations=30\n",
    ")\n",
    "\n",
    "print(f\"Final θ: {theta_history[-1]:.4f}\")\n",
    "plot_gradient_descent_1d(f2, theta_history, theta_range=(-6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Linear Regression with PyTorch\n",
    "\n",
    "Now we apply gradient descent to a real machine learning problem: **linear regression**.\n",
    "\n",
    "**Goal:** Find weights $w$ and bias $b$ such that $\\hat{y} = Xw + b$ minimizes the Mean Squared Error (MSE):\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y, true_weights, true_bias = generate_linear_data(\n",
    "    n_samples=100,\n",
    "    n_features=1,\n",
    "    noise=0.5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.6)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Generated Data for Linear Regression')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple linear model: y = Xw + b\n",
    "model = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "# Show initial (random) parameters\n",
    "print_model_params(model, \"Initial Model Parameters\")\n",
    "\n",
    "# Make initial predictions\n",
    "with torch.no_grad():\n",
    "    y_pred_initial = model(X)\n",
    "    initial_loss = nn.MSELoss()(y_pred_initial, y)\n",
    "\n",
    "print(f\"Initial Loss (MSE): {initial_loss.item():.4f}\")\n",
    "\n",
    "# Visualize initial predictions\n",
    "plot_predictions(X.numpy(), y.numpy(), y_pred_initial.numpy(), \n",
    "                title=\"Initial Predictions (Before Training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model\n",
    "model = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "# Loss function: Mean Squared Error\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optimizer: Stochastic Gradient Descent\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Loss function: MSE\")\n",
    "print(f\"Optimizer: SGD with lr={learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step - Detailed View\n",
    "\n",
    "Let's look at ONE training step in detail to understand what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\" DETAILED TRAINING STEP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 0: Show current parameters\n",
    "print(\"\\n[BEFORE] Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.data.numpy().flatten()}\")\n",
    "\n",
    "# Step 1: Forward pass - compute predictions\n",
    "print(\"\\n[STEP 1] Forward pass: y_pred = model(X)\")\n",
    "y_pred = model(X)\n",
    "print(f\"  Predictions shape: {y_pred.shape}\")\n",
    "\n",
    "# Step 2: Compute loss\n",
    "print(\"\\n[STEP 2] Compute loss: loss = MSE(y_pred, y)\")\n",
    "loss = loss_fn(y_pred, y)\n",
    "print(f\"  Loss value: {loss.item():.6f}\")\n",
    "\n",
    "# Step 3: Zero gradients (important!)\n",
    "print(\"\\n[STEP 3] Zero gradients: optimizer.zero_grad()\")\n",
    "optimizer.zero_grad()\n",
    "print(\"  Gradients reset to zero\")\n",
    "\n",
    "# Step 4: Backward pass - compute gradients\n",
    "print(\"\\n[STEP 4] Backward pass: loss.backward()\")\n",
    "loss.backward()\n",
    "print(\"  Gradients computed:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"    {name}.grad: {param.grad.numpy().flatten()}\")\n",
    "\n",
    "# Step 5: Update parameters\n",
    "print(\"\\n[STEP 5] Update parameters: optimizer.step()\")\n",
    "print(f\"  Rule: param_new = param_old - lr * gradient\")\n",
    "optimizer.step()\n",
    "\n",
    "# Show updated parameters\n",
    "print(\"\\n[AFTER] Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.data.numpy().flatten()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model and optimizer\n",
    "model = nn.Linear(in_features=1, out_features=1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 100\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Print every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Loss = {loss.item():.6f}\")\n",
    "\n",
    "# Final results\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" TRAINING COMPLETE\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss:   {losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nLearned parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.data.numpy().flatten()}\")\n",
    "print(f\"\\nTrue parameters:\")\n",
    "print(f\"  weight: {true_weights.flatten()}\")\n",
    "print(f\"  bias: {true_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plot_loss_history(losses, title=\"Training Loss over Epochs\")\n",
    "\n",
    "# Visualize final predictions\n",
    "with torch.no_grad():\n",
    "    y_pred_final = model(X)\n",
    "plot_predictions(X.numpy(), y.numpy(), y_pred_final.numpy(), \n",
    "                title=\"Final Predictions (After Training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Experiment with Training Parameters\n",
    "\n",
    "Modify the code below to experiment with:\n",
    "1. **Learning rate:** Try `0.01`, `0.1`, `0.5`, `1.0`\n",
    "2. **Number of epochs:** Try `10`, `50`, `100`, `500`\n",
    "\n",
    "**Questions:**\n",
    "- What happens if the learning rate is too high?\n",
    "- How many epochs are needed to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify these parameters\n",
    "learning_rate = 0.1  # <-- Try different values!\n",
    "n_epochs = 100       # <-- Try different values!\n",
    "\n",
    "# Reset model\n",
    "model = nn.Linear(in_features=1, out_features=1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Learning rate: {learning_rate}, Epochs: {n_epochs}\")\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "plot_loss_history(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4: Modify the Dataset\n",
    "\n",
    "Try different dataset configurations:\n",
    "- More samples: `n_samples=500`\n",
    "- More noise: `noise=2.0`\n",
    "- More features: `n_features=3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify dataset parameters\n",
    "X_new, y_new, true_w, true_b = generate_linear_data(\n",
    "    n_samples=100,   # <-- Try 500\n",
    "    n_features=1,    # <-- Try 3\n",
    "    noise=0.5,       # <-- Try 2.0\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create and train model\n",
    "n_features = X_new.shape[1]\n",
    "model = nn.Linear(in_features=n_features, out_features=1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(X_new)\n",
    "    loss = loss_fn(y_pred, y_new)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "plot_loss_history(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 5: Build a Neural Network\n",
    "\n",
    "Let's add more layers! A neural network is just multiple linear layers with **non-linear activation functions** between them.\n",
    "\n",
    "Architecture:\n",
    "- Input layer: `n_features` neurons\n",
    "- Hidden layer 1: 16 neurons + ReLU activation\n",
    "- Hidden layer 2: 8 neurons + ReLU activation\n",
    "- Output layer: 1 neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_features, 16),  # Hidden layer 1\n",
    "            nn.ReLU(),                   # Activation\n",
    "            nn.Linear(16, 8),            # Hidden layer 2\n",
    "            nn.ReLU(),                   # Activation\n",
    "            nn.Linear(8, 1)              # Output layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Create model\n",
    "nn_model = SimpleNN(n_features=1)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in nn_model.parameters())\n",
    "print(f\"Neural Network Architecture:\")\n",
    "print(nn_model)\n",
    "print(f\"\\nTotal parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "nn_model = SimpleNN(n_features=1)\n",
    "optimizer = torch.optim.SGD(nn_model.parameters(), lr=0.01)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(500):\n",
    "    y_pred = nn_model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.6f}\")\n",
    "\n",
    "plot_loss_history(losses, title=\"Neural Network Training Loss\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_nn = nn_model(X)\n",
    "plot_predictions(X.numpy(), y.numpy(), y_pred_nn.numpy(), \n",
    "                title=\"Neural Network Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: CPU vs GPU Performance\n",
    "\n",
    "Deep learning benefits enormously from GPU acceleration. Let's compare training speed on CPU vs GPU.\n",
    "\n",
    "## Setting Up GPU in Colab\n",
    "\n",
    "To use GPU in Google Colab:\n",
    "1. Go to **Runtime** > **Change runtime type**\n",
    "2. Select **GPU** as the Hardware accelerator\n",
    "3. Click **Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available devices\n",
    "print(\"Device Information:\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Function\n",
    "\n",
    "We'll train the same model on CPU and GPU and compare the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model(X, y, device, n_epochs=1000, hidden_size=128, n_layers=4, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a neural network and measure time.\n",
    "    \n",
    "    Args:\n",
    "        X, y: Training data\n",
    "        device: 'cpu' or 'cuda'\n",
    "        n_epochs: Number of training epochs\n",
    "        hidden_size: Neurons per hidden layer\n",
    "        n_layers: Number of hidden layers\n",
    "        verbose: Print progress\n",
    "    \"\"\"\n",
    "    # Move data to device\n",
    "    X_dev = X.to(device)\n",
    "    y_dev = y.to(device)\n",
    "    \n",
    "    # Build model\n",
    "    layers = []\n",
    "    in_features = X.shape[1]\n",
    "    for i in range(n_layers):\n",
    "        layers.append(nn.Linear(in_features, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_features = hidden_size\n",
    "    layers.append(nn.Linear(hidden_size, 1))\n",
    "    \n",
    "    model = nn.Sequential(*layers).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # Count parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    if verbose:\n",
    "        print(f\"Model: {n_layers} hidden layers, {hidden_size} neurons each\")\n",
    "        print(f\"Total parameters: {n_params:,}\")\n",
    "        print(f\"Device: {device}\")\n",
    "    \n",
    "    # Warm-up (for GPU)\n",
    "    if device == 'cuda':\n",
    "        for _ in range(10):\n",
    "            y_pred = model(X_dev)\n",
    "            loss = loss_fn(y_pred, y_dev)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Timed training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = model(X_dev)\n",
    "        loss = loss_fn(y_pred, y_dev)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Ensure GPU operations are complete\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Training time: {elapsed_time:.4f} seconds\")\n",
    "        print(f\"Final loss: {loss.item():.6f}\")\n",
    "    \n",
    "    return elapsed_time, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate larger dataset for meaningful comparison\n",
    "X_large, y_large, _, _ = generate_linear_data(\n",
    "    n_samples=5000,\n",
    "    n_features=20,\n",
    "    noise=0.5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" TRAINING ON CPU\")\n",
    "print(\"=\"*50)\n",
    "cpu_time, cpu_loss = train_model(\n",
    "    X_large, y_large, \n",
    "    device='cpu', \n",
    "    n_epochs=500,\n",
    "    hidden_size=256,\n",
    "    n_layers=4\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" TRAINING ON GPU\")\n",
    "    print(\"=\"*50)\n",
    "    gpu_time, gpu_loss = train_model(\n",
    "        X_large, y_large, \n",
    "        device='cuda', \n",
    "        n_epochs=500,\n",
    "        hidden_size=256,\n",
    "        n_layers=4\n",
    "    )\n",
    "    \n",
    "    # Print comparison\n",
    "    print_device_comparison(cpu_time, gpu_time)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" GPU NOT AVAILABLE\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"To enable GPU in Colab:\")\n",
    "    print(\"  Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 6: Scale Up the Model\n",
    "\n",
    "Try increasing the model size to see bigger GPU speedups:\n",
    "- More data: `n_samples=10000`\n",
    "- More features: `n_features=50`\n",
    "- Bigger network: `hidden_size=512`, `n_layers=6`\n",
    "- More epochs: `n_epochs=1000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify these parameters to see larger speedups\n",
    "n_samples = 5000      # <-- Try 10000, 20000\n",
    "n_features = 20       # <-- Try 50, 100\n",
    "hidden_size = 256     # <-- Try 512, 1024\n",
    "n_layers = 4          # <-- Try 6, 8\n",
    "n_epochs = 500        # <-- Try 1000\n",
    "\n",
    "X_test, y_test, _, _ = generate_linear_data(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    noise=0.5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nCPU Training:\")\n",
    "cpu_time, _ = train_model(\n",
    "    X_test, y_test, 'cpu', n_epochs=n_epochs,\n",
    "    hidden_size=hidden_size, n_layers=n_layers\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nGPU Training:\")\n",
    "    gpu_time, _ = train_model(\n",
    "        X_test, y_test, 'cuda', n_epochs=n_epochs,\n",
    "        hidden_size=hidden_size, n_layers=n_layers\n",
    "    )\n",
    "    print_device_comparison(cpu_time, gpu_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this practical, you learned:\n",
    "\n",
    "1. **Gradient Descent**: The core optimization algorithm\n",
    "   - $\\theta_{new} = \\theta_{old} - \\eta \\cdot \\nabla f(\\theta)$\n",
    "   - Learning rate controls step size\n",
    "\n",
    "2. **PyTorch Training Loop**:\n",
    "   - Forward pass: `y_pred = model(X)`\n",
    "   - Compute loss: `loss = loss_fn(y_pred, y)`\n",
    "   - Zero gradients: `optimizer.zero_grad()`\n",
    "   - Backward pass: `loss.backward()`\n",
    "   - Update parameters: `optimizer.step()`\n",
    "\n",
    "3. **CPU vs GPU**: GPUs can dramatically accelerate deep learning training\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Learning rate** is crucial: too small = slow, too large = unstable\n",
    "- **Epochs** determine how long we train\n",
    "- **GPUs** are essential for large-scale deep learning\n",
    "- **Neural networks** are compositions of linear layers + activations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
