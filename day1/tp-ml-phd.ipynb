{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Practical: Movie Recommendation System\n",
    "\n",
    "**Author:** Nicolas Baskiotis (nicolas.baskiotis@sorbonne-universite.fr)  \n",
    "**Institution:** MLIA/ISIR, Sorbonne Université\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This practical aims to revisit fundamental Machine Learning concepts through a concrete application: the **movie recommendation problem**. \n",
    "The recommendation problem was popularized by the famous Netflix Challenge in the late 2000s. Typically, such problems involve:\n",
    "- A set of **users**\n",
    "- A set of **items** (movies in our case)\n",
    "- Information about them (user profiles, item features, user ratings on items)\n",
    "- **Goal:** Predict ratings of new users on items (or users on new items)\n",
    "\n",
    "Recommendation systems are used in numerous applications: e-commerce sites, social networks (suggesting friends, articles of interest), etc.\n",
    "\n",
    "We will work with data from the **MovieLens project** (https://movielens.org/): recommendation data on users and movies.\n",
    "\n",
    "In this practical, we will consider a simpler problem: predicting the average rating of a movie across all users. This will allow us to review the standard methodology of a machine learning problem: processing data, evaluating models, and choosing the right algorithm among the main Machine Learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Structure\n",
    "\n",
    "1. **Data Exploration and Preprocessing**\n",
    "2. **Regression - Score Prediction**\n",
    "3. **Binary Classification and Metrics**\n",
    "4. **Methodology and Validation**\n",
    "\n",
    "---\n",
    "\n",
    "## Required Libraries\n",
    "\n",
    "You will use the following Python modules: `sklearn`, `pandas`, `numpy`, `matplotlib`, `seaborn`, `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score,\n",
    "                             classification_report, confusion_matrix, roc_curve, auc)\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Data Exploration and Preprocessing\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Download the data from: https://files.grouplens.org/datasets/movielens/ml-latest-small.zip (We will work with the **ml-latest-small.zip** file - 700 users, 9000 movies - of the https://grouplens.org/datasets/movielens/ site).\n",
    "\n",
    "\n",
    "\n",
    "The archive contains:\n",
    "- `movies.csv`: movie information (ID, title, genres among 22 categories)\n",
    "- `ratings.csv`: ratings given by users\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Before anything else, it is very important in a Data Science problem to understand the data you are dealing with. The first step is to perform an Exploratory Data Analysis (EDA), which involves studying the statistical properties of the data: histograms, correlations, amplitudes, ranges, etc. It may be necessary to transform the data (pre-processing) to scale it or eliminate certain dimensions. This first section illustrates in a simplified version this preliminary work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1: Load and Explore the Data\n",
    "\n",
    "Run the cells below to load the data, then answer the questions.\n",
    "\n",
    "**Step 1: Load the raw data**\n",
    "\n",
    "We load two CSV files:\n",
    "- `movies.csv`: contains movie information (ID, title, genres)\n",
    "- `ratings.csv`: contains user ratings (userId, movieId, rating from 0.5 to 5.0)\n",
    "\n",
    "**Step 2: Build the rating matrix**\n",
    "\n",
    "The **rating matrix** is a 2D representation where:\n",
    "- Rows = movies\n",
    "- Columns = users\n",
    "- Values = ratings (0 if the user hasn't rated the movie)\n",
    "\n",
    "This matrix is typically very **sparse** (mostly zeros) because users only rate a small fraction of available movies.\n",
    "\n",
    "**Step 3: Build movie features**\n",
    "\n",
    "For machine learning, we need to represent each movie as a **feature vector**. We extract:\n",
    "- **Year**: extracted from the title (e.g., \"Toy Story (1995)\" → 1995)\n",
    "- **Genres**: converted to binary columns (one-hot encoding). Example: a Comedy+Romance movie → [Comedy=1, Romance=1, Action=0, ...]\n",
    "- **Rating statistics**: number of ratings and average rating per movie\n",
    "\n",
    "**Your tasks:**\n",
    "1. Observe the structure of the two dataframes (`movies` and `ratings`)\n",
    "2. Calculate the sparsity of the rating matrix (percentage of empty cells)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# STEP 1: Load raw data\n",
    "# ==================================================\n",
    "\n",
    "movies = pd.read_csv('ml-latest-small/movies.csv')\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MOVIES dataset\")\n",
    "print(\"=\" * 50)\n",
    "print(movies.head())\n",
    "print(f\"\\nShape: {movies.shape} (rows x columns)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RATINGS dataset\")\n",
    "print(\"=\" * 50)\n",
    "print(ratings.head())\n",
    "print(f\"\\nShape: {ratings.shape} (rows x columns)\")\n",
    "print(f\"  -> Each row = one rating from one user on one movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# STEP 2: Build the rating matrix\n",
    "# ==================================================\n",
    "# pivot_table transforms the long format (one row per rating) \n",
    "# into a matrix format (rows=movies, columns=users)\n",
    "# fill_value=0 means \"no rating\" is represented as 0\n",
    "\n",
    "rating_matrix = ratings.pivot_table(index='movieId', columns='userId', values='rating', fill_value=0)\n",
    "\n",
    "print(f\"Rating matrix shape: {rating_matrix.shape}\")\n",
    "print(f\"Number of movies: {rating_matrix.shape[0]}\")\n",
    "print(f\"Number of users: {rating_matrix.shape[1]}\")\n",
    "\n",
    "# TODO: Calculate the sparsity (percentage of zeros in the matrix)\n",
    "# Sparsity = (number of zeros) / (total cells) * 100\n",
    "# Hint: use (rating_matrix == 0).sum().sum() to count zeros\n",
    "\n",
    "## [STUDENT]\n",
    "n_zeros = ...  # Complete this line\n",
    "total_cells = ...  # Complete this line\n",
    "sparsity = ...  # Complete this line\n",
    "## [/STUDENT]\n",
    "\n",
    "print(f\"\\nSparsity: {sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# STEP 3: Build movie feature matrix\n",
    "# ==================================================\n",
    "# Each movie will be described by a feature vector for ML\n",
    "\n",
    "# --- Extract year from title ---\n",
    "# Example: \"Toy Story (1995)\" -> 1995\n",
    "movies['year'] = movies['title'].str.extract(r'\\((\\d{4})\\)').astype(float)\n",
    "movies['year'] = movies['year'].fillna(movies['year'].median())  # Fill missing years with median\n",
    "\n",
    "# --- Binarize genres (one-hot encoding) ---\n",
    "# Example: \"Comedy|Romance\" -> Comedy=1, Romance=1, Action=0, Drama=0, ...\n",
    "genres_split = movies['genres'].str.get_dummies(sep='|')\n",
    "genres_split.index = movies['movieId']\n",
    "\n",
    "# --- Compute rating statistics per movie ---\n",
    "rating_count = ratings.groupby('movieId').size().rename('rating_count')  # How many ratings?\n",
    "rating_mean = ratings.groupby('movieId')['rating'].mean().rename('rating_mean')  # Average rating?\n",
    "\n",
    "# --- Combine all features into one dataframe ---\n",
    "movie_features = pd.concat([\n",
    "    movies[['movieId', 'year']].set_index('movieId'),\n",
    "    genres_split,\n",
    "    rating_count,\n",
    "    rating_mean\n",
    "], axis=1)\n",
    "\n",
    "movie_features = movie_features.fillna(0)  # Fill missing values with 0\n",
    "\n",
    "print(f\"Movie features shape: {movie_features.shape}\")\n",
    "print(f\"  -> {movie_features.shape[0]} movies\")\n",
    "print(f\"  -> {movie_features.shape[1]} features per movie\")\n",
    "print(f\"\\nFeature columns: {list(movie_features.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(movie_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Data Statistics\n",
    "\n",
    "**Your tasks:**\n",
    "1. Calculate the mean and median number of ratings per user\n",
    "2. Calculate the mean and median number of ratings per movie\n",
    "3. Run the visualization cells and observe the distributions\n",
    "\n",
    "**Questions to answer:**\n",
    "- Are ratings uniformly distributed across users and movies?\n",
    "- What does this imply for building a recommendation system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating matrix (sample)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sample_matrix = rating_matrix.iloc[:100, :50]  # Sample for visualization\n",
    "plt.imshow(sample_matrix, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Rating')\n",
    "plt.xlabel('User ID (sample)')\n",
    "plt.ylabel('Movie ID (sample)')\n",
    "plt.title('Rating Matrix Visualization (Sample: 100 movies x 50 users)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Matrix sparsity (zeros): {(rating_matrix == 0).sum().sum() / rating_matrix.size:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of ratings per user\n",
    "ratings_per_user = ratings.groupby('userId').size()\n",
    "\n",
    "# TODO: Calculate mean and median ratings per user\n",
    "\n",
    "## [STUDENT]\n",
    "mean_ratings_user = ...  # Use .mean()\n",
    "median_ratings_user = ...  # Use .median()\n",
    "## [/STUDENT]\n",
    "\n",
    "print(f\"Ratings per user:\")\n",
    "print(f\"  Mean: {mean_ratings_user:.2f}\")\n",
    "print(f\"  Median: {median_ratings_user:.2f}\")\n",
    "print(f\"  Min: {ratings_per_user.min()}, Max: {ratings_per_user.max()}\")\n",
    "\n",
    "# Visualization (provided)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(ratings_per_user, bins=50, edgecolor='black')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Distribution of Ratings per User')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of ratings per movie\n",
    "ratings_per_movie = ratings.groupby('movieId').size()\n",
    "\n",
    "# TODO: Calculate mean and median ratings per movie\n",
    "\n",
    "## [STUDENT]\n",
    "mean_ratings_movie = ...  # Use .mean()\n",
    "median_ratings_movie = ...  # Use .median()\n",
    "## [/STUDENT]\n",
    "\n",
    "print(f\"Ratings per movie:\")\n",
    "print(f\"  Mean: {mean_ratings_movie:.2f}\")\n",
    "print(f\"  Median: {median_ratings_movie:.2f}\")\n",
    "print(f\"  Min: {ratings_per_movie.min()}, Max: {ratings_per_movie.max()}\")\n",
    "\n",
    "# TODO: Calculate percentage of movies with less than 10 ratings\n",
    "\n",
    "## [STUDENT]\n",
    "movies_few_ratings = ...  # Count movies with < 10 ratings\n",
    "pct_few_ratings = ...  # Calculate percentage\n",
    "## [/STUDENT]\n",
    "\n",
    "print(f\"\\nMovies with < 10 ratings: {movies_few_ratings} ({pct_few_ratings:.2f}%)\")\n",
    "\n",
    "# Visualization (provided)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(ratings_per_movie, bins=50, edgecolor='black')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.title('Distribution of Ratings per Movie')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of average rating per user\n",
    "avg_rating_per_user = ratings.groupby('userId')['rating'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(avg_rating_per_user, bins=30, edgecolor='black', color='green')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Distribution of Average Rating per User')\n",
    "plt.axvline(avg_rating_per_user.mean(), color='red', linestyle='--', label=f'Mean: {avg_rating_per_user.mean():.2f}')\n",
    "plt.axvline(avg_rating_per_user.median(), color='orange', linestyle='--', label=f'Median: {avg_rating_per_user.median():.2f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean of average ratings: {avg_rating_per_user.mean():.2f}\")\n",
    "print(f\"Std of average ratings: {avg_rating_per_user.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between number of ratings and average rating per movie\n",
    "movie_stats = ratings.groupby('movieId').agg({\n",
    "    'rating': ['count', 'mean']\n",
    "}).reset_index()\n",
    "movie_stats.columns = ['movieId', 'count', 'mean_rating']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(movie_stats['count'], movie_stats['mean_rating'], alpha=0.3)\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Correlation: Rating Count vs Average Rating')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "correlation = movie_stats[['count', 'mean_rating']].corr().iloc[0, 1]\n",
    "plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(np.log1p(movie_stats['count']), movie_stats['mean_rating'], alpha=0.3, color='orange')\n",
    "plt.xlabel('Log(1 + Number of Ratings)')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Correlation (Log Scale)')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Pearson correlation: {correlation:.3f}\")\n",
    "print(\"\\nInterpretation: Positive correlation suggests popular movies tend to have higher ratings (popularity bias)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: Challenges for Machine Learning\n",
    "\n",
    "Based on your observations above, list the main challenges for building a recommendation system:\n",
    "\n",
    "**Write your answer below** (think about: sparsity, imbalanced data, cold start problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "\n",
    "1. ...\n",
    "\n",
    "2. ...\n",
    "\n",
    "3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4: Choosing a Classification Threshold\n",
    "\n",
    "For binary classification, we need to convert ratings into two classes: \"liked\" and \"disliked\".\n",
    "\n",
    "**Your task:** \n",
    "1. Visualize the rating distribution\n",
    "2. For each candidate threshold (3.0, 3.5, 4.0), calculate the percentage of \"liked\" ratings\n",
    "3. Choose the threshold that gives the most balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating distribution (provided)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(ratings['rating'], bins=10, edgecolor='black')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rating Distribution')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# TODO: For each threshold, calculate the percentage of \"liked\" ratings\n",
    "# A rating is \"liked\" if rating >= threshold\n",
    "\n",
    "## [STUDENT]\n",
    "for threshold in [3.0, 3.5, 4.0]:\n",
    "    pct_liked = ...  # Calculate: (number of ratings >= threshold) / (total ratings) * 100\n",
    "    print(f\"Threshold {threshold}: {pct_liked:.1f}% liked\")\n",
    "## [/STUDENT]\n",
    "\n",
    "## [STUDENT]\n",
    "THRESHOLD = ...  # Your choice based on the results above\n",
    "## [/STUDENT]\n",
    "\n",
    "print(f\"\\nChosen threshold: {THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why did you choose this threshold?**\n",
    "\n",
    "Your answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Regression - Predicting Average Movie Ratings\n",
    "\n",
    "**Goal:** Predict the average rating of a movie based on its features (genres, year).\n",
    "\n",
    "Each sample = 1 movie. We predict how well-liked a movie is in general.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Data Preparation\n",
    "\n",
    "Run the cell below to prepare the data for regression. Observe:\n",
    "- What features are used (X)?\n",
    "- What is the target (y)?\n",
    "- How many movies are in the training/test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "\n",
    "# Filter movies with at least 5 ratings for reliable averages\n",
    "min_ratings = 5\n",
    "movie_data = movie_features[movie_features['rating_count'] >= min_ratings].copy()\n",
    "\n",
    "print(f\"Movies with >= {min_ratings} ratings: {len(movie_data)} (out of {len(movie_features)})\")\n",
    "\n",
    "# Features: year + genres (exclude rating_mean and rating_count to avoid \"cheating\")\n",
    "feature_cols = [col for col in movie_data.columns if col not in ['rating_mean', 'rating_count']]\n",
    "X = movie_data[feature_cols].values\n",
    "y = movie_data['rating_mean'].values  # Target: average rating\n",
    "\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"X shape: {X.shape} (movies x features)\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Train/test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: Count the number of movies in training and test sets\n",
    "\n",
    "## [STUDENT]\n",
    "n_train = ...  # Use len() on X_train or y_train\n",
    "n_test = ...   # Use len() on X_test or y_test\n",
    "## [/STUDENT]\n",
    "\n",
    "print(f\"\\nTraining set: {n_train} movies\")\n",
    "print(f\"Test set: {n_test} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Train and Compare Regression Models\n",
    "\n",
    "Run the cell below to train 3 regression models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression models\n",
    "\n",
    "# Model 1: Linear Regression (provided)\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "print(\"Linear Regression trained!\")\n",
    "\n",
    "# TODO: Train Model 2 - Random Forest\n",
    "# Use: RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "## [STUDENT]\n",
    "rf_model = ...  # Create the model\n",
    "rf_model.fit(...)  # Train on training data\n",
    "y_pred_rf = ...  # Predict on test data\n",
    "## [/STUDENT]\n",
    "\n",
    "print(\"Random Forest trained!\")\n",
    "\n",
    "# TODO: Train Model 3 - Gradient Boosting\n",
    "# Use: GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "## [STUDENT]\n",
    "gb_model = ...  # Create the model\n",
    "gb_model.fit(...)  # Train on training data\n",
    "y_pred_gb = ...  # Predict on test data\n",
    "## [/STUDENT]\n",
    "\n",
    "print(\"Gradient Boosting trained!\")\n",
    "print(\"\\nAll models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Evaluate Models\n",
    "\n",
    "**Metrics:**\n",
    "- **MAE** (Mean Absolute Error): Average error in stars (lower is better)\n",
    "- **RMSE** (Root Mean Squared Error): Similar to MAE but penalizes large errors more\n",
    "- **R²** (Coefficient of determination): Proportion of variance explained (1.0 = perfect, 0 = useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "\n",
    "# TODO: Calculate MAE, RMSE and R² for each model\n",
    "# Use: mean_absolute_error(y_true, y_pred), mean_squared_error(y_true, y_pred), r2_score(y_true, y_pred)\n",
    "# Note: RMSE = sqrt(MSE), use np.sqrt()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REGRESSION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<20} {'MAE':>8} {'RMSE':>8} {'R²':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Linear Regression (example provided)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "print(f\"{'Linear Regression':<20} {mae_lr:>8.3f} {rmse_lr:>8.3f} {r2_lr:>8.3f}\")\n",
    "\n",
    "# TODO: Calculate metrics for Random Forest\n",
    "\n",
    "## [STUDENT]\n",
    "mae_rf = ...  # MAE for Random Forest\n",
    "rmse_rf = ...  # RMSE for Random Forest\n",
    "r2_rf = ...  # R² for Random Forest\n",
    "## [/STUDENT]\n",
    "\n",
    "print(f\"{'Random Forest':<20} {mae_rf:>8.3f} {rmse_rf:>8.3f} {r2_rf:>8.3f}\")\n",
    "\n",
    "# TODO: Calculate metrics for Gradient Boosting\n",
    "\n",
    "## [STUDENT]\n",
    "mae_gb = ...  # MAE for Gradient Boosting\n",
    "rmse_gb = ...  # RMSE for Gradient Boosting\n",
    "r2_gb = ...  # R² for Gradient Boosting\n",
    "## [/STUDENT]\n",
    "\n",
    "print(f\"{'Gradient Boosting':<20} {mae_gb:>8.3f} {rmse_gb:>8.3f} {r2_gb:>8.3f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Analysis Questions\n",
    "\n",
    "**Answer the following questions:**\n",
    "\n",
    "1. Which model performs best? Why do you think so?\n",
    "\n",
    "2. Is an MAE of ~0.4 stars good enough for a recommendation system?\n",
    "\n",
    "3. What are the limitations of predicting ratings using only genres and year?\n",
    "\n",
    "**Your answers:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "\n",
    "1. ...\n",
    "\n",
    "2. ...\n",
    "\n",
    "3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Binary Classification\n",
    "\n",
    "**Goal:** Classify movies as \"generally liked\" (rating >= threshold) or \"generally disliked\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Create Binary Labels\n",
    "\n",
    "Run the cell below to create binary labels from the threshold you chose earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary labels (code provided)\n",
    "y_binary = (y >= THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Threshold: {THRESHOLD}\")\n",
    "print(f\"Class 0 (disliked): {(y_binary == 0).sum()} movies ({(y_binary == 0).mean():.1%})\")\n",
    "print(f\"Class 1 (liked): {(y_binary == 1).sum()} movies ({(y_binary == 1).mean():.1%})\")\n",
    "\n",
    "# Train/test split\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train_clf)} movies\")\n",
    "print(f\"Test set: {len(X_test_clf)} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Train Classification Models\n",
    "\n",
    "Run the cell below to train 5 different classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define and train 5 classifiers\n",
    "# Classes to use:\n",
    "#   - SVC(kernel='linear', probability=True, random_state=42)\n",
    "#   - SVC(kernel='rbf', probability=True, random_state=42)\n",
    "#   - KNeighborsClassifier(n_neighbors=5)\n",
    "#   - RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "#   - GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "## [STUDENT]\n",
    "classifiers = {\n",
    "    'SVM (Linear)': ...,       # Create SVC with linear kernel\n",
    "    'SVM (RBF)': ...,          # Create SVC with RBF kernel\n",
    "    'k-NN (k=5)': ...,         # Create KNeighborsClassifier\n",
    "    'Random Forest': ...,      # Create RandomForestClassifier\n",
    "    'Gradient Boosting': ...   # Create GradientBoostingClassifier\n",
    "}\n",
    "\n",
    "# Train all classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    ...  # Use clf.fit() with training data\n",
    "    print(f\"{name} trained!\")\n",
    "## [/STUDENT]\n",
    "\n",
    "print(\"\\nAll classifiers trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Classification Metrics\n",
    "\n",
    "**Key metrics:**\n",
    "- **Accuracy**: Percentage of correct predictions\n",
    "- **Precision**: Of predicted positives, how many are truly positive?\n",
    "- **Recall**: Of actual positives, how many did we find?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **AUC**: Area under the ROC curve (ranking ability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate all classifiers\n",
    "# Functions to use:\n",
    "#   - clf.predict(X_test_clf) -> predicted labels\n",
    "#   - clf.predict_proba(X_test_clf)[:, 1] -> probability of class 1\n",
    "#   - accuracy_score(y_true, y_pred)\n",
    "#   - precision_score(y_true, y_pred, zero_division=0)\n",
    "#   - recall_score(y_true, y_pred)\n",
    "#   - f1_score(y_true, y_pred)\n",
    "#   - roc_curve(y_true, y_proba) -> returns (fpr, tpr, thresholds)\n",
    "#   - auc(fpr, tpr) -> AUC score\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<20} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1':>10} {'AUC':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "## [STUDENT]\n",
    "for name, clf in classifiers.items():\n",
    "    # Step 1: Get predictions\n",
    "    y_pred = ...  # Predict labels\n",
    "    y_proba = ...  # Predict probabilities for class 1\n",
    "    \n",
    "    # Step 2: Calculate metrics\n",
    "    acc = ...   # Accuracy\n",
    "    prec = ...  # Precision\n",
    "    rec = ...   # Recall\n",
    "    f1 = ...    # F1-score\n",
    "    \n",
    "    # Step 3: Calculate AUC\n",
    "    fpr, tpr, _ = ...  # ROC curve\n",
    "    auc_score = ...    # AUC\n",
    "    \n",
    "    print(f\"{name:<20} {acc:>10.3f} {prec:>10.3f} {rec:>10.3f} {f1:>10.3f} {auc_score:>10.3f}\")\n",
    "## [/STUDENT]\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: ROC Curves\n",
    "\n",
    "The ROC curve shows the trade-off between True Positive Rate and False Positive Rate. AUC = 1 is perfect, AUC = 0.5 is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot ROC curves for all classifiers\n",
    "# Functions to use:\n",
    "#   - clf.predict_proba(X_test_clf)[:, 1] -> probability of class 1\n",
    "#   - roc_curve(y_true, y_proba) -> returns (fpr, tpr, thresholds)\n",
    "#   - auc(fpr, tpr) -> AUC score\n",
    "#   - plt.plot(fpr, tpr, label=f'{name} (AUC={auc_score:.2f})')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "## [STUDENT]\n",
    "for name, clf in classifiers.items():\n",
    "    y_proba = ...  # Predict probabilities\n",
    "    fpr, tpr, _ = ...  # Compute ROC curve\n",
    "    auc_score = ...  # Compute AUC\n",
    "    plt.plot(...)  # Plot the curve with label\n",
    "## [/STUDENT]\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC=0.50)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: Analysis Questions\n",
    "\n",
    "**Answer the following:**\n",
    "\n",
    "1. Which classifier has the best AUC? Which has the worst?\n",
    "\n",
    "2. Why does SVM (RBF) perform poorly? (Hint: think about feature scaling)\n",
    "\n",
    "3. For a recommendation system, would you prioritize precision or recall? Why?\n",
    "\n",
    "4. Is an AUC of ~0.74 good enough for production?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "\n",
    "1. ...\n",
    "\n",
    "2. ...\n",
    "\n",
    "3. ...\n",
    "\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Model Validation and Improvement\n",
    "\n",
    "**Key concepts:**\n",
    "- Cross-validation: More robust evaluation than single train/test split\n",
    "- Hyperparameter tuning: Finding the best model configuration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Cross-Validation\n",
    "\n",
    "A single train/test split might be \"lucky\" or \"unlucky\". Cross-validation:\n",
    "1. Splits data into k folds\n",
    "2. Trains k times, each time using a different fold as test set\n",
    "3. Averages results for more reliable estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation on Random Forest (code provided)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Evaluate with 5-fold cross-validation\n",
    "cv_scores = cross_val_score(rf_clf, X, y_binary, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results (AUC):\")\n",
    "print(f\"  Scores per fold: {cv_scores.round(3)}\")\n",
    "print(f\"  Mean: {cv_scores.mean():.3f}\")\n",
    "print(f\"  Std:  {cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Hyperparameter Tuning\n",
    "\n",
    "Models have **hyperparameters** that control their behavior:\n",
    "- Random Forest: `n_estimators` (number of trees), `max_depth` (tree depth)\n",
    "- SVM: `C` (regularization), `gamma` (kernel width)\n",
    "\n",
    "**GridSearchCV** automatically searches for the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ============================================\n",
    "# Model 1: Random Forest (example provided)\n",
    "# ============================================\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid_rf,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_rf.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "print(\"RANDOM FOREST\")\n",
    "print(f\"  Best parameters: {grid_search_rf.best_params_}\")\n",
    "print(f\"  Best CV score (AUC): {grid_search_rf.best_score_:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# Model 2: SVM (Linear)\n",
    "# ============================================\n",
    "# TODO: Find the hyperparameter to tune for SVM\n",
    "# Hint: The main hyperparameter for SVM is 'C' (regularization strength)\n",
    "# Try values: [0.1, 1, 10]\n",
    "\n",
    "## [STUDENT]\n",
    "param_grid_svm = {\n",
    "    ...  # Which hyperparameter? Which values?\n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(\n",
    "    SVC(kernel='linear', probability=True, random_state=42),\n",
    "    param_grid_svm,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_svm.fit(...)  # Train on which data?\n",
    "## [/STUDENT]\n",
    "\n",
    "print(\"\\nSVM (Linear)\")\n",
    "print(f\"  Best parameters: {grid_search_svm.best_params_}\")\n",
    "print(f\"  Best CV score (AUC): {grid_search_svm.best_score_:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# Model 3: Gradient Boosting\n",
    "# ============================================\n",
    "# TODO: Find hyperparameters to tune for Gradient Boosting\n",
    "# Hint: Similar to Random Forest - 'n_estimators' and 'max_depth'\n",
    "# Also try 'learning_rate': [0.05, 0.1, 0.2]\n",
    "\n",
    "## [STUDENT]\n",
    "param_grid_gb = {\n",
    "    ...  # Which hyperparameters? Which values?\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid_gb,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_gb.fit(...)  # Train on which data?\n",
    "## [/STUDENT]\n",
    "\n",
    "print(\"\\nGRADIENT BOOSTING\")\n",
    "print(f\"  Best parameters: {grid_search_gb.best_params_}\")\n",
    "print(f\"  Best CV score (AUC): {grid_search_gb.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate optimized models on test set\n",
    "# For each model, calculate accuracy and AUC\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"OPTIMIZED MODELS - TEST SET PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_models = {\n",
    "    'Random Forest': grid_search_rf.best_estimator_,\n",
    "    'SVM (Linear)': grid_search_svm.best_estimator_,\n",
    "    'Gradient Boosting': grid_search_gb.best_estimator_\n",
    "}\n",
    "\n",
    "## [STUDENT]\n",
    "for name, model in best_models.items():\n",
    "    # Step 1: Get predictions\n",
    "    y_pred = ...  # Predict labels\n",
    "    y_proba = ...  # Predict probabilities for class 1\n",
    "    \n",
    "    # Step 2: Calculate metrics\n",
    "    acc = ...  # Accuracy\n",
    "    fpr, tpr, _ = ...  # ROC curve\n",
    "    auc_score = ...  # AUC\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {acc:.3f}\")\n",
    "    print(f\"  AUC: {auc_score:.3f}\")\n",
    "    print()\n",
    "## [/STUDENT]\n",
    "\n",
    "best_model = grid_search_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Final Questions\n",
    "\n",
    "**Answer the following:**\n",
    "\n",
    "1. Did hyperparameter tuning improve the model? By how much?\n",
    "\n",
    "2. What are the limitations of our content-based approach (using only genres and year)?\n",
    "\n",
    "3. How could we improve the recommendation system? (Think about: more features, collaborative filtering, user information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "\n",
    "1. ...\n",
    "\n",
    "2. ...\n",
    "\n",
    "3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4: Overfitting vs Underfitting\n",
    "\n",
    "**Overfitting**: Model is too complex, memorizes training data but fails on new data\n",
    "- Training accuracy >> Test accuracy\n",
    "\n",
    "**Underfitting**: Model is too simple, fails on both training and test data\n",
    "- Both accuracies are low\n",
    "\n",
    "**Learning curves** help diagnose these issues by plotting performance vs training set size.\n",
    "\n",
    "**Your task:** Implement a learning curve manually:\n",
    "1. Train the model on increasing portions of the training data (20%, 40%, 60%, 80%, 100%)\n",
    "2. For each portion, compute the AUC on both training and test sets\n",
    "3. Plot both curves to visualize overfitting/underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement learning curves manually\n",
    "# For each training size, train the model and compute AUC on train and test sets\n",
    "\n",
    "train_sizes_pct = [0.2, 0.4, 0.6, 0.8, 1.0]  # Percentages of training data to use\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "## [STUDENT]\n",
    "for pct in train_sizes_pct:\n",
    "    # Step 1: Select a subset of training data\n",
    "    n_samples = int(pct * len(X_train_clf))\n",
    "    X_train_subset = X_train_clf[:n_samples]\n",
    "    y_train_subset = y_train_clf[:n_samples]\n",
    "    \n",
    "    # Step 2: Train a Random Forest on the subset\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(...)  # Train on subset\n",
    "    \n",
    "    # Step 3: Compute AUC on training subset\n",
    "    y_proba_train = ...  # Predict probabilities on training subset\n",
    "    fpr_train, tpr_train, _ = ...\n",
    "    auc_train = ...\n",
    "    \n",
    "    # Step 4: Compute AUC on test set (always the same test set)\n",
    "    y_proba_test = ...  # Predict probabilities on test set\n",
    "    fpr_test, tpr_test, _ = ...\n",
    "    auc_test = ...\n",
    "    \n",
    "    train_scores.append(auc_train)\n",
    "    test_scores.append(auc_test)\n",
    "    \n",
    "    print(f\"Training size: {n_samples:4d} ({pct:.0%}) -> Train AUC: {auc_train:.3f}, Test AUC: {auc_test:.3f}\")\n",
    "## [/STUDENT]\n",
    "\n",
    "# Plot learning curves (provided)\n",
    "train_sizes_abs = [int(pct * len(X_train_clf)) for pct in train_sizes_pct]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes_abs, train_scores, 'o-', color='#2ecc71', linewidth=2, markersize=8, \n",
    "         label=f'Training score (final: {train_scores[-1]:.3f})')\n",
    "plt.plot(train_sizes_abs, test_scores, 's-', color='#e74c3c', linewidth=2, markersize=8,\n",
    "         label=f'Test score (final: {test_scores[-1]:.3f})')\n",
    "\n",
    "# Add gap annotation\n",
    "gap = train_scores[-1] - test_scores[-1]\n",
    "plt.fill_between(train_sizes_abs, train_scores, test_scores, alpha=0.2, color='gray')\n",
    "plt.annotate(f'Gap = {gap:.3f}', xy=(train_sizes_abs[-1], (train_scores[-1] + test_scores[-1])/2),\n",
    "             xytext=(train_sizes_abs[-1] - 400, (train_scores[-1] + test_scores[-1])/2),\n",
    "             fontsize=11, ha='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.xlabel('Training set size (number of movies)', fontsize=12)\n",
    "plt.ylabel('AUC Score', fontsize=12)\n",
    "plt.title('Learning Curves - Random Forest Classifier', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim(0.5, 1.05)\n",
    "\n",
    "# Add interpretation zone\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.text(train_sizes_abs[0], 0.52, 'Random baseline (AUC=0.5)', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"LEARNING CURVE ANALYSIS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Final training score: {train_scores[-1]:.3f}\")\n",
    "print(f\"Final test score:     {test_scores[-1]:.3f}\")\n",
    "print(f\"Gap (train - test):   {gap:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Based on the learning curves, is the model overfitting, underfitting, or well-fitted? Justify.\n",
    "\n",
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "**What you learned:**\n",
    "\n",
    "1. **EDA is essential**: Understanding data structure (sparsity, distributions) before modeling\n",
    "\n",
    "2. **Multiple models**: Always compare different approaches; tree-based methods often work well\n",
    "\n",
    "3. **Metrics matter**: Choose based on your goal (AUC for ranking, precision/recall for decisions)\n",
    "\n",
    "4. **Validation**: Cross-validation and hyperparameter tuning improve reliability\n",
    "\n",
    "5. **Limitations**: Content-based approaches (genres, year) have limited predictive power without user information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
