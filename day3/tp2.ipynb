{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3: Object Detection with YOLO\n",
    "\n",
    "**Day 3 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day3/tp3.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this practical, you will learn **object detection** - the task of locating and classifying multiple objects in an image.\n",
    "\n",
    "By the end of this practical, you will:\n",
    "\n",
    "1. **Understand object detection**: Classification + localization with bounding boxes\n",
    "2. **Learn YOLO format**: How to represent bounding box annotations\n",
    "3. **Fine-tune YOLOv8**: Adapt a pre-trained model to detect aquarium animals\n",
    "4. **Evaluate detection**: Understand mAP and analyze predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Object Detection vs Classification\n",
    "\n",
    "## What's the Difference?\n",
    "\n",
    "| Task | Input | Output | Example |\n",
    "|------|-------|--------|---------|\n",
    "| **Classification** | Image | Single label | \"This is a fish\" |\n",
    "| **Detection** | Image | Multiple boxes + labels | \"Fish at (x1,y1,x2,y2), Shark at (x3,y3,x4,y4)\" |\n",
    "\n",
    "```\n",
    "Classification:                    Detection:\n",
    "┌─────────────────┐               ┌─────────────────┐\n",
    "│                 │               │  ┌───┐          │\n",
    "│    (image)      │   →  \"fish\"   │  │fish│  ┌────┐│\n",
    "│                 │               │  └───┘  │shark││\n",
    "│                 │               │         └────┘│\n",
    "└─────────────────┘               └─────────────────┘\n",
    "```\n",
    "\n",
    "## YOLO: You Only Look Once\n",
    "\n",
    "**YOLO** is a fast, accurate object detection model that processes the entire image in one pass.\n",
    "\n",
    "Key advantages:\n",
    "- **Real-time**: Can process video at 30+ FPS\n",
    "- **End-to-end**: Single neural network predicts boxes and classes\n",
    "- **Transfer learning**: Pre-trained on COCO (80 classes), easy to fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "!pip install -q ultralytics\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: The Aquarium Dataset\n",
    "\n",
    "We'll use the **Aquarium Dataset** - images from real aquariums with labeled marine animals.\n",
    "\n",
    "**7 classes**: fish, jellyfish, penguin, puffin, shark, starfish, stingray\n",
    "\n",
    "**638 images**: 448 train / 127 validation / 63 test\n",
    "\n",
    "This is a great dataset for learning detection because:\n",
    "- Multiple objects per image\n",
    "- Varying object sizes\n",
    "- Relevant to marine biology research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the dataset\n",
    "!wget -q https://www.raphaelcousin.com/modules/sandbox/aquarium_yolo.zip\n",
    "!unzip -q -o aquarium_yolo.zip\n",
    "!rm aquarium_yolo.zip\n",
    "\n",
    "# Check the structure\n",
    "print(\"Dataset structure:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset configuration\n",
    "with open('data.yaml', 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "class_names = data_config['names']\n",
    "num_classes = data_config['nc']\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"\\nClasses:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images in each split\n",
    "train_images = list(Path('train/images').glob('*.jpg'))\n",
    "valid_images = list(Path('valid/images').glob('*.jpg'))\n",
    "test_images = list(Path('test/images').glob('*.jpg'))\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"  Train:      {len(train_images)} images\")\n",
    "print(f\"  Validation: {len(valid_images)} images\")\n",
    "print(f\"  Test:       {len(test_images)} images\")\n",
    "print(f\"  Total:      {len(train_images) + len(valid_images) + len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding YOLO Label Format\n",
    "\n",
    "YOLO uses a simple text format for bounding box annotations:\n",
    "\n",
    "```\n",
    "class_id  x_center  y_center  width  height\n",
    "```\n",
    "\n",
    "All coordinates are **normalized** (0-1 relative to image size):\n",
    "\n",
    "```\n",
    "┌─────────────────────────────┐\n",
    "│ (0,0)                       │\n",
    "│      ┌─────────┐            │\n",
    "│      │ (x_c,   │            │\n",
    "│      │  y_c)   │ height     │\n",
    "│      │    *    │            │\n",
    "│      └─────────┘            │\n",
    "│         width               │\n",
    "│                       (1,1) │\n",
    "└─────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a sample label file\n",
    "sample_label = list(Path('train/labels').glob('*.txt'))[0]\n",
    "print(f\"Sample label file: {sample_label.name}\")\n",
    "print(f\"\\nContents (class_id, x_center, y_center, width, height):\")\n",
    "print(\"-\" * 50)\n",
    "with open(sample_label) as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        class_id = int(parts[0])\n",
    "        coords = [float(x) for x in parts[1:]]\n",
    "        print(f\"  Class {class_id} ({class_names[class_id]:10s}): x={coords[0]:.3f}, y={coords[1]:.3f}, w={coords[2]:.3f}, h={coords[3]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for each class\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, num_classes))\n",
    "\n",
    "def plot_image_with_boxes(image_path, label_path, ax=None):\n",
    "    \"\"\"Plot image with YOLO format bounding boxes.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Load image\n",
    "    img = Image.open(image_path)\n",
    "    img_width, img_height = img.size\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Load and draw boxes\n",
    "    if label_path.exists():\n",
    "        with open(label_path) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                class_id = int(parts[0])\n",
    "                x_center, y_center, width, height = [float(x) for x in parts[1:]]\n",
    "                \n",
    "                # Convert normalized coords to pixels\n",
    "                x1 = (x_center - width/2) * img_width\n",
    "                y1 = (y_center - height/2) * img_height\n",
    "                box_w = width * img_width\n",
    "                box_h = height * img_height\n",
    "                \n",
    "                # Draw box\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), box_w, box_h,\n",
    "                    linewidth=2, edgecolor=colors[class_id], facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add label\n",
    "                ax.text(x1, y1-5, class_names[class_id], color=colors[class_id],\n",
    "                       fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.axis('off')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images with bounding boxes\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "sample_images = random.sample(train_images, 6)\n",
    "\n",
    "for ax, img_path in zip(axes.flat, sample_images):\n",
    "    label_path = Path('train/labels') / (img_path.stem + '.txt')\n",
    "    plot_image_with_boxes(img_path, label_path, ax=ax)\n",
    "\n",
    "plt.suptitle('Aquarium Dataset - Training Samples', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Look at the sample images:\n",
    "1. How many objects are typically in each image?\n",
    "2. Which classes seem most common? Which are rare?\n",
    "3. What challenges do you notice? (occlusion, size variation, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Load Pre-trained YOLO\n",
    "\n",
    "YOLOv8 comes pre-trained on **COCO** (80 classes including some animals).\n",
    "\n",
    "Let's see how well it works on our aquarium images **before fine-tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load pre-trained YOLOv8 nano (smallest, fastest)\n",
    "model_pretrained = YOLO('yolov8n.pt')\n",
    "\n",
    "print(\"Pre-trained YOLOv8n loaded!\")\n",
    "print(f\"Model trained on COCO dataset (80 classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample image\n",
    "sample_image = random.choice(train_images)\n",
    "results = model_pretrained.predict(source=str(sample_image), conf=0.25, verbose=False)\n",
    "\n",
    "# Display result\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original with ground truth\n",
    "label_path = Path('train/labels') / (sample_image.stem + '.txt')\n",
    "plot_image_with_boxes(sample_image, label_path, ax=axes[0])\n",
    "axes[0].set_title('Ground Truth Labels', fontsize=12)\n",
    "\n",
    "# Pre-trained model predictions\n",
    "axes[1].imshow(Image.open(sample_image))\n",
    "for box in results[0].boxes:\n",
    "    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "    conf = box.conf[0].cpu().numpy()\n",
    "    cls = int(box.cls[0].cpu().numpy())\n",
    "    cls_name = model_pretrained.names[cls]\n",
    "    \n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                             linewidth=2, edgecolor='red', facecolor='none')\n",
    "    axes[1].add_patch(rect)\n",
    "    axes[1].text(x1, y1-5, f'{cls_name} {conf:.2f}', color='red', fontsize=9,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(f'Pre-trained YOLO Predictions ({len(results[0].boxes)} detections)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPre-trained model detected: {[model_pretrained.names[int(b.cls[0])] for b in results[0].boxes]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "1. Did the pre-trained model detect the aquarium animals correctly?\n",
    "2. What classes from COCO might it confuse with our classes?\n",
    "3. Why do we need to fine-tune instead of using the pre-trained model directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Fine-tune YOLO on Aquarium Data\n",
    "\n",
    "Now we'll fine-tune the pre-trained model on our specific dataset.\n",
    "\n",
    "**Transfer learning for detection:**\n",
    "- Keep the backbone features (edges, textures, shapes)\n",
    "- Adapt the detection head to our 7 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to fix the data.yaml paths for our current directory\n",
    "data_yaml_content = f\"\"\"train: train/images\n",
    "val: valid/images\n",
    "test: test/images\n",
    "\n",
    "nc: {num_classes}\n",
    "names: {class_names}\n",
    "\"\"\"\n",
    "\n",
    "with open('aquarium.yaml', 'w') as f:\n",
    "    f.write(data_yaml_content)\n",
    "\n",
    "print(\"Created aquarium.yaml:\")\n",
    "print(data_yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a fresh model for fine-tuning\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Fine-tune on our dataset\n",
    "# Note: In Colab with GPU, this takes ~5-10 minutes\n",
    "results = model.train(\n",
    "    data='aquarium.yaml',\n",
    "    epochs=30,           # Number of training epochs\n",
    "    imgsz=640,           # Image size\n",
    "    batch=16,            # Batch size (reduce if out of memory)\n",
    "    patience=10,         # Early stopping patience\n",
    "    device=0,            # GPU (use 'cpu' if no GPU)\n",
    "    verbose=True,\n",
    "    plots=True           # Generate training plots\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Training Metrics\n",
    "\n",
    "During training, YOLO reports several metrics:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **box_loss** | How well boxes match ground truth (lower = better) |\n",
    "| **cls_loss** | Classification accuracy (lower = better) |\n",
    "| **mAP50** | Mean Average Precision at IoU=0.5 (higher = better) |\n",
    "| **mAP50-95** | mAP averaged over IoU 0.5-0.95 (stricter metric) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training results\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "# Show training curves\n",
    "results_dir = Path(model.trainer.save_dir)\n",
    "print(f\"Results saved to: {results_dir}\")\n",
    "\n",
    "# Display training plots if they exist\n",
    "plots = ['results.png', 'confusion_matrix.png', 'F1_curve.png', 'PR_curve.png']\n",
    "for plot_name in plots:\n",
    "    plot_path = results_dir / plot_name\n",
    "    if plot_path.exists():\n",
    "        print(f\"\\n{plot_name}:\")\n",
    "        display(IPImage(filename=str(plot_path), width=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Evaluate the Fine-tuned Model\n",
    "\n",
    "Let's test our fine-tuned model on the validation set and compare to the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from training\n",
    "best_model_path = results_dir / 'weights' / 'best.pt'\n",
    "model_finetuned = YOLO(str(best_model_path))\n",
    "\n",
    "print(f\"Loaded fine-tuned model from: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "metrics = model_finetuned.val(data='aquarium.yaml', verbose=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"mAP50:    {metrics.box.map50:.3f}\")\n",
    "print(f\"mAP50-95: {metrics.box.map:.3f}\")\n",
    "print(\"\\nPer-class AP50:\")\n",
    "for i, ap in enumerate(metrics.box.ap50):\n",
    "    print(f\"  {class_names[i]:12s}: {ap:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Comparison: Before vs After Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image_path, model, ax, title):\n",
    "    \"\"\"Run model on image and visualize predictions.\"\"\"\n",
    "    results = model.predict(source=str(image_path), conf=0.25, verbose=False)\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "        conf = box.conf[0].cpu().numpy()\n",
    "        cls = int(box.cls[0].cpu().numpy())\n",
    "        cls_name = model.names[cls]\n",
    "        \n",
    "        color = colors[cls] if cls < len(colors) else 'red'\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                 linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-5, f'{cls_name} {conf:.2f}', color=color, fontsize=9,\n",
    "                fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{title}\\n({len(results[0].boxes)} detections)', fontsize=11)\n",
    "    return len(results[0].boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on validation images\n",
    "test_samples = random.sample(valid_images, 4)\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 18))\n",
    "\n",
    "for row, img_path in enumerate(test_samples):\n",
    "    # Ground truth\n",
    "    label_path = Path('valid/labels') / (img_path.stem + '.txt')\n",
    "    plot_image_with_boxes(img_path, label_path, ax=axes[row, 0])\n",
    "    axes[row, 0].set_title('Ground Truth', fontsize=11)\n",
    "    \n",
    "    # Pre-trained model\n",
    "    visualize_predictions(img_path, model_pretrained, axes[row, 1], 'Pre-trained YOLO')\n",
    "    \n",
    "    # Fine-tuned model\n",
    "    visualize_predictions(img_path, model_finetuned, axes[row, 2], 'Fine-tuned YOLO')\n",
    "\n",
    "plt.suptitle('Comparison: Ground Truth vs Pre-trained vs Fine-tuned', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Comparing the results:\n",
    "1. How much did fine-tuning improve detection accuracy?\n",
    "2. Which classes are detected best? Which are still challenging?\n",
    "3. Do you notice any false positives or false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Test on Unseen Images\n",
    "\n",
    "Let's evaluate on the test set (images the model has never seen during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "test_samples = random.sample(test_images, 6)\n",
    "\n",
    "for ax, img_path in zip(axes.flat, test_samples):\n",
    "    visualize_predictions(img_path, model_finetuned, ax, '')\n",
    "\n",
    "plt.suptitle('Fine-tuned Model - Test Set Predictions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Exercise - Experiment with Training\n",
    "\n",
    "Try modifying the training parameters to improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different training settings\n",
    "# Try modifying these parameters:\n",
    "\n",
    "# Option 1: Use a larger model (better accuracy, slower)\n",
    "# model = YOLO('yolov8s.pt')  # small instead of nano\n",
    "\n",
    "# Option 2: Train longer\n",
    "# epochs = 50  # instead of 30\n",
    "\n",
    "# Option 3: Different image size\n",
    "# imgsz = 800  # instead of 640\n",
    "\n",
    "# Option 4: Adjust learning rate\n",
    "# lr0 = 0.001  # initial learning rate\n",
    "\n",
    "# Uncomment and run to experiment:\n",
    "# model_exp = YOLO('yolov8s.pt')\n",
    "# results_exp = model_exp.train(\n",
    "#     data='aquarium.yaml',\n",
    "#     epochs=50,\n",
    "#     imgsz=640,\n",
    "#     batch=16,\n",
    "#     device=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "1. What is the trade-off between model size (nano/small/medium) and accuracy?\n",
    "2. How does the number of epochs affect overfitting?\n",
    "3. For a real scientific application, how would you decide which model to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Object detection** = classification + localization (multiple objects per image)\n",
    "\n",
    "2. **YOLO format**: `class_id x_center y_center width height` (normalized 0-1)\n",
    "\n",
    "3. **Fine-tuning strategy**:\n",
    "   - Start with pre-trained weights (COCO)\n",
    "   - Train on your domain-specific data\n",
    "   - Monitor mAP metrics during training\n",
    "\n",
    "4. **Key metrics**:\n",
    "   - **mAP50**: Standard detection metric (IoU threshold = 0.5)\n",
    "   - **mAP50-95**: Stricter metric (averaged over multiple IoU thresholds)\n",
    "\n",
    "5. **Model selection**:\n",
    "   - YOLOv8n: Fastest, least accurate\n",
    "   - YOLOv8s/m/l/x: Progressively more accurate but slower\n",
    "\n",
    "## Scientific Applications\n",
    "\n",
    "Object detection is used in many scientific domains:\n",
    "- **Biology**: Cell counting, animal tracking, species identification\n",
    "- **Medicine**: Tumor detection, organ localization in scans\n",
    "- **Ecology**: Wildlife monitoring, population surveys\n",
    "- **Astronomy**: Galaxy/star detection in telescope images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "Before finishing, think about:\n",
    "\n",
    "1. **In your research**: What objects would you want to detect? (cells, animals, particles, structures?)\n",
    "\n",
    "2. **Data annotation**: How would you create bounding box labels for your data? (manual annotation tools like LabelImg, CVAT)\n",
    "\n",
    "3. **Pre-trained models**: Would COCO pre-training help for your domain, or would you need domain-specific pre-training?\n",
    "\n",
    "4. **Deployment**: How would you use a trained detection model in practice? (batch processing, real-time video, edge devices?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
