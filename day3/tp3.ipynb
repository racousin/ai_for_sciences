{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3: Generative Models - Face Generation with DCGAN\n",
    "\n",
    "**Day 3 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day3/tp3.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this practical, you will understand:\n",
    "\n",
    "1. **How GANs work**: The adversarial game between Generator and Discriminator\n",
    "2. **DCGAN architecture**: Deep convolutional networks for image generation\n",
    "3. **Training dynamics**: Observing the Generator improve over epochs\n",
    "4. **Latent space**: How random noise maps to meaningful images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding GANs\n",
    "\n",
    "**Generative Adversarial Networks (GANs)** learn to generate new data by playing a game:\n",
    "\n",
    "```\n",
    "┌─────────────────┐                    ┌─────────────────┐\n",
    "│    Generator    │  generates fake →  │  Discriminator  │  → Real or Fake?\n",
    "│   (the artist)  │      images        │   (the critic)  │\n",
    "└─────────────────┘                    └─────────────────┘\n",
    "        ↑                                      ↓\n",
    "   random noise                         feedback to improve\n",
    "```\n",
    "\n",
    "- **Generator**: Takes random noise, produces fake images. Goal: fool the Discriminator.\n",
    "- **Discriminator**: Sees real and fake images. Goal: tell them apart.\n",
    "\n",
    "As training progresses, both networks improve. The Generator learns to create increasingly realistic images to fool an increasingly sophisticated Discriminator.\n",
    "\n",
    "**DCGAN** (Deep Convolutional GAN) uses convolutional layers for stable training on images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Data Preparation\n",
    "\n",
    "We'll use the **CelebA dataset** - 200,000 celebrity face images. The faces are cropped, resized to 64x64, and normalized to [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "image_size = 64\n",
    "batch_size = 128\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Download CelebA dataset (this may take a few minutes the first time)\n",
    "train_dataset = torchvision.datasets.CelebA(\n",
    "    root='./data',\n",
    "    split='train',\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_dataset):,}')\n",
    "print(f'Batches per epoch: {len(train_loader):,}')\n",
    "print(f'Image shape: {train_dataset[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize real samples\n",
    "samples = next(iter(train_loader))[0][:64]\n",
    "grid = make_grid(samples, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "plt.title('Real CelebA Face Images', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "1. Why do we normalize images to [-1, 1] instead of [0, 1]?\n",
    "2. What would happen if we used much smaller images (e.g., 16x16)? What about larger (256x256)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Building the DCGAN\n",
    "\n",
    "DCGAN uses specific architectural guidelines for stability:\n",
    "\n",
    "- **Convolutional layers** instead of fully connected layers\n",
    "- **Batch normalization** in both Generator and Discriminator\n",
    "- **ReLU** in Generator, **LeakyReLU** in Discriminator\n",
    "- **Tanh** output for Generator, **Sigmoid** for Discriminator\n",
    "\n",
    "## Generator Architecture\n",
    "\n",
    "The Generator transforms a random noise vector (latent space) into an image:\n",
    "\n",
    "```\n",
    "Noise (100D) → 4x4 → 8x8 → 16x16 → 32x32 → 64x64 RGB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN Generator for 64x64 RGB images.\n",
    "    \n",
    "    Takes a latent vector and progressively upsamples it to a full image.\n",
    "    Uses transposed convolutions (\"deconvolutions\") for upsampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=100, ngf=64):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: latent_dim x 1 x 1\n",
    "            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf*8) x 4 x 4\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf*4) x 8 x 8\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf*2) x 16 x 16\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf) x 32 x 32\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output: 3 x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        return self.main(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Architecture\n",
    "\n",
    "The Discriminator is the mirror image - it takes an image and outputs a probability (real or fake):\n",
    "\n",
    "```\n",
    "64x64 RGB → 32x32 → 16x16 → 8x8 → 4x4 → 1 (probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN Discriminator for 64x64 RGB images.\n",
    "    \n",
    "    Takes an image and outputs probability that it's real.\n",
    "    Uses strided convolutions for downsampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, ndf=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: 3 x 64 x 64\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: (ndf) x 32 x 32\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: (ndf*2) x 16 x 16\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: (ndf*4) x 8 x 8\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: (ndf*8) x 4 x 4\n",
    "\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # Output: 1 x 1 x 1\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.main(image).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator(latent_dim=100, ngf=64).to(device)\n",
    "discriminator = Discriminator(ndf=64).to(device)\n",
    "\n",
    "# Weight initialization (DCGAN paper recommendation)\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "print(f'Generator parameters: {sum(p.numel() for p in generator.parameters()):,}')\n",
    "print(f'Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "1. Why does the Generator use ReLU while the Discriminator uses LeakyReLU?\n",
    "2. Why is the final activation Tanh for the Generator and Sigmoid for the Discriminator?\n",
    "3. What does `ngf=64` control in the architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Training Setup\n",
    "\n",
    "The GAN training objective:\n",
    "\n",
    "- **Discriminator**: Maximize $\\log D(x) + \\log(1 - D(G(z)))$\n",
    "  - Correctly classify real images as real\n",
    "  - Correctly classify fake images as fake\n",
    "\n",
    "- **Generator**: Maximize $\\log D(G(z))$\n",
    "  - Make the Discriminator think fake images are real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Fixed noise for consistent visualization across epochs\n",
    "fixed_noise = torch.randn(64, generator.latent_dim, 1, 1).to(device)\n",
    "\n",
    "print('Training setup complete!')\n",
    "print(f'Fixed noise shape: {fixed_noise.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(generator, discriminator, loader, optimizer_g, optimizer_d, criterion, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    for real_images, _ in tqdm(loader, desc='Training'):\n",
    "        batch_size = real_images.size(0)\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        # Labels for real and fake\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # ============================================\n",
    "        # Train Discriminator\n",
    "        # ============================================\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # Loss on real images\n",
    "        real_output = discriminator(real_images)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "\n",
    "        # Loss on fake images\n",
    "        noise = torch.randn(batch_size, generator.latent_dim, 1, 1).to(device)\n",
    "        fake_images = generator(noise)\n",
    "        fake_output = discriminator(fake_images.detach())  # detach to not update G\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # ============================================\n",
    "        # Train Generator\n",
    "        # ============================================\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        # Generate new fake images\n",
    "        noise = torch.randn(batch_size, generator.latent_dim, 1, 1).to(device)\n",
    "        fake_images = generator(noise)\n",
    "        fake_output = discriminator(fake_images)\n",
    "\n",
    "        # Generator wants discriminator to think fakes are real\n",
    "        g_loss = criterion(fake_output, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        d_losses.append(d_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "    return np.mean(d_losses), np.mean(g_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "1. Why do we use `.detach()` when training the Discriminator on fake images?\n",
    "2. Why does the Generator use `real_labels` (ones) even though it's generating fake images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Training\n",
    "\n",
    "Watch the generated faces improve progressively! Early epochs show blurry shapes, later epochs show realistic facial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "sample_interval = 2  # Show samples every N epochs\n",
    "\n",
    "history = {'d_loss': [], 'g_loss': []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n=== Epoch {epoch+1}/{epochs} ===')\n",
    "\n",
    "    d_loss, g_loss = train_epoch(generator, discriminator, train_loader,\n",
    "                                  optimizer_g, optimizer_d, criterion, device)\n",
    "\n",
    "    history['d_loss'].append(d_loss)\n",
    "    history['g_loss'].append(g_loss)\n",
    "\n",
    "    print(f'D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}')\n",
    "\n",
    "    # Generate samples at intervals\n",
    "    if (epoch + 1) % sample_interval == 0 or epoch == 0:\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            fake_images = generator(fixed_noise)\n",
    "\n",
    "        grid = make_grid(fake_images, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "        plt.title(f'Generated Faces - Epoch {epoch+1}', fontsize=16)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['d_loss'], label='Discriminator Loss', linewidth=2)\n",
    "plt.plot(history['g_loss'], label='Generator Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "1. What happened to the generated images from epoch 1 to epoch 20?\n",
    "2. Looking at the loss curves, what would indicate that training is stable?\n",
    "3. What might happen if the Discriminator becomes too good too fast?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Generating New Faces\n",
    "\n",
    "Now we can generate unlimited unique faces by sampling new random noise vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_faces(generator, num_samples=16):\n",
    "    \"\"\"Generate random faces.\"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_samples, generator.latent_dim, 1, 1).to(device)\n",
    "        generated = generator(noise)\n",
    "\n",
    "    grid = make_grid(generated, nrow=4, normalize=True, value_range=(-1, 1))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "    plt.title('Generated Faces', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate multiple batches\n",
    "print('Generating random faces...')\n",
    "for i in range(3):\n",
    "    print(f'\\nBatch {i+1}:')\n",
    "    generate_faces(generator, num_samples=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Generate a Large Gallery\n",
    "\n",
    "Generate a large grid showing the variety of faces the model can create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a gallery of 64 faces in an 8x8 grid\n",
    "num_faces = 64  # <-- Modify this!\n",
    "\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(num_faces, generator.latent_dim, 1, 1).to(device)\n",
    "    generated_faces = generator(noise)\n",
    "\n",
    "grid = make_grid(generated_faces, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "plt.title(f'Generated Face Gallery ({num_faces} unique faces)', fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Exploring the Latent Space\n",
    "\n",
    "The **latent space** is the 100-dimensional space from which we sample noise. One fascinating property: nearby points in latent space produce similar images!\n",
    "\n",
    "We can **interpolate** between two points to see smooth transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_latent(generator, num_steps=10):\n",
    "    \"\"\"\n",
    "    Interpolate between two random latent vectors.\n",
    "    Shows smooth transitions between faces.\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "\n",
    "    # Two random starting points\n",
    "    z1 = torch.randn(1, generator.latent_dim, 1, 1).to(device)\n",
    "    z2 = torch.randn(1, generator.latent_dim, 1, 1).to(device)\n",
    "\n",
    "    interpolations = []\n",
    "    with torch.no_grad():\n",
    "        for alpha in torch.linspace(0, 1, num_steps):\n",
    "            z = (1 - alpha) * z1 + alpha * z2\n",
    "            img = generator(z)\n",
    "            interpolations.append(img)\n",
    "\n",
    "    interpolations = torch.cat(interpolations)\n",
    "    grid = make_grid(interpolations, nrow=num_steps, normalize=True, value_range=(-1, 1))\n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "    plt.title('Latent Space Interpolation (smooth transitions)', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show multiple interpolations\n",
    "print('Watch faces morph smoothly!')\n",
    "for i in range(3):\n",
    "    print(f'\\nInterpolation {i+1}:')\n",
    "    interpolate_latent(generator, num_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Longer Interpolations\n",
    "\n",
    "Try creating longer interpolations with more steps to see finer transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a longer interpolation with 20 steps\n",
    "num_steps = 10  # <-- Modify this!\n",
    "\n",
    "interpolate_latent(generator, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "1. What does smooth interpolation tell us about the latent space structure?\n",
    "2. If two random points always produce smooth transitions, what does that suggest about the Generator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Exercises\n",
    "\n",
    "## Exercise 3: Modify the Architecture\n",
    "\n",
    "The `ngf` parameter controls the number of filters. Try changing it and observe the effect on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a smaller generator with ngf=32 instead of 64\n",
    "# Compare the number of parameters\n",
    "\n",
    "ngf_small = 32  # <-- Modify this!\n",
    "\n",
    "generator_small = Generator(latent_dim=100, ngf=ngf_small).to(device)\n",
    "generator_large = Generator(latent_dim=100, ngf=64).to(device)\n",
    "\n",
    "params_small = sum(p.numel() for p in generator_small.parameters())\n",
    "params_large = sum(p.numel() for p in generator_large.parameters())\n",
    "\n",
    "print(f'Small Generator (ngf={ngf_small}): {params_small:,} parameters')\n",
    "print(f'Large Generator (ngf=64): {params_large:,} parameters')\n",
    "print(f'Reduction: {(1 - params_small/params_large)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Effect of Latent Dimension\n",
    "\n",
    "The latent dimension (default 100) controls the \"information capacity\" of the noise. Try different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create generators with different latent dimensions\n",
    "# and compare their parameter counts\n",
    "\n",
    "latent_dims = [10, 50, 100, 200]  # <-- Modify this!\n",
    "\n",
    "for latent_dim in latent_dims:\n",
    "    gen = Generator(latent_dim=latent_dim, ngf=64)\n",
    "    params = sum(p.numel() for p in gen.parameters())\n",
    "    print(f'Latent dim {latent_dim:3d}: {params:,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **GANs** consist of two networks in competition:\n",
    "   - Generator creates fake images\n",
    "   - Discriminator distinguishes real from fake\n",
    "\n",
    "2. **DCGAN** uses convolutional layers for stable image generation:\n",
    "   - Transposed convolutions for upsampling\n",
    "   - Batch normalization for stability\n",
    "   - Specific activations (ReLU/LeakyReLU/Tanh/Sigmoid)\n",
    "\n",
    "3. **Training dynamics**:\n",
    "   - Both networks improve together\n",
    "   - Balance is crucial - neither should dominate\n",
    "   - Visible quality improvement over epochs\n",
    "\n",
    "4. **Latent space**:\n",
    "   - Random noise maps to realistic images\n",
    "   - Smooth interpolation shows learned structure\n",
    "   - Each dimension may capture some aspect of variation\n",
    "\n",
    "## What We Didn't Cover\n",
    "\n",
    "- **Mode collapse**: When Generator produces limited variety\n",
    "- **Wasserstein GAN**: More stable training loss\n",
    "- **Conditional GAN**: Control what to generate\n",
    "- **StyleGAN**: State-of-the-art face generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **For your research domain**: What could GANs generate? (synthetic data, augmentation, simulation?)\n",
    "\n",
    "2. **Data augmentation**: How could generated data help train other models in your field?\n",
    "\n",
    "3. **Ethical considerations**: What are the risks of generative models that create realistic fake data?\n",
    "\n",
    "4. **Beyond images**: GANs can generate molecules, protein structures, time series. What would be most useful for your research?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
