{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2: Fine-Tuning for Medical Image Classification\n",
    "\n",
    "**Day 3 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day3/tp2.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this exercise, you will apply transfer learning to a **real scientific task**: classifying blood cells from microscopy images.\n",
    "\n",
    "By the end of this practical, you will:\n",
    "\n",
    "1. **Apply fine-tuning** to a medical imaging dataset (BloodMNIST)\n",
    "2. **Compare approaches**: frozen backbone vs full fine-tuning\n",
    "3. **Analyze results**: understand when each approach works best\n",
    "4. **Evaluate models**: using accuracy, confusion matrix, and per-class metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Dataset - Blood Cell Classification\n",
    "\n",
    "## About BloodMNIST\n",
    "\n",
    "**BloodMNIST** is a dataset of **17,092 microscopy images** of individual white blood cells:\n",
    "\n",
    "- 8 classes of blood cells (basophil, eosinophil, erythroblast, etc.)\n",
    "- Image size: 28x28 pixels (we'll resize to 224x224 for ResNet)\n",
    "- Clinical relevance: automated blood cell counting aids diagnosis\n",
    "\n",
    "```\n",
    "Blood Cell Types:\n",
    "├── Basophil          (class 0)\n",
    "├── Eosinophil        (class 1)  \n",
    "├── Erythroblast      (class 2)\n",
    "├── Immature granulocyte (class 3)\n",
    "├── Lymphocyte        (class 4)\n",
    "├── Monocyte          (class 5)\n",
    "├── Neutrophil        (class 6)\n",
    "└── Platelet          (class 7)\n",
    "```\n",
    "\n",
    "This is a realistic scientific dataset where transfer learning can help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "!pip install -q medmnist\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"MedMNIST version: {medmnist.__version__}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BloodMNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "data_flag = 'bloodmnist'\n",
    "info = INFO[data_flag]\n",
    "n_classes = len(info['label'])\n",
    "class_names = list(info['label'].values())\n",
    "\n",
    "print(f\"Dataset: {info['description']}\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"\\nClass names:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms - resize to 224x224 for pre-trained models\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "train_dataset = DataClass(split='train', transform=transform, download=True)\n",
    "val_dataset = DataClass(split='val', transform=transform, download=True)\n",
    "test_dataset = DataClass(split='test', transform=transform, download=True)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training:   {len(train_dataset):,} samples\")\n",
    "print(f\"  Validation: {len(val_dataset):,} samples\")\n",
    "print(f\"  Test:       {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to denormalize images for visualization\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return (tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "# Show samples from each class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "# Get one sample per class\n",
    "samples_per_class = {i: None for i in range(n_classes)}\n",
    "for img, label in train_dataset:\n",
    "    label_idx = label.item()\n",
    "    if samples_per_class[label_idx] is None:\n",
    "        samples_per_class[label_idx] = img\n",
    "    if all(v is not None for v in samples_per_class.values()):\n",
    "        break\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = denormalize(samples_per_class[i])\n",
    "    ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "    ax.set_title(class_names[i], fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Blood Cell Types', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Look at the cell images:\n",
    "1. Can you visually distinguish between the different cell types?\n",
    "2. Which classes look most similar to each other?\n",
    "3. Why might this be a challenging classification task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Training Utilities\n",
    "\n",
    "We'll reuse the training functions from TP1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(loader, desc='Training', leave=False):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.squeeze().long().to(device)  # MedMNIST labels need squeeze\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.squeeze().long().to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total, all_preds, all_labels\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters.\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Exercise - Frozen Backbone\n",
    "\n",
    "Your first task: train a ResNet18 with a **frozen backbone** on the blood cell dataset.\n",
    "\n",
    "This approach is ideal when:\n",
    "- You have limited data\n",
    "- You want fast training\n",
    "- The source domain (ImageNet) shares some features with your domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a ResNet18 model with frozen backbone\n",
    "# Step 1: Load pre-trained ResNet18\n",
    "model_frozen = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Step 2: Freeze all parameters\n",
    "# TODO: Complete this loop  # <-- Modify this!\n",
    "for param in model_frozen.parameters():\n",
    "    pass  # What should go here?\n",
    "\n",
    "# Step 3: Replace the classifier for 8 classes\n",
    "# TODO: Replace the fc layer  # <-- Modify this!\n",
    "# Hint: model_frozen.fc = nn.Linear(..., n_classes)\n",
    "\n",
    "model_frozen = model_frozen.to(device)\n",
    "\n",
    "trainable, total = count_parameters(model_frozen)\n",
    "print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the frozen model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_frozen.fc.parameters(), lr=0.001)  # Only train classifier\n",
    "\n",
    "EPOCHS = 5\n",
    "history_frozen = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "print(\"Training ResNet18 (frozen backbone)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model_frozen, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _ = evaluate(model_frozen, val_loader, criterion, device)\n",
    "    \n",
    "    history_frozen['train_acc'].append(train_acc)\n",
    "    history_frozen['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} | Train: {train_acc:.1f}% | Val: {val_acc:.1f}%')\n",
    "\n",
    "# Final test evaluation\n",
    "_, test_acc_frozen, preds_frozen, labels_frozen = evaluate(model_frozen, test_loader, criterion, device)\n",
    "print(f'\\nTest Accuracy (frozen): {test_acc_frozen:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Exercise - Full Fine-Tuning\n",
    "\n",
    "Now train a ResNet18 with **full fine-tuning** - all layers are trainable.\n",
    "\n",
    "Remember the key tips:\n",
    "- Use a **lower learning rate** (10x lower than from scratch)\n",
    "- All layers will adapt to the new domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a ResNet18 model for full fine-tuning\n",
    "model_finetune = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# TODO: Replace the classifier (but DON'T freeze any layers!)  # <-- Modify this!\n",
    "# model_finetune.fc = ...\n",
    "\n",
    "model_finetune = model_finetune.to(device)\n",
    "\n",
    "trainable, total = count_parameters(model_finetune)\n",
    "print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train with full fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: What learning rate should you use?  # <-- Modify this!\n",
    "# Hint: Lower than 0.001!\n",
    "optimizer = optim.Adam(model_finetune.parameters(), lr=0.001)  # <-- Modify this!\n",
    "\n",
    "history_finetune = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "print(\"Training ResNet18 (full fine-tuning)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model_finetune, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _ = evaluate(model_finetune, val_loader, criterion, device)\n",
    "    \n",
    "    history_finetune['train_acc'].append(train_acc)\n",
    "    history_finetune['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} | Train: {train_acc:.1f}% | Val: {val_acc:.1f}%')\n",
    "\n",
    "# Final test evaluation\n",
    "_, test_acc_finetune, preds_finetune, labels_finetune = evaluate(model_finetune, test_loader, criterion, device)\n",
    "print(f'\\nTest Accuracy (fine-tuned): {test_acc_finetune:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Compare Results\n",
    "\n",
    "Let's visualize and compare both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "# Training accuracy\n",
    "axes[0].plot(epochs_range, history_frozen['train_acc'], 'g-o', label='Frozen', linewidth=2)\n",
    "axes[0].plot(epochs_range, history_finetune['train_acc'], 'r-^', label='Fine-tuned', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Accuracy (%)')\n",
    "axes[0].set_title('Training Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "axes[1].plot(epochs_range, history_frozen['val_acc'], 'g-o', label='Frozen', linewidth=2)\n",
    "axes[1].plot(epochs_range, history_finetune['val_acc'], 'r-^', label='Fine-tuned', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST ACCURACY COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Frozen backbone:  {test_acc_frozen:.1f}%\")\n",
    "print(f\"Full fine-tuning: {test_acc_finetune:.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis\n",
    "\n",
    "Let's see which cell types are most often confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for both models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, preds, labels, title in [\n",
    "    (axes[0], preds_frozen, labels_frozen, 'Frozen Backbone'),\n",
    "    (axes[1], preds_finetune, labels_finetune, 'Full Fine-tuning')\n",
    "]:\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    # Normalize by row (true labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=[c[:6] for c in class_names],\n",
    "                yticklabels=[c[:6] for c in class_names],\n",
    "                ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(f'{title}\\n(normalized by row)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics for the best model\n",
    "best_preds = preds_finetune if test_acc_finetune > test_acc_frozen else preds_frozen\n",
    "best_labels = labels_finetune if test_acc_finetune > test_acc_frozen else labels_frozen\n",
    "best_name = \"Fine-tuned\" if test_acc_finetune > test_acc_frozen else \"Frozen\"\n",
    "\n",
    "print(f\"\\nDetailed Classification Report ({best_name} model):\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(best_labels, best_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Looking at the confusion matrices:\n",
    "1. Which cell types are most often confused with each other?\n",
    "2. Does full fine-tuning improve classification for the difficult classes?\n",
    "3. What biological reason might explain why certain classes are confused?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Bonus Exercise - Data Augmentation\n",
    "\n",
    "Data augmentation can improve generalization, especially with limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a transform with data augmentation\n",
    "# Experiment with different augmentations!\n",
    "\n",
    "transform_augmented = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    # TODO: Add augmentations here  # <-- Modify this!\n",
    "    # Suggestions:\n",
    "    # - transforms.RandomHorizontalFlip()\n",
    "    # - transforms.RandomRotation(10)\n",
    "    # - transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Transform with augmentation created!\")\n",
    "print(\"\\nTo use it, create new datasets with this transform:\")\n",
    "print(\"  train_dataset_aug = DataClass(split='train', transform=transform_augmented, download=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "1. Which augmentations make sense for microscopy images? (Think about what variations occur naturally)\n",
    "2. Which augmentations would NOT make sense? (e.g., would vertical flip be appropriate?)\n",
    "3. How could you validate that augmentation helps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Bonus - Try Another Medical Dataset\n",
    "\n",
    "MedMNIST includes many medical imaging datasets. You can easily try another one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available MedMNIST datasets\n",
    "print(\"Available MedMNIST datasets:\\n\")\n",
    "for flag, info in INFO.items():\n",
    "    print(f\"{flag:20s}: {info['description'][:60]}...\")\n",
    "    print(f\"{'':20s}  Classes: {len(info['label'])}, Task: {info['task']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try a different dataset!\n",
    "# Change data_flag to try another dataset:\n",
    "# - 'dermamnist': Skin lesion images (7 classes)\n",
    "# - 'pathmnist': Colon pathology (9 classes)\n",
    "# - 'chestmnist': Chest X-ray (14 classes, multi-label)\n",
    "# - 'retinamnist': Retinal OCT (5 classes)\n",
    "\n",
    "# new_data_flag = 'dermamnist'  # <-- Modify this!\n",
    "# new_info = INFO[new_data_flag]\n",
    "# print(f\"Dataset: {new_info['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "1. **Applied transfer learning** to a real medical imaging task\n",
    "\n",
    "2. **Compared approaches**:\n",
    "   - Frozen backbone: Fast training, good for limited data\n",
    "   - Full fine-tuning: Better accuracy, adapts features to new domain\n",
    "\n",
    "3. **Analyzed results** using confusion matrices and per-class metrics\n",
    "\n",
    "4. **Key findings**:\n",
    "   - Pre-trained ImageNet features transfer to medical images!\n",
    "   - Some cell types are inherently harder to distinguish\n",
    "   - Fine-tuning typically improves performance, especially on difficult classes\n",
    "\n",
    "## For Your Research\n",
    "\n",
    "- **Medical imaging**: Consider fine-tuning from RadImageNet or other medical pre-trained models\n",
    "- **Microscopy**: Transfer from ImageNet often works surprisingly well\n",
    "- **Limited data**: Start with frozen backbone, add augmentation\n",
    "- **Different modalities**: Domain-specific pre-training helps (X-ray, MRI, microscopy, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **Your data**: Could you apply similar techniques to images in your research?\n",
    "\n",
    "2. **Domain gap**: How different is your imaging modality from natural images? Does transfer still help?\n",
    "\n",
    "3. **Class imbalance**: Many scientific datasets have imbalanced classes. How would you handle this?\n",
    "\n",
    "4. **Interpretation**: How would you explain model predictions to domain experts?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
