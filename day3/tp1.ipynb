{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2: Fine-Tuning for Medical Image Classification\n",
    "\n",
    "**Day 3 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day3/tp2.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this exercise, you will apply transfer learning to a **real scientific task**: classifying blood cells from microscopy images.\n",
    "\n",
    "By the end of this practical, you will:\n",
    "\n",
    "1. **Apply fine-tuning** to a medical imaging dataset (BloodMNIST)\n",
    "2. **Compare approaches**: frozen backbone vs full fine-tuning\n",
    "3. **Analyze results**: understand when each approach works best\n",
    "4. **Evaluate models**: using accuracy, confusion matrix, and per-class metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Power of Pre-trained Models\n",
    "\n",
    "## The Problem: Training From Scratch is Hard\n",
    "\n",
    "Training a neural network from scratch requires:\n",
    "- **Large datasets**: Millions of labeled examples\n",
    "- **Significant compute**: Days or weeks of GPU time\n",
    "- **Careful tuning**: Learning rate, architecture, regularization\n",
    "\n",
    "**But what if you only have 1,000 samples?** This is the reality for most scientific applications.\n",
    "\n",
    "## The Solution: Transfer Learning\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                     PRE-TRAINED MODEL                          │\n",
    "│   (trained on ImageNet: 14M images, 1000 classes)              │\n",
    "│                                                                │\n",
    "│   ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐    │\n",
    "│   │ Conv 1  │ →  │ Conv 2  │ →  │  ...    │ →  │ Classifier│   │\n",
    "│   │ (edges) │    │(textures)│   │(objects)│    │(1000 cls) │   │\n",
    "│   └─────────┘    └─────────┘    └─────────┘    └─────────┘    │\n",
    "│      Generic features ────────────────→ Task-specific         │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "                    Replace classifier\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                      YOUR MODEL                                 │\n",
    "│   (fine-tuned on your data: 1,000 images, 8 classes)           │\n",
    "│                                                                │\n",
    "│   ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐    │\n",
    "│   │ Conv 1  │ →  │ Conv 2  │ →  │  ...    │ →  │ Classifier│   │\n",
    "│   │ (edges) │    │(textures)│   │(objects)│    │(8 cls)   │   │\n",
    "│   └─────────┘    └─────────┘    └─────────┘    └─────────┘    │\n",
    "│   Reuse learned features              New classifier          │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "> **Key insight**: Early layers learn generic features (edges, textures) that transfer across tasks. Only later layers are task-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "!pip install -q medmnist\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"MedMNIST version: {medmnist.__version__}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BloodMNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "data_flag = 'bloodmnist'\n",
    "info = INFO[data_flag]\n",
    "n_classes = len(info['label'])\n",
    "class_names = list(info['label'].values())\n",
    "\n",
    "print(f\"Dataset: {info['description']}\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"\\nClass names:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms - resize to 224x224 for pre-trained models\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "train_dataset = DataClass(split='train', transform=transform, download=True)\n",
    "val_dataset = DataClass(split='val', transform=transform, download=True)\n",
    "test_dataset = DataClass(split='test', transform=transform, download=True)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training:   {len(train_dataset):,} samples\")\n",
    "print(f\"  Validation: {len(val_dataset):,} samples\")\n",
    "print(f\"  Test:       {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to denormalize images for visualization\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return (tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "# Show samples from each class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "# Get one sample per class\n",
    "samples_per_class = {i: None for i in range(n_classes)}\n",
    "for img, label in train_dataset:\n",
    "    label_idx = label.item()\n",
    "    if samples_per_class[label_idx] is None:\n",
    "        samples_per_class[label_idx] = img\n",
    "    if all(v is not None for v in samples_per_class.values()):\n",
    "        break\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = denormalize(samples_per_class[i])\n",
    "    ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "    ax.set_title(class_names[i], fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Blood Cell Types', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Look at the cell images:\n",
    "1. Can you visually distinguish between the different cell types?\n",
    "2. Which classes look most similar to each other?\n",
    "3. Why might this be a challenging classification task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Training Utilities\n",
    "\n",
    "We'll reuse the training functions from TP1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(loader, desc='Training', leave=False):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.squeeze().long().to(device)  # MedMNIST labels need squeeze\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.squeeze().long().to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total, all_preds, all_labels\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters.\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "With ~17,000 training images and 10 epochs, we get reasonable accuracy. But what if you only had 500 images? (Common in specialized scientific applications!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "\n",
    "# Part 4: Approach 2 - Feature Extraction (Frozen Backbone)\n",
    "\n",
    "Now let's use a pre-trained ResNet18 and **freeze** all layers except the final classifier.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│              ResNet18 (pre-trained)             │\n",
    "│                                                 │\n",
    "│   ┌─────────────────────────┐   ┌───────────┐  │\n",
    "│   │   Feature Extractor     │   │ Classifier │  │\n",
    "│   │   (FROZEN - 11M params) │ → │ (TRAINABLE)│  │\n",
    "│   │   Keep ImageNet weights │   │ 4K params  │  │\n",
    "│   └─────────────────────────┘   └───────────┘  │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "> **When to use**: Very small dataset, or domain similar to ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the frozen model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_frozen.fc.parameters(), lr=0.001)  # Only train classifier\n",
    "\n",
    "EPOCHS = 5\n",
    "history_frozen = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "print(\"Training ResNet18 (frozen backbone)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model_frozen, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _ = evaluate(model_frozen, val_loader, criterion, device)\n",
    "    \n",
    "    history_frozen['train_acc'].append(train_acc)\n",
    "    history_frozen['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} | Train: {train_acc:.1f}% | Val: {val_acc:.1f}%')\n",
    "\n",
    "# Final test evaluation\n",
    "_, test_acc_frozen, preds_frozen, labels_frozen = evaluate(model_frozen, test_loader, criterion, device)\n",
    "print(f'\\nTest Accuracy (frozen): {test_acc_frozen:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Exercise - Full Fine-Tuning\n",
    "\n",
    "Now train a ResNet18 with **full fine-tuning** - all layers are trainable.\n",
    "\n",
    "Remember the key tips:\n",
    "- Use a **lower learning rate** (10x lower than from scratch)\n",
    "- All layers will adapt to the new domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2\n",
    "\n",
    "Compare the frozen ResNet18 to SimpleCNN from scratch:\n",
    "1. How do the accuracies compare?\n",
    "2. How many trainable parameters does each have?\n",
    "3. Why does frozen ResNet18 work well despite training only ~4K parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train with full fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: What learning rate should you use?  # <-- Modify this!\n",
    "# Hint: Lower than 0.001!\n",
    "optimizer = optim.Adam(model_finetune.parameters(), lr=0.001)  # <-- Modify this!\n",
    "\n",
    "history_finetune = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "print(\"Training ResNet18 (full fine-tuning)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model_finetune, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _ = evaluate(model_finetune, val_loader, criterion, device)\n",
    "    \n",
    "    history_finetune['train_acc'].append(train_acc)\n",
    "    history_finetune['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} | Train: {train_acc:.1f}% | Val: {val_acc:.1f}%')\n",
    "\n",
    "# Final test evaluation\n",
    "_, test_acc_finetune, preds_finetune, labels_finetune = evaluate(model_finetune, test_loader, criterion, device)\n",
    "print(f'\\nTest Accuracy (fine-tuned): {test_acc_finetune:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Compare Results\n",
    "\n",
    "Let's visualize and compare both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "# Training accuracy\n",
    "axes[0].plot(epochs_range, history_frozen['train_acc'], 'g-o', label='Frozen', linewidth=2)\n",
    "axes[0].plot(epochs_range, history_finetune['train_acc'], 'r-^', label='Fine-tuned', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Accuracy (%)')\n",
    "axes[0].set_title('Training Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "axes[1].plot(epochs_range, history_frozen['val_acc'], 'g-o', label='Frozen', linewidth=2)\n",
    "axes[1].plot(epochs_range, history_finetune['val_acc'], 'r-^', label='Fine-tuned', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST ACCURACY COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Frozen backbone:  {test_acc_frozen:.1f}%\")\n",
    "print(f\"Full fine-tuning: {test_acc_finetune:.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis\n",
    "\n",
    "Let's see which cell types are most often confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for both models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, preds, labels, title in [\n",
    "    (axes[0], preds_frozen, labels_frozen, 'Frozen Backbone'),\n",
    "    (axes[1], preds_finetune, labels_finetune, 'Full Fine-tuning')\n",
    "]:\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    # Normalize by row (true labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=[c[:6] for c in class_names],\n",
    "                yticklabels=[c[:6] for c in class_names],\n",
    "                ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(f'{title}\\n(normalized by row)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Approach':<25} {'Trainable Params':<18} {'Test Accuracy':>15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Note: These values will be dynamically computed above\n",
    "results = [\n",
    "    ('SimpleCNN (scratch)', f\"{count_parameters(model_scratch)[0]:,} (100%)\", f\"{history_scratch['test_acc'][-1]:.1f}%\"),\n",
    "    ('ResNet18 (frozen)', f\"{count_parameters(model_frozen)[0]:,} (0.04%)\", f\"{history_frozen['test_acc'][-1]:.1f}%\"),\n",
    "    ('ResNet18 (full fine-tune)', f\"{count_parameters(model_finetune)[0]:,} (100%)\", f\"{history_finetune['test_acc'][-1]:.1f}%\"),\n",
    "]\n",
    "\n",
    "for name, params, acc in results:\n",
    "    print(f\"{name:<25} {params:<18} {acc:>15}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Looking at the results:\n",
    "1. Which approach achieves the best accuracy?\n",
    "2. The frozen backbone trains only ~0.04% of parameters but gets good accuracy. Why?\n",
    "3. When would you choose frozen backbone over full fine-tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Bonus Exercise - Data Augmentation\n",
    "\n",
    "Data augmentation can improve generalization, especially with limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small subset (500 samples)\n",
    "subset_indices = np.random.choice(len(trainset_pretrained), 500, replace=False)\n",
    "small_trainset = Subset(trainset_pretrained, subset_indices)\n",
    "\n",
    "small_train_loader = DataLoader(small_trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"Small dataset: {len(small_trainset)} training samples\")\n",
    "print(f\"Original dataset: {len(trainset_pretrained)} training samples\")\n",
    "print(f\"This simulates having limited labeled medical data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "1. Which augmentations make sense for microscopy images? (Think about what variations occur naturally)\n",
    "2. Which augmentations would NOT make sense? (e.g., would vertical flip be appropriate?)\n",
    "3. How could you validate that augmentation helps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Create a frozen ResNet18 model\n",
    "# Hint: Copy the code from Part 4\n",
    "\n",
    "model_small_frozen = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# TODO: Freeze all layers\n",
    "# for param in model_small_frozen.parameters():\n",
    "#     ...  # <-- Modify this!\n",
    "\n",
    "# TODO: Replace the classifier for 8 classes\n",
    "# model_small_frozen.fc = ...  # <-- Modify this!\n",
    "\n",
    "model_small_frozen = model_small_frozen.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available MedMNIST datasets\n",
    "print(\"Available MedMNIST datasets:\\n\")\n",
    "for flag, info in INFO.items():\n",
    "    print(f\"{flag:20s}: {info['description'][:60]}...\")\n",
    "    print(f\"{'':20s}  Classes: {len(info['label'])}, Task: {info['task']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try a different dataset!\n",
    "# Change data_flag to try another dataset:\n",
    "# - 'dermamnist': Skin lesion images (7 classes)\n",
    "# - 'pathmnist': Colon pathology (9 classes)\n",
    "# - 'chestmnist': Chest X-ray (14 classes, multi-label)\n",
    "# - 'retinamnist': Retinal OCT (5 classes)\n",
    "\n",
    "# new_data_flag = 'dermamnist'  # <-- Modify this!\n",
    "# new_info = INFO[new_data_flag]\n",
    "# print(f\"Dataset: {new_info['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "1. **Applied transfer learning** to a real medical imaging task\n",
    "\n",
    "2. **Compared approaches**:\n",
    "   - Frozen backbone: Fast training, good for limited data\n",
    "   - Full fine-tuning: Better accuracy, adapts features to new domain\n",
    "\n",
    "3. **Analyzed results** using confusion matrices and per-class metrics\n",
    "\n",
    "4. **Key findings**:\n",
    "   - Pre-trained ImageNet features transfer to medical images!\n",
    "   - Some cell types are inherently harder to distinguish\n",
    "   - Fine-tuning typically improves performance, especially on difficult classes\n",
    "\n",
    "## For Your Research\n",
    "\n",
    "- **Medical imaging**: Consider fine-tuning from RadImageNet or other medical pre-trained models\n",
    "- **Microscopy**: Transfer from ImageNet often works surprisingly well\n",
    "- **Limited data**: Start with frozen backbone, add augmentation\n",
    "- **Different modalities**: Domain-specific pre-training helps (X-ray, MRI, microscopy, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "Before moving to the next practical, think about:\n",
    "\n",
    "1. **For your research**: Do you have a classification or detection task? How much labeled data do you have? (Like our blood cell classification example)\n",
    "\n",
    "2. **Domain similarity**: Are there pre-trained models for your domain? (Medical imaging, microscopy, satellite imagery, etc.)\n",
    "\n",
    "3. **Data augmentation**: What augmentations make sense for your data? (rotation, flipping, color jitter, etc.)\n",
    "\n",
    "4. **Evaluation**: What metrics matter for your task? (accuracy, precision, recall, AUC?)\n",
    "\n",
    "5. **Medical imaging context**: In the blood cell classification task, which cell types were hardest to distinguish? Why might this be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
