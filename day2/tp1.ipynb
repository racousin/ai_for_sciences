{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Understanding Tokenization\n",
    "\n",
    "**Day 2 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day2/tp1.ipynb)\n",
    "\n",
    "## Objectives\n",
    "1. Understand **why tokenization matters** for machine learning\n",
    "2. Compare tokenization strategies: character, word, and subword (BPE)\n",
    "3. Explore **domain-specific tokenizers** for molecules, DNA, and proteins\n",
    "4. Understand trade-offs for scientific applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "!pip install -q transformers sentencepiece\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from aiforscience import (\n",
    "    visualize_tokens,\n",
    "    compare_tokenizers,\n",
    "    tokenizer_stats,\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Why Tokenization Matters\n",
    "\n",
    "**The Problem:** Neural networks only understand numbers, not text.\n",
    "\n",
    "**Tokenization** converts text into a sequence of integers:\n",
    "\n",
    "```\n",
    "\"Hello world\" → [\"Hello\", \" world\"] → [15496, 995]\n",
    "```\n",
    "\n",
    "The choice of how to split text has major implications for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# See tokenization in action\n",
    "text = \"Machine learning transforms scientific research.\"\n",
    "visualize_tokens(text, tokenizer)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Tokenization Strategies\n",
    "\n",
    "| Strategy | How it works | Vocab Size | Sequence Length |\n",
    "|----------|--------------|------------|------------------|\n",
    "| **Character** | Each character = 1 token | ~100 | Very long |\n",
    "| **Word** | Each word = 1 token | 100K+ | Short |\n",
    "| **Subword (BPE)** | Frequent substrings = tokens | ~30-50K | Medium |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Photosynthesis occurs in chloroplasts.\"\n",
    "\n",
    "# Character-level\n",
    "char_tokens = list(text)\n",
    "print(f\"Character-level: {len(char_tokens)} tokens\")\n",
    "print(f\"  {char_tokens[:15]}...\\n\")\n",
    "\n",
    "# Word-level\n",
    "word_tokens = text.replace('.', ' .').split()\n",
    "print(f\"Word-level: {len(word_tokens)} tokens\")\n",
    "print(f\"  {word_tokens}\\n\")\n",
    "\n",
    "# Subword (BPE) - GPT-2\n",
    "subword_tokens = tokenizer.tokenize(text)\n",
    "print(f\"Subword (GPT-2): {len(subword_tokens)} tokens\")\n",
    "print(f\"  {subword_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight:** BPE keeps common words as single tokens but splits rare words into subwords.\n",
    "\n",
    "This allows handling **any** input while keeping sequences reasonably short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Explore Tokenization\n",
    "\n",
    "How does GPT-2 tokenize text from your domain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try your own scientific text!\n",
    "my_text = \"Your scientific text here\"  # <-- Modify this!\n",
    "\n",
    "visualize_tokens(my_text, tokenizer)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "1. Are technical terms from your field single tokens or split?\n",
    "2. What might this mean for a model's understanding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: How BPE Works\n",
    "\n",
    "**Byte Pair Encoding (BPE)** builds vocabulary by iteratively merging frequent character pairs:\n",
    "\n",
    "1. Start with individual characters\n",
    "2. Count all adjacent pairs\n",
    "3. Merge the most frequent pair → new token\n",
    "4. Repeat until vocabulary size reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def simple_bpe_demo(words, n_merges=5):\n",
    "    \"\"\"Demonstrate BPE algorithm.\"\"\"\n",
    "    # Initialize: each word split into characters\n",
    "    vocab = {word: list(word) + ['</w>'] for word in words}\n",
    "    \n",
    "    print(f\"Corpus: {words}\")\n",
    "    print(f\"Initial: {vocab}\\n\")\n",
    "    \n",
    "    for step in range(n_merges):\n",
    "        # Count pairs\n",
    "        pairs = Counter()\n",
    "        for word, tokens in vocab.items():\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pairs[(tokens[i], tokens[i+1])] += words.count(word)\n",
    "        \n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        # Find most frequent\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        print(f\"Step {step+1}: Merge '{best[0]}' + '{best[1]}' → '{best[0]+best[1]}' (count: {pairs[best]})\")\n",
    "        \n",
    "        # Apply merge\n",
    "        new_vocab = {}\n",
    "        for word, tokens in vocab.items():\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens)-1 and (tokens[i], tokens[i+1]) == best:\n",
    "                    new_tokens.append(tokens[i] + tokens[i+1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            new_vocab[word] = new_tokens\n",
    "        vocab = new_vocab\n",
    "    \n",
    "    print(f\"\\nFinal: {vocab}\")\n",
    "\n",
    "# Demo\n",
    "simple_bpe_demo(['low', 'lower', 'lowest', 'new', 'newer'], n_merges=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** BPE discovers common morphemes like `-er`, `-est` automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common words → single tokens\n",
    "print(\"Common words:\")\n",
    "for word in [\"the\", \"and\", \"computer\", \"science\"]:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    print(f\"  '{word}' → {tokens}\")\n",
    "\n",
    "print(\"\\nRare words (split into subwords):\")\n",
    "for word in [\"antidisestablishmentarianism\", \"electroencephalography\"]:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    print(f\"  '{word}' → {len(tokens)} tokens\")\n",
    "    print(f\"    {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Domain-Specific Tokenizers\n",
    "\n",
    "**Key insight:** Tokenizers trained on English text don't work well for scientific data!\n",
    "\n",
    "| Domain | Data | Specialized Tokenizer |\n",
    "|--------|------|----------------------|\n",
    "| Chemistry | SMILES strings | ChemBERTa |\n",
    "| Genomics | DNA (ATCG) | DNABERT (k-mers) |\n",
    "| Proteomics | Amino acids | ESM-2, ProtBERT |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Chemistry: SMILES Tokenization\n",
    "\n",
    "**SMILES** represents molecules as strings:\n",
    "- Aspirin: `CC(=O)OC1=CC=CC=C1C(=O)O`\n",
    "- Caffeine: `CN1C=NC2=C1C(=O)N(C(=O)N2C)C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chemistry tokenizer\n",
    "chem_tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "aspirin = \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
    "\n",
    "# Compare\n",
    "compare_tokenizers(aspirin, {\n",
    "    \"ChemBERTa (chemistry)\": chem_tokenizer,\n",
    "    \"GPT-2 (English)\": gpt2_tokenizer,\n",
    "})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChemBERTa understands chemical notation (`C`, `=O`, ring numbers) as meaningful units!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Genomics: DNA k-mer Tokenization\n",
    "\n",
    "DNA uses 4 bases: **A, T, C, G**\n",
    "\n",
    "**k-mer tokenization** splits into overlapping windows:\n",
    "```\n",
    "ATCGATCG → [ATC, TCG, CGA, GAT, ATC, TCG]  (3-mers)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmer_tokenize(sequence, k=3):\n",
    "    \"\"\"Split DNA sequence into k-mers.\"\"\"\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "dna = \"ATCGATCGATCGATCG\"\n",
    "\n",
    "for k in [3, 4, 6]:\n",
    "    kmers = kmer_tokenize(dna, k)\n",
    "    vocab_size = 4 ** k  # 4 bases, k positions\n",
    "    print(f\"{k}-mers: {len(kmers)} tokens, vocab size = {vocab_size}\")\n",
    "    print(f\"  {kmers[:6]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How GPT-2 sees DNA (not well!)\n",
    "dna = \"ATCGATCGATCGATCG\"\n",
    "visualize_tokens(dna, gpt2_tokenizer, title=\"GPT-2 on DNA\")\n",
    "plt.show()\n",
    "print(\"GPT-2 doesn't understand DNA structure!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Proteomics: Amino Acid Tokenization\n",
    "\n",
    "Proteins are sequences of **20 amino acids** (A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y).\n",
    "\n",
    "Protein models typically use **character-level tokenization** where each amino acid = 1 token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load protein tokenizer\n",
    "try:\n",
    "    protein_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "    tokenizer_name = \"ESM-2\"\n",
    "except:\n",
    "    protein_tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\")\n",
    "    tokenizer_name = \"ProtBERT\"\n",
    "\n",
    "# Insulin fragment\n",
    "insulin = \"MALWMRLLPLLALLALWGPDPAAA\"\n",
    "\n",
    "# ESM/ProtBERT expects spaces between amino acids\n",
    "spaced = \" \".join(list(insulin))\n",
    "tokens = protein_tokenizer.tokenize(spaced)\n",
    "\n",
    "print(f\"{tokenizer_name} tokenization of insulin fragment:\")\n",
    "print(f\"  Sequence: {insulin}\")\n",
    "print(f\"  Length: {len(insulin)} amino acids\")\n",
    "print(f\"  Tokens: {len(tokens)}\")\n",
    "print(f\"  → Each amino acid is one token!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Compare Tokenizers on Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers\n",
    "tokenizers = {\n",
    "    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"ChemBERTa\": AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\"),\n",
    "}\n",
    "\n",
    "# Test data\n",
    "test_samples = [\n",
    "    \"The mitochondria is the powerhouse of the cell.\",\n",
    "    \"CC(=O)OC1=CC=CC=C1C(=O)O\",  # Aspirin\n",
    "    \"ATCGATCGATCGATCG\",           # DNA\n",
    "    \"MVLSPADKTNVKAAWGKVGAHAGEY\",  # Protein\n",
    "]\n",
    "\n",
    "# Compare token counts\n",
    "print(\"Token counts by tokenizer:\")\n",
    "print(\"-\" * 50)\n",
    "for sample in test_samples:\n",
    "    short = sample[:30] + \"...\" if len(sample) > 30 else sample\n",
    "    print(f\"\\n{short}\")\n",
    "    for name, tok in tokenizers.items():\n",
    "        n_tokens = len(tok.tokenize(sample))\n",
    "        print(f\"  {name}: {n_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size comparison\n",
    "tokenizer_stats(tokenizers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Why This Matters\n",
    "\n",
    "## The Problem with General Tokenizers on Scientific Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific terms from different domains\n",
    "domains = {\n",
    "    \"Biology\": [\"mitochondria\", \"chloroplast\", \"photosynthesis\", \"ribosome\"],\n",
    "    \"Chemistry\": [\"stoichiometry\", \"electronegativity\", \"chromatography\"],\n",
    "    \"Physics\": [\"thermodynamics\", \"superconductivity\", \"electromagnetism\"],\n",
    "    \"Medicine\": [\"pharmacokinetics\", \"immunotherapy\", \"pathogenesis\"],\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\"How GPT-2 tokenizes scientific terms:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for domain, terms in domains.items():\n",
    "    print(f\"\\n{domain}:\")\n",
    "    for term in terms:\n",
    "        tokens = tokenizer.tokenize(term)\n",
    "        print(f\"  {term:25} → {len(tokens)} tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways:**\n",
    "\n",
    "1. General tokenizers fragment scientific terms into many subwords\n",
    "2. This means the model doesn't \"see\" these as single concepts\n",
    "3. Domain-specific tokenizers have vocabularies optimized for their field\n",
    "4. Using the right tokenizer can significantly improve model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Analyze Your Domain\n",
    "\n",
    "Add technical terms from your research field and see how they're tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add terms from your domain!\n",
    "my_domain = \"Your Field\"  # <-- Change this\n",
    "my_terms = [\n",
    "    \"term1\",  # <-- Add your terms\n",
    "    \"term2\",\n",
    "    \"term3\",\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(f\"\\n{my_domain}:\")\n",
    "for term in my_terms:\n",
    "    tokens = tokenizer.tokenize(term)\n",
    "    print(f\"  {term:25} → {len(tokens)} tokens: {tokens}\")\n",
    "\n",
    "# Calculate statistics\n",
    "total_tokens = sum(len(tokenizer.tokenize(t)) for t in my_terms)\n",
    "avg_tokens = total_tokens / len(my_terms)\n",
    "single_token = sum(1 for t in my_terms if len(tokenizer.tokenize(t)) == 1)\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Average tokens per term: {avg_tokens:.1f}\")\n",
    "print(f\"  Single-token terms: {single_token}/{len(my_terms)} ({100*single_token/len(my_terms):.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Tokenization is foundational** - it determines what the model \"sees\"\n",
    "\n",
    "2. **Three strategies:**\n",
    "   - Character: small vocab, long sequences\n",
    "   - Word: large vocab, OOV problems\n",
    "   - Subword (BPE): balanced trade-off\n",
    "\n",
    "3. **Domain matters:**\n",
    "   - GPT-2 fragments scientific terms\n",
    "   - ChemBERTa understands molecular notation\n",
    "   - DNABERT uses k-mers for genetic sequences\n",
    "   - ESM-2 tokenizes amino acids individually\n",
    "\n",
    "4. **Practical advice:**\n",
    "   - Use domain-specific models when available\n",
    "   - Check how your data is tokenized before training\n",
    "   - More tokens = more computation + harder learning\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. Are there domain-specific tokenizers/models for your research area?\n",
    "2. How might tokenization affect the results in your field?\n",
    "3. What trade-offs would you consider when choosing a tokenization strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next: TP2 - Embeddings\n",
    "\n",
    "In the next practical, we'll explore **embeddings** - the dense vector representations that come after tokenization. We'll see how domain-specific models create meaningful representations for molecules, proteins, DNA, and scientific text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
