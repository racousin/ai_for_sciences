{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Transformers - From Tokens to Language Understanding\n",
    "\n",
    "**Day 1 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day1/tp2.ipynb)\n",
    "\n",
    "## Objectives\n",
    "1. Understand how text is converted to numbers (tokenization)\n",
    "2. Explore the attention mechanism - the core of Transformers\n",
    "3. Use pre-trained models from Hugging Face\n",
    "4. Visualize word embeddings and their semantic relationships\n",
    "5. See how Transformers extend beyond text to molecules, signals, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to install and import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "!pip install -q transformers\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from aiforscience import (\n",
    "    plot_attention_weights,\n",
    "    plot_embeddings_2d,\n",
    "    print_tokenization,\n",
    "    print_model_summary,\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Text Representation - Tokenization\n",
    "\n",
    "**Key Question:** How do we convert text (strings) into numbers that neural networks can process?\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Neural networks work with numbers, not text. We need a way to:\n",
    "1. Break text into meaningful units (tokens)\n",
    "2. Convert each token to a unique number (token ID)\n",
    "3. Optionally convert IDs to dense vectors (embeddings)\n",
    "\n",
    "```\n",
    "\"Hello world\" → [\"Hello\", \"world\"] → [15496, 995] → [[0.1, -0.3, ...], [0.4, 0.2, ...]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Tokenization Strategies\n",
    "\n",
    "Let's explore different ways to tokenize text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Strategy 1: Character-level tokenization\n",
    "char_tokens = list(text)\n",
    "print(\"Character-level tokenization:\")\n",
    "print(f\"  Text: '{text}'\")\n",
    "print(f\"  Tokens: {char_tokens}\")\n",
    "print(f\"  Number of tokens: {len(char_tokens)}\")\n",
    "print(f\"  Vocabulary size: {len(set(char_tokens))} unique characters\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Strategy 2: Word-level tokenization\n",
    "word_tokens = text.lower().replace('.', '').split()\n",
    "print(\"Word-level tokenization:\")\n",
    "print(f\"  Text: '{text}'\")\n",
    "print(f\"  Tokens: {word_tokens}\")\n",
    "print(f\"  Number of tokens: {len(word_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade-offs\n",
    "\n",
    "| Strategy | Vocabulary Size | Sequence Length | Handles Unknown Words? |\n",
    "|----------|----------------|-----------------|------------------------|\n",
    "| Character | ~100 (small) | Very long | Yes (all chars known) |\n",
    "| Word | Very large | Short | No (OOV problem) |\n",
    "| **Subword (BPE)** | Medium (~50K) | Medium | Yes (breaks into subwords) |\n",
    "\n",
    "**Modern models use Subword tokenization** (like BPE or WordPiece) - a compromise between character and word level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Real Tokenizers\n",
    "\n",
    "Let's use the tokenizer from GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"GPT-2 Tokenization:\")\n",
    "print(f\"  Text: '{text}'\")\n",
    "print(f\"  Tokens: {tokens}\")\n",
    "print(f\"  Token IDs: {token_ids}\")\n",
    "print(f\"  Number of tokens: {len(tokens)}\")\n",
    "print(f\"  Vocabulary size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tokenization\n",
    "print_tokenization(text, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Explore Tokenization\n",
    "\n",
    "Try different texts and observe how they are tokenized:\n",
    "- Common words vs rare words\n",
    "- Scientific terms (like \"photosynthesis\" or \"mitochondria\")\n",
    "- Numbers and special characters\n",
    "- Words in different languages\n",
    "\n",
    "**Questions:**\n",
    "1. How are rare/scientific words handled?\n",
    "2. What happens with numbers?\n",
    "3. Are common words single tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different texts!\n",
    "test_texts = [\n",
    "    \"Hello world!\",                          # Simple\n",
    "    \"Photosynthesis occurs in chloroplasts\", # Scientific\n",
    "    \"The price is $123.45\",                  # Numbers\n",
    "    \"café résumé naïve\",                     # Accented characters\n",
    "    \"CRISPR-Cas9 gene editing\",              # Technical\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    print_tokenization(text, tokenizer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: The Attention Mechanism\n",
    "\n",
    "**Key Question:** How do Transformers understand relationships between words in a sentence?\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "Attention allows each word to \"look at\" other words in the sentence and decide which ones are most relevant.\n",
    "\n",
    "For example, in:\n",
    "> \"The cat sat on the mat because **it** was tired.\"\n",
    "\n",
    "The word \"it\" should attend to \"cat\" to understand what \"it\" refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Step by Step\n",
    "\n",
    "Given an input sequence, self-attention computes:\n",
    "\n",
    "1. **Query (Q)**: What am I looking for?\n",
    "2. **Key (K)**: What do I contain?\n",
    "3. **Value (V)**: What information do I provide?\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a simple self-attention mechanism\n",
    "class SimpleSelfAttention(nn.Module):\n",
    "    \"\"\"A simplified self-attention layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        Returns:\n",
    "            output: Attended output\n",
    "            attention_weights: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        # Project to Q, K, V\n",
    "        Q = self.query(x)  # (batch, seq_len, embed_dim)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # Compute attention scores: Q @ K^T / sqrt(d)\n",
    "        d_k = self.embed_dim ** 0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / d_k  # (batch, seq_len, seq_len)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Count parameters\n",
    "embed_dim = 64\n",
    "attention = SimpleSelfAttention(embed_dim)\n",
    "n_params = sum(p.numel() for p in attention.parameters())\n",
    "print(f\"Self-Attention Layer:\")\n",
    "print(f\"  Embedding dimension: {embed_dim}\")\n",
    "print(f\"  Total parameters: {n_params:,}\")\n",
    "print(f\"    - Query projection: {embed_dim * embed_dim} (W_Q)\")\n",
    "print(f\"    - Key projection: {embed_dim * embed_dim} (W_K)\")\n",
    "print(f\"    - Value projection: {embed_dim * embed_dim} (W_V)\")\n",
    "print(f\"    - Biases: {3 * embed_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention\n",
    "\n",
    "Let's see how attention works on a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example\n",
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "seq_len = len(sentence)\n",
    "\n",
    "# Create random embeddings for our words (in practice, these come from an embedding layer)\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1, seq_len, embed_dim)  # (batch=1, seq_len=6, embed_dim=64)\n",
    "\n",
    "# Apply self-attention\n",
    "attention = SimpleSelfAttention(embed_dim)\n",
    "output, attn_weights = attention(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "# Visualize attention weights\n",
    "plot_attention_weights(attn_weights[0].detach().numpy(), sentence, \n",
    "                       title=\"Self-Attention Weights (Random Init)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Complete Transformer Block\n",
    "\n",
    "A real Transformer layer combines:\n",
    "1. **Multi-Head Attention**: Multiple attention heads learning different relationships\n",
    "2. **Feed-Forward Network**: A small neural network applied to each position\n",
    "3. **Layer Normalization**: Stabilizes training\n",
    "4. **Residual Connections**: Help gradients flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single Transformer block.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create a Transformer block\n",
    "block = TransformerBlock(embed_dim=64, num_heads=4, ff_dim=256)\n",
    "\n",
    "# Count parameters\n",
    "print_model_summary(block, \"Transformer Block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Transformers Work Well for Sequences\n",
    "\n",
    "| Property | RNNs/LSTMs | Transformers |\n",
    "|----------|------------|---------------|\n",
    "| Parallelization | Sequential (slow) | Fully parallel (fast) |\n",
    "| Long-range dependencies | Difficult (vanishing gradients) | Easy (direct attention) |\n",
    "| Training speed | Slow | Fast (on GPU) |\n",
    "| Memory | O(1) per step | O(n²) for attention |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Modify the Transformer Block\n",
    "\n",
    "Try changing the hyperparameters and observe how the model size changes:\n",
    "- `embed_dim`: Try 128, 256, 512\n",
    "- `num_heads`: Try 2, 4, 8 (must divide embed_dim)\n",
    "- `ff_dim`: Try 512, 1024, 2048\n",
    "\n",
    "**Question:** How does GPT-3 (175B parameters) achieve its size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify these parameters\n",
    "embed_dim = 64   # <-- Try 128, 256, 512\n",
    "num_heads = 4    # <-- Try 2, 4, 8\n",
    "ff_dim = 256     # <-- Try 512, 1024\n",
    "\n",
    "block = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)\n",
    "print_model_summary(block, f\"Transformer Block (d={embed_dim}, h={num_heads}, ff={ff_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Using Pre-trained Models from Hugging Face\n",
    "\n",
    "**Key Insight:** Training a language model from scratch requires massive compute (GPT-3 cost ~$4.6M to train). Instead, we can use **pre-trained models**.\n",
    "\n",
    "Hugging Face Hub hosts thousands of pre-trained models: https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pre-trained GPT-2 Model\n",
    "\n",
    "GPT-2 is a decoder-only Transformer trained to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 (small version: 124M parameters)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Model info\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: {n_params:,} ({n_params/1e6:.1f}M)\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation with GPT-2\n",
    "\n",
    "GPT-2 generates text by predicting the next token, then using that to predict the next, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=50, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate text continuation from a prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting text\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        temperature: Higher = more creative, Lower = more deterministic\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and return\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "# Try it!\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"\\nGenerated text:\")\n",
    "print(\"-\" * 50)\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Experiment with Text Generation\n",
    "\n",
    "Try different prompts and parameters:\n",
    "\n",
    "**Prompts to try:**\n",
    "- Scientific: \"The experiment showed that\"\n",
    "- Creative: \"Once upon a time\"\n",
    "- Technical: \"To implement a neural network,\"\n",
    "\n",
    "**Parameters:**\n",
    "- `temperature`: 0.1 (conservative) to 1.5 (creative)\n",
    "- `max_new_tokens`: 20, 50, 100\n",
    "\n",
    "**Questions:**\n",
    "1. How does temperature affect the output?\n",
    "2. What are the limitations of GPT-2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try your own prompts!\n",
    "prompt = \"In machine learning, the most important concept is\"  # <-- Modify this!\n",
    "temperature = 0.7  # <-- Try 0.1, 0.5, 1.0, 1.5\n",
    "max_tokens = 50    # <-- Try 20, 50, 100\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(\"\\nGenerated:\")\n",
    "print(\"-\" * 50)\n",
    "print(generate_text(prompt, max_new_tokens=max_tokens, temperature=temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different temperatures\n",
    "prompt = \"The best way to learn programming is\"\n",
    "\n",
    "for temp in [0.1, 0.7, 1.5]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(generate_text(prompt, max_new_tokens=40, temperature=temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Word Embeddings\n",
    "\n",
    "**Key Question:** How do neural networks represent the meaning of words?\n",
    "\n",
    "## From IDs to Vectors\n",
    "\n",
    "Each token ID is mapped to a dense vector (embedding). These embeddings capture semantic relationships:\n",
    "\n",
    "- Similar words have similar embeddings\n",
    "- Relationships are encoded as directions: king - man + woman ≈ queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding layer from GPT-2\n",
    "embedding_layer = model.transformer.wte  # word token embeddings\n",
    "\n",
    "print(f\"Embedding layer:\")\n",
    "print(f\"  Vocabulary size: {embedding_layer.num_embeddings:,}\")\n",
    "print(f\"  Embedding dimension: {embedding_layer.embedding_dim}\")\n",
    "print(f\"  Total parameters: {embedding_layer.num_embeddings * embedding_layer.embedding_dim:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    \"\"\"Get the embedding for a word.\"\"\"\n",
    "    token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "    if len(token_ids) > 1:\n",
    "        print(f\"Note: '{word}' is split into {len(token_ids)} tokens\")\n",
    "    \n",
    "    # Get embeddings for each token and average\n",
    "    embeddings = embedding_layer(torch.tensor(token_ids))\n",
    "    return embeddings.mean(dim=0).detach()\n",
    "\n",
    "# Get embeddings for some words\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"cat\", \"dog\"]\n",
    "embeddings = {word: get_word_embedding(word) for word in words}\n",
    "\n",
    "print(\"Embedding shapes:\")\n",
    "for word, emb in embeddings.items():\n",
    "    print(f\"  {word}: {emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Similarity\n",
    "\n",
    "Cosine similarity measures how similar two embeddings are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()\n",
    "\n",
    "# Compare similarities\n",
    "print(\"Cosine Similarities:\")\n",
    "print(f\"  king - queen:  {cosine_similarity(embeddings['king'], embeddings['queen']):.4f}\")\n",
    "print(f\"  king - man:    {cosine_similarity(embeddings['king'], embeddings['man']):.4f}\")\n",
    "print(f\"  king - cat:    {cosine_similarity(embeddings['king'], embeddings['cat']):.4f}\")\n",
    "print(f\"  cat - dog:     {cosine_similarity(embeddings['cat'], embeddings['dog']):.4f}\")\n",
    "print(f\"  man - woman:   {cosine_similarity(embeddings['man'], embeddings['woman']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings in 2D\n",
    "\n",
    "We can use dimensionality reduction (PCA or t-SNE) to visualize embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get embeddings for related words\n",
    "word_groups = {\n",
    "    \"Royalty\": [\"king\", \"queen\", \"prince\", \"princess\", \"crown\"],\n",
    "    \"Animals\": [\"cat\", \"dog\", \"bird\", \"fish\", \"mouse\"],\n",
    "    \"Science\": [\"physics\", \"chemistry\", \"biology\", \"math\", \"science\"],\n",
    "    \"Programming\": [\"code\", \"program\", \"software\", \"computer\", \"algorithm\"],\n",
    "}\n",
    "\n",
    "all_words = []\n",
    "all_embeddings = []\n",
    "all_categories = []\n",
    "\n",
    "for category, words in word_groups.items():\n",
    "    for word in words:\n",
    "        emb = get_word_embedding(word)\n",
    "        all_words.append(word)\n",
    "        all_embeddings.append(emb.numpy())\n",
    "        all_categories.append(category)\n",
    "\n",
    "# Stack embeddings\n",
    "embedding_matrix = np.stack(all_embeddings)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# Reduce to 2D with PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embedding_matrix)\n",
    "\n",
    "# Visualize\n",
    "plot_embeddings_2d(embeddings_2d, all_words, all_categories,\n",
    "                   title=\"Word Embeddings Visualization (PCA)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Explore Word Relationships\n",
    "\n",
    "Try adding your own word groups and see how they cluster:\n",
    "- Countries and cities\n",
    "- Emotions (happy, sad, angry, ...)\n",
    "- Scientific domains relevant to your research\n",
    "\n",
    "**Question:** Do semantically similar words cluster together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add your own word groups!\n",
    "custom_groups = {\n",
    "    \"Countries\": [\"France\", \"Germany\", \"Italy\", \"Spain\", \"Japan\"],  # <-- Modify!\n",
    "    \"Emotions\": [\"happy\", \"sad\", \"angry\", \"fear\", \"love\"],           # <-- Modify!\n",
    "    # Add more groups here!\n",
    "}\n",
    "\n",
    "all_words = []\n",
    "all_embeddings = []\n",
    "all_categories = []\n",
    "\n",
    "for category, words in custom_groups.items():\n",
    "    for word in words:\n",
    "        emb = get_word_embedding(word)\n",
    "        all_words.append(word)\n",
    "        all_embeddings.append(emb.numpy())\n",
    "        all_categories.append(category)\n",
    "\n",
    "embedding_matrix = np.stack(all_embeddings)\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embedding_matrix)\n",
    "\n",
    "plot_embeddings_2d(embeddings_2d, all_words, all_categories,\n",
    "                   title=\"Custom Word Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Transformers Beyond Text\n",
    "\n",
    "**Key Insight:** The Transformer architecture is not limited to text! The attention mechanism works on any sequential or structured data.\n",
    "\n",
    "## Transformers in Different Domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vision Transformers (ViT)\n",
    "\n",
    "Images are split into patches, each patch becomes a \"token\":\n",
    "\n",
    "```\n",
    "Image (224x224) → 196 patches (16x16 each) → Transformer → Classification\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how images become sequences\n",
    "def image_to_patches(image_size=224, patch_size=16):\n",
    "    \"\"\"Demonstrate how images are converted to sequences of patches.\"\"\"\n",
    "    n_patches = (image_size // patch_size) ** 2\n",
    "    \n",
    "    print(\"Vision Transformer (ViT) tokenization:\")\n",
    "    print(f\"  Image size: {image_size}x{image_size} pixels\")\n",
    "    print(f\"  Patch size: {patch_size}x{patch_size} pixels\")\n",
    "    print(f\"  Number of patches: {n_patches}\")\n",
    "    print(f\"  Sequence length: {n_patches} (like {n_patches} 'words')\")\n",
    "    print(f\"\\n  Each patch becomes a 'token' that attends to other patches!\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Draw grid\n",
    "    for i in range(0, image_size + 1, patch_size):\n",
    "        ax.axhline(y=i, color='blue', linewidth=1)\n",
    "        ax.axvline(x=i, color='blue', linewidth=1)\n",
    "    \n",
    "    # Number some patches\n",
    "    for i in range(image_size // patch_size):\n",
    "        for j in range(image_size // patch_size):\n",
    "            patch_num = i * (image_size // patch_size) + j\n",
    "            if patch_num < 10 or patch_num >= n_patches - 3:\n",
    "                ax.text(j * patch_size + patch_size/2, \n",
    "                       (image_size // patch_size - 1 - i) * patch_size + patch_size/2,\n",
    "                       str(patch_num), ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    ax.set_xlim(0, image_size)\n",
    "    ax.set_ylim(0, image_size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f'Image → {n_patches} patches (tokens)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "image_to_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transformers for Molecules\n",
    "\n",
    "Molecules can be represented as:\n",
    "- **SMILES strings**: Text representation of molecules\n",
    "- **Graph structure**: Atoms as nodes, bonds as edges\n",
    "\n",
    "```\n",
    "Aspirin: CC(=O)OC1=CC=CC=C1C(=O)O\n",
    "```\n",
    "\n",
    "Transformers like **ChemBERTa** are trained on molecular data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: SMILES tokenization\n",
    "smiles_examples = [\n",
    "    (\"Water\", \"O\"),\n",
    "    (\"Ethanol\", \"CCO\"),\n",
    "    (\"Aspirin\", \"CC(=O)OC1=CC=CC=C1C(=O)O\"),\n",
    "    (\"Caffeine\", \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\"),\n",
    "]\n",
    "\n",
    "print(\"Molecules as sequences (SMILES notation):\")\n",
    "print(\"=\"*60)\n",
    "for name, smiles in smiles_examples:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  SMILES: {smiles}\")\n",
    "    print(f\"  Length: {len(smiles)} characters\")\n",
    "    # Simple character-level tokenization\n",
    "    tokens = list(smiles)\n",
    "    print(f\"  Tokens: {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transformers for Time Series / Signals\n",
    "\n",
    "Time series data (ECG, stock prices, sensor data) can be:\n",
    "- Divided into fixed-length windows (patches)\n",
    "- Each window becomes a token\n",
    "\n",
    "Examples:\n",
    "- **Informer**: Long sequence time-series forecasting\n",
    "- **Temporal Fusion Transformer**: Multi-horizon forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample signal\n",
    "np.random.seed(42)\n",
    "t = np.linspace(0, 10, 500)\n",
    "signal = np.sin(2 * np.pi * 0.5 * t) + 0.5 * np.sin(2 * np.pi * 2 * t) + 0.2 * np.random.randn(500)\n",
    "\n",
    "# Show how it's converted to patches\n",
    "patch_size = 50\n",
    "n_patches = len(signal) // patch_size\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "# Original signal\n",
    "axes[0].plot(t, signal, 'b-', linewidth=1)\n",
    "axes[0].set_title('Original Signal', fontsize=12)\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Signal with patches highlighted\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, n_patches))\n",
    "for i in range(n_patches):\n",
    "    start = i * patch_size\n",
    "    end = start + patch_size\n",
    "    axes[1].plot(t[start:end], signal[start:end], color=colors[i], linewidth=2)\n",
    "    axes[1].axvline(x=t[start], color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[1].set_title(f'Signal divided into {n_patches} patches (tokens)', fontsize=12)\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSignal tokenization:\")\n",
    "print(f\"  Total samples: {len(signal)}\")\n",
    "print(f\"  Patch size: {patch_size} samples\")\n",
    "print(f\"  Number of patches (tokens): {n_patches}\")\n",
    "print(f\"  → Each patch is a vector of {patch_size} values\")\n",
    "print(f\"  → Attention learns which time windows are related!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. AlphaFold: Transformers for Protein Structure\n",
    "\n",
    "AlphaFold uses attention to understand relationships between amino acids:\n",
    "\n",
    "```\n",
    "Protein sequence: MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSH...\n",
    "                   ↓ (Attention + MSA)\n",
    "               3D Structure prediction\n",
    "```\n",
    "\n",
    "Each amino acid attends to others to predict how they fold in 3D space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demo: protein sequence as tokens\n",
    "hemoglobin_seq = \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSH\"\n",
    "\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "\n",
    "print(\"Protein as sequence (like text!):\")\n",
    "print(f\"  Sequence: {hemoglobin_seq[:30]}...\")\n",
    "print(f\"  Length: {len(hemoglobin_seq)} amino acids\")\n",
    "print(f\"  Vocabulary: {len(amino_acids)} amino acid types\")\n",
    "print(f\"\\n  Each amino acid is a 'token'\")\n",
    "print(f\"  Attention learns which amino acids interact in 3D!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Universal Pattern\n",
    "\n",
    "| Domain | Input | Tokenization | What Attention Learns |\n",
    "|--------|-------|--------------|----------------------|\n",
    "| Text | Words/sentences | Subword (BPE) | Word relationships |\n",
    "| Images | Pixels | Patches | Spatial relationships |\n",
    "| Molecules | SMILES/Graphs | Characters/Atoms | Chemical bonds |\n",
    "| Time series | Signal | Windows/Patches | Temporal patterns |\n",
    "| Proteins | Amino acid sequence | Single residues | 3D interactions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Think About Your Domain\n",
    "\n",
    "**Questions to consider:**\n",
    "1. What type of data do you work with in your research?\n",
    "2. How could it be tokenized for a Transformer?\n",
    "3. What relationships would attention learn?\n",
    "\n",
    "Think about:\n",
    "- Genomic sequences (DNA/RNA)\n",
    "- Medical images (CT scans, X-rays)\n",
    "- Climate data (spatial-temporal)\n",
    "- Chemical reactions\n",
    "- Social networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your notes here:\n",
    "# Data type: ?\n",
    "# Tokenization strategy: ?\n",
    "# What attention could learn: ?\n",
    "print(\"Think about how Transformers could apply to your research domain!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this practical, you learned:\n",
    "\n",
    "1. **Tokenization**: Converting text to numbers\n",
    "   - Character, word, and subword (BPE) approaches\n",
    "   - Modern tokenizers balance vocabulary size and sequence length\n",
    "\n",
    "2. **Attention Mechanism**: The core of Transformers\n",
    "   - Query, Key, Value projections\n",
    "   - Softmax attention weights\n",
    "   - Allows direct connections between any positions\n",
    "\n",
    "3. **Pre-trained Models**: Using Hugging Face\n",
    "   - Load models with `AutoModel.from_pretrained()`\n",
    "   - Generate text with temperature control\n",
    "\n",
    "4. **Embeddings**: Dense vector representations\n",
    "   - Similar words have similar embeddings\n",
    "   - Can visualize with PCA/t-SNE\n",
    "\n",
    "5. **Beyond Text**: Transformers are universal\n",
    "   - Vision, molecules, time series, proteins\n",
    "   - Key: tokenization + attention\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Transformers are flexible**: Any sequential/structured data can be processed\n",
    "- **Attention is the key**: Learns which parts of the input relate to each other\n",
    "- **Pre-training is powerful**: Billions of parameters capture rich representations\n",
    "- **Embeddings encode meaning**: Vector representations capture semantics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
