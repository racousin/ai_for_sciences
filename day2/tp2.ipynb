{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2: Mathematical Problem Solving with LLMs\n",
    "\n",
    "**Day 2 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day2/tp2.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this practical, you will understand:\n",
    "\n",
    "1. **Context Engineering**: How prompt design affects LLM performance\n",
    "2. **Prompting Strategies**: Zero-shot, few-shot, and chain-of-thought\n",
    "3. **Fine-tuning with LoRA**: Adapting models efficiently with limited resources\n",
    "4. **Evaluation**: Measuring accuracy on mathematical reasoning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "!pip install -q transformers torch peft accelerate bitsandbytes\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Math Problems Dataset\n",
    "\n",
    "We have 900 math problems across different categories: arithmetic, algebra, geometry, fractions, percentages, and word problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from local file (or download if running on Colab)\n",
    "import os\n",
    "\n",
    "data_path = 'data/maths.csv'\n",
    "if not os.path.exists(data_path):\n",
    "    # Download from GitHub if not available locally\n",
    "    !mkdir -p data\n",
    "    !wget -q -O data/maths.csv https://raw.githubusercontent.com/racousin/ai_for_sciences/main/day2/data/maths.csv\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "print(f\"Dataset size: {len(data)} problems\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(data['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some example problems\n",
    "print(\"Sample problems:\\n\")\n",
    "for i in range(5):\n",
    "    row = data.iloc[i]\n",
    "    print(f\"[{row['category']}] {row['problem']}\")\n",
    "    print(f\"  Answer: {row['solution']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test set (100 problems) for evaluation\n",
    "test_data = data.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "train_data = data.drop(test_data.index).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train set: {len(train_data)} problems (for few-shot examples & fine-tuning)\")\n",
    "print(f\"Test set: {len(test_data)} problems (for evaluation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Utility Functions\n",
    "\n",
    "We need functions to extract numerical answers from model outputs and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(text):\n",
    "    \"\"\"\n",
    "    Extract a numerical answer from model output text.\n",
    "    \n",
    "    Handles formats like:\n",
    "    - \"The answer is 42\"\n",
    "    - \"= 42.5\"\n",
    "    - \"Result: -15\"\n",
    "    - Just \"42\"\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    # Pattern 1: Look for \"answer/result/equals\" followed by a number\n",
    "    match = re.search(r'(?:answer|result|equals?|=)\\s*:?\\s*(-?\\d+\\.?\\d*)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Pattern 2: Look for a number at the very end of the text\n",
    "    match = re.search(r'(-?\\d+\\.?\\d*)\\s*$', text)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Pattern 3: Find all numbers and take the last one (often the final answer)\n",
    "    numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n",
    "    if numbers:\n",
    "        try:\n",
    "            return float(numbers[-1])\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# Test the extraction function\n",
    "test_cases = [\n",
    "    \"The answer is 42\",\n",
    "    \"42\",\n",
    "    \"15 + 27 = 42\",\n",
    "    \"Let me calculate... 10 + 5 = 15, then 15 * 2 = 30. The result is 30.\",\n",
    "    \"The value is -15.5\",\n",
    "    \"No number here\",\n",
    "]\n",
    "\n",
    "print(\"Number extraction tests:\")\n",
    "for s in test_cases:\n",
    "    result = extract_number(s)\n",
    "    print(f\"  '{s[:50]}{'...' if len(s) > 50 else ''}' -> {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, ground_truth, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Calculate accuracy with tolerance for floating point comparisons.\n",
    "    \n",
    "    Two values are considered equal if:\n",
    "    - They round to the same value at 2 decimal places, OR\n",
    "    - Their absolute difference is <= tolerance\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for pred, truth in zip(predictions, ground_truth):\n",
    "        if pred is None:\n",
    "            continue\n",
    "        if round(pred, 2) == round(truth, 2):\n",
    "            correct += 1\n",
    "        elif abs(pred - truth) <= tolerance:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Load Pre-trained Model\n",
    "\n",
    "We'll use a small model that can run on limited hardware. GPT-2 is a good starting point to understand the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 (small, fast, good for learning)\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "# Set padding token (GPT-2 doesn't have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(problem, prompt_template, model, tokenizer, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate an answer using the specified prompt template.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt_template, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the prompt from the response\n",
    "    response = response[len(prompt_template):].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Context Engineering - Prompting Strategies\n",
    "\n",
    "**Context engineering** (or prompt engineering) is the art of designing prompts that help LLMs perform better. The same model can produce very different results depending on how you ask!\n",
    "\n",
    "We'll explore three main strategies:\n",
    "\n",
    "1. **Zero-shot**: Just ask the question directly\n",
    "2. **Few-shot**: Provide examples before asking\n",
    "3. **Chain-of-Thought (CoT)**: Ask the model to reason step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1: Zero-shot Prompting\n",
    "\n",
    "The simplest approach: just ask the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zero_shot_prompt(problem):\n",
    "    \"\"\"Simple prompt - just the problem and ask for answer.\"\"\"\n",
    "    return f\"Problem: {problem}\\nAnswer:\"\n",
    "\n",
    "# Test on one problem\n",
    "test_problem = test_data.iloc[0]\n",
    "prompt = make_zero_shot_prompt(test_problem['problem'])\n",
    "\n",
    "print(\"ZERO-SHOT PROMPT:\")\n",
    "print(\"-\" * 50)\n",
    "print(prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = generate_answer(test_problem['problem'], prompt, model, tokenizer)\n",
    "extracted = extract_number(response)\n",
    "\n",
    "print(f\"\\nModel response: {response[:100]}{'...' if len(response) > 100 else ''}\")\n",
    "print(f\"Extracted answer: {extracted}\")\n",
    "print(f\"Correct answer: {test_problem['solution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: Few-shot Prompting\n",
    "\n",
    "Provide examples to show the model what format we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_few_shot_prompt(problem, n_examples=3):\n",
    "    \"\"\"Include examples from training data before the question.\"\"\"\n",
    "    examples = []\n",
    "    for i in range(n_examples):\n",
    "        ex = train_data.iloc[i]\n",
    "        examples.append(f\"Problem: {ex['problem']}\\nAnswer: {ex['solution']}\")\n",
    "    \n",
    "    examples_text = \"\\n\\n\".join(examples)\n",
    "    return f\"{examples_text}\\n\\nProblem: {problem}\\nAnswer:\"\n",
    "\n",
    "# Test on the same problem\n",
    "prompt = make_few_shot_prompt(test_problem['problem'])\n",
    "\n",
    "print(\"FEW-SHOT PROMPT:\")\n",
    "print(\"-\" * 50)\n",
    "print(prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = generate_answer(test_problem['problem'], prompt, model, tokenizer)\n",
    "extracted = extract_number(response)\n",
    "\n",
    "print(f\"\\nModel response: {response[:100]}{'...' if len(response) > 100 else ''}\")\n",
    "print(f\"Extracted answer: {extracted}\")\n",
    "print(f\"Correct answer: {test_problem['solution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3: Chain-of-Thought Prompting\n",
    "\n",
    "Ask the model to reason step by step. This often improves performance on math problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cot_prompt(problem):\n",
    "    \"\"\"Chain-of-thought: ask for step-by-step reasoning.\"\"\"\n",
    "    return f\"\"\"Solve this math problem step by step.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Let's solve this step by step:\n",
    "1.\"\"\"\n",
    "\n",
    "# Test on the same problem\n",
    "prompt = make_cot_prompt(test_problem['problem'])\n",
    "\n",
    "print(\"CHAIN-OF-THOUGHT PROMPT:\")\n",
    "print(\"-\" * 50)\n",
    "print(prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = generate_answer(test_problem['problem'], prompt, model, tokenizer, max_new_tokens=100)\n",
    "extracted = extract_number(response)\n",
    "\n",
    "print(f\"\\nModel response: {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
    "print(f\"Extracted answer: {extracted}\")\n",
    "print(f\"Correct answer: {test_problem['solution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare All Strategies\n",
    "\n",
    "Let's evaluate each strategy on a subset of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strategy(strategy_name, prompt_fn, test_subset, model, tokenizer, max_new_tokens=50):\n",
    "    \"\"\"Evaluate a prompting strategy on test data.\"\"\"\n",
    "    predictions = []\n",
    "    ground_truth = test_subset['solution'].tolist()\n",
    "    \n",
    "    for idx, row in test_subset.iterrows():\n",
    "        prompt = prompt_fn(row['problem'])\n",
    "        response = generate_answer(row['problem'], prompt, model, tokenizer, max_new_tokens)\n",
    "        pred = extract_number(response)\n",
    "        predictions.append(pred if pred is not None else 0.0)\n",
    "    \n",
    "    accuracy = compute_accuracy(predictions, ground_truth)\n",
    "    return accuracy, predictions\n",
    "\n",
    "# Evaluate on a small subset (20 problems) for speed\n",
    "eval_subset = test_data.head(20)\n",
    "\n",
    "print(\"Evaluating prompting strategies on 20 test problems...\\n\")\n",
    "\n",
    "strategies = {\n",
    "    'Zero-shot': make_zero_shot_prompt,\n",
    "    'Few-shot (3 examples)': make_few_shot_prompt,\n",
    "    'Chain-of-Thought': make_cot_prompt,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, prompt_fn in strategies.items():\n",
    "    print(f\"Testing {name}...\", end=\" \")\n",
    "    max_tokens = 100 if name == 'Chain-of-Thought' else 50\n",
    "    acc, preds = evaluate_strategy(name, prompt_fn, eval_subset, model, tokenizer, max_tokens)\n",
    "    results[name] = acc\n",
    "    print(f\"Accuracy: {acc:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "for name, acc in results.items():\n",
    "    bar = \"*\" * int(acc * 20)\n",
    "    print(f\"{name:25} {acc:6.1%}  {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "1. Which prompting strategy performed best? Why do you think that is?\n",
    "2. GPT-2 is a relatively small model (124M parameters). How might results differ with larger models?\n",
    "3. Can you think of other prompting strategies that might help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Fine-tuning with LoRA\n",
    "\n",
    "When prompting isn't enough, we can **fine-tune** the model on our specific task.\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is an efficient fine-tuning technique that:\n",
    "- Freezes the original model weights\n",
    "- Adds small trainable matrices to specific layers\n",
    "- Reduces memory usage by 10-100x compared to full fine-tuning\n",
    "- Can be trained on consumer hardware\n",
    "\n",
    "```\n",
    "Original weight matrix W (frozen)\n",
    "         +\n",
    "LoRA matrices: A @ B (trainable, low-rank)\n",
    "         =\n",
    "Adapted weights: W + A @ B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,                    # Rank of the update matrices (lower = fewer parameters)\n",
    "    lora_alpha=32,          # Scaling factor\n",
    "    lora_dropout=0.1,       # Dropout for regularization\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Which layers to adapt (attention layers for GPT-2)\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the base model for fine-tuning\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nParameter comparison:\")\n",
    "print(f\"  Total parameters: {total_params / 1e6:.1f}M\")\n",
    "print(f\"  Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(f\"\\nWe're only training {trainable_params / total_params:.2%} of the model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "We need to format our math problems for causal language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        # Format: \"Problem: ... Answer: ...\"\n",
    "        text = f\"Problem: {row['problem']}\\nAnswer: {row['solution']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': input_ids.clone()  # For causal LM, labels = input_ids\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_subset = train_data.head(200)  # Use 200 examples for fine-tuning\n",
    "train_dataset = MathDataset(train_subset, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Training setup\n",
    "optimizer = AdamW(lora_model.parameters(), lr=1e-4)\n",
    "num_epochs = 3\n",
    "\n",
    "lora_model.train()\n",
    "losses = []\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = lora_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(losses, alpha=0.7)\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('LoRA Fine-tuning Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base model vs fine-tuned model\n",
    "lora_model.eval()\n",
    "\n",
    "print(\"Comparing Base Model vs LoRA Fine-tuned Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate both on same test subset\n",
    "eval_subset = test_data.head(20)\n",
    "\n",
    "# Base model (few-shot, since it performed best)\n",
    "print(\"\\nEvaluating base model (few-shot)...\", end=\" \")\n",
    "base_acc, _ = evaluate_strategy(\"Base\", make_few_shot_prompt, eval_subset, model, tokenizer)\n",
    "print(f\"Accuracy: {base_acc:.1%}\")\n",
    "\n",
    "# Fine-tuned model (zero-shot, since it learned the task)\n",
    "print(\"Evaluating LoRA model (zero-shot)...\", end=\" \")\n",
    "lora_acc, _ = evaluate_strategy(\"LoRA\", make_zero_shot_prompt, eval_subset, lora_model, tokenizer)\n",
    "print(f\"Accuracy: {lora_acc:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"  Base model (few-shot):     {base_acc:.1%}\")\n",
    "print(f\"  LoRA fine-tuned (zero-shot): {lora_acc:.1%}\")\n",
    "if lora_acc > base_acc:\n",
    "    print(f\"  Improvement: +{lora_acc - base_acc:.1%}\")\n",
    "else:\n",
    "    print(f\"  (Fine-tuning may need more data or epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "1. How does the fine-tuned model compare to the prompting approaches?\n",
    "2. Why might fine-tuning help (or not help) for this task?\n",
    "3. What are the trade-offs between prompting vs fine-tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Exercise - Improve the Results\n",
    "\n",
    "Now it's your turn! Try to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Better Prompts\n",
    "\n",
    "Design a better prompt template. Consider:\n",
    "- More specific instructions\n",
    "- Different number of few-shot examples\n",
    "- Category-specific examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your own prompt template\n",
    "def make_custom_prompt(problem):\n",
    "    \"\"\"Your custom prompt strategy.\"\"\"\n",
    "    # <-- Modify this!\n",
    "    prompt = f\"Problem: {problem}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "# Test your prompt\n",
    "test_problem = test_data.iloc[0]\n",
    "prompt = make_custom_prompt(test_problem['problem'])\n",
    "print(\"Your prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "response = generate_answer(test_problem['problem'], prompt, model, tokenizer)\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Extracted: {extract_number(response)}\")\n",
    "print(f\"Correct: {test_problem['solution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Experiment with LoRA Parameters\n",
    "\n",
    "Try different LoRA configurations:\n",
    "- `r`: 4, 8, 16 (higher = more parameters)\n",
    "- `lora_alpha`: 16, 32, 64\n",
    "- More training epochs\n",
    "- Different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different LoRA configurations\n",
    "# Hint: Reload the base model and try different settings\n",
    "\n",
    "# Example:\n",
    "# new_lora_config = LoraConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM,\n",
    "#     r=16,  # <-- Try different values\n",
    "#     lora_alpha=64,\n",
    "#     lora_dropout=0.05,\n",
    "#     target_modules=[\"c_attn\", \"c_proj\"],\n",
    "# )\n",
    "\n",
    "print(\"Experiment with different configurations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Context Engineering (Prompting)\n",
    "- **Zero-shot**: Simple but often insufficient for complex tasks\n",
    "- **Few-shot**: Providing examples helps the model understand the task format\n",
    "- **Chain-of-Thought**: Step-by-step reasoning improves math performance\n",
    "- The same model can perform very differently based on how you prompt it\n",
    "\n",
    "### Fine-tuning with LoRA\n",
    "- **LoRA** adds small trainable matrices while keeping base model frozen\n",
    "- Much more efficient than full fine-tuning (trains ~0.1-1% of parameters)\n",
    "- Can be done on consumer hardware\n",
    "- Trade-off: requires training data and compute, but can outperform prompting\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "| Approach | When to Use |\n",
    "|----------|-------------|\n",
    "| Zero-shot | Quick experiments, simple tasks |\n",
    "| Few-shot | Have a few examples, need better format adherence |\n",
    "| Chain-of-Thought | Reasoning tasks (math, logic) |\n",
    "| LoRA Fine-tuning | Have training data, need maximum performance |\n",
    "\n",
    "## For Your Research\n",
    "\n",
    "Consider:\n",
    "1. **What task** do you need the model to perform?\n",
    "2. **How much data** do you have available?\n",
    "3. **What compute resources** are available?\n",
    "4. Start with prompting, move to fine-tuning if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **In your research domain**, what tasks might benefit from context engineering vs fine-tuning?\n",
    "\n",
    "2. **What kind of examples** would you include in few-shot prompts for your domain?\n",
    "\n",
    "3. **If you were to fine-tune** a model for your research, what data would you use?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
