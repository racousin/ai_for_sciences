{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2: Embeddings for Scientific Domains\n",
    "\n",
    "**Day 2 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day2/tp2.ipynb)\n",
    "\n",
    "## Objectives\n",
    "1. Understand how different scientific domains represent data\n",
    "2. Explore embeddings for text, molecules, proteins, and DNA\n",
    "3. Visualize embedding spaces with dimensionality reduction (PCA, t-SNE, UMAP)\n",
    "4. Use embeddings as features for classification with MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to install and import the required packages. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets sentence-transformers umap-learn\n",
    "!pip install -q fair-esm  # For protein embeddings (ESM-2)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Embedding Exploration\n",
    "\n",
    "In this part, we'll explore how embeddings work across different scientific domains. For each domain:\n",
    "1. **Data**: Understand the representation (text, SMILES, amino acids, nucleotides)\n",
    "2. **Tokenization**: See how data is tokenized\n",
    "3. **Embedding**: Extract embeddings from pre-trained models\n",
    "4. **Visualization**: Use dimensionality reduction to visualize\n",
    "5. **Similarity**: Find similar items using cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section A: Scientific Text Embeddings\n",
    "\n",
    "We'll use embeddings to analyze scientific project descriptions. The model captures semantic meaning, allowing us to find similar projects.\n",
    "\n",
    "### The Data\n",
    "\n",
    "Below are 15 scientific project descriptions from various fields. **Your task**: Add your own project description!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 Scientific project descriptions from different fields\n",
    "projects = [\n",
    "    # Physics / Astrophysics\n",
    "    \"Detecting gravitational waves from binary black hole mergers using deep learning on LIGO data\",\n",
    "    \"Simulating dark matter distribution in galaxy clusters with N-body simulations\",\n",
    "    \"Quantum error correction codes for fault-tolerant quantum computing\",\n",
    "    \n",
    "    # Biology / Medicine\n",
    "    \"Predicting protein-protein interactions using graph neural networks\",\n",
    "    \"Single-cell RNA sequencing analysis to identify cancer cell subtypes\",\n",
    "    \"Drug discovery for Alzheimer's disease targeting amyloid-beta aggregation\",\n",
    "    \n",
    "    # Chemistry / Materials\n",
    "    \"Machine learning for predicting catalyst activity in CO2 reduction reactions\",\n",
    "    \"Designing new battery materials using high-throughput DFT calculations\",\n",
    "    \"Molecular dynamics simulation of polymer degradation in marine environments\",\n",
    "    \n",
    "    # Environmental / Climate\n",
    "    \"Climate model downscaling using convolutional neural networks\",\n",
    "    \"Predicting extreme weather events from satellite imagery with transformers\",\n",
    "    \"Ocean acidification impact on coral reef ecosystems modeling\",\n",
    "    \n",
    "    # Computer Science / AI\n",
    "    \"Natural language processing for scientific literature summarization\",\n",
    "    \"Reinforcement learning for autonomous drone navigation in complex environments\",\n",
    "    \"Federated learning for privacy-preserving medical image analysis\"\n",
    "]\n",
    "\n",
    "# Field labels for coloring\n",
    "fields = [\n",
    "    \"Physics\", \"Physics\", \"Physics\",\n",
    "    \"Biology\", \"Biology\", \"Biology\",\n",
    "    \"Chemistry\", \"Chemistry\", \"Chemistry\",\n",
    "    \"Climate\", \"Climate\", \"Climate\",\n",
    "    \"Computer Science\", \"Computer Science\", \"Computer Science\"\n",
    "]\n",
    "\n",
    "print(f\"Number of projects: {len(projects)}\")\n",
    "print(f\"\\nFields: {set(fields)}\")\n",
    "print(f\"\\nExample project (Biology):\")\n",
    "print(f\"  '{projects[3]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add Your Own Project\n",
    "\n",
    "Add your own project description to the list. What field does it belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add your own project!\n",
    "my_project = \"Your project description here\"  # <-- Modify this!\n",
    "my_field = \"Your field\"  # <-- Modify this! (Physics, Biology, Chemistry, Climate, Computer Science, or other)\n",
    "\n",
    "# Add to lists\n",
    "projects_with_yours = projects + [my_project]\n",
    "fields_with_yours = fields + [my_field]\n",
    "\n",
    "print(f\"Total projects: {len(projects_with_yours)}\")\n",
    "print(f\"Your project: '{my_project}'\")\n",
    "print(f\"Your field: {my_field}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text Embedding Model\n",
    "\n",
    "We'll use **Sentence-BERT** (`all-MiniLM-L6-v2`), a model specifically trained to produce meaningful sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"Model: all-MiniLM-L6-v2\")\n",
    "print(f\"Embedding dimension: {text_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"Max sequence length: {text_model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Let's see how the model tokenizes text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokenizer from the model\n",
    "tokenizer = text_model.tokenizer\n",
    "\n",
    "# Example tokenization\n",
    "example_text = projects[3]  # Biology project\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "\n",
    "print(f\"Original text:\")\n",
    "print(f\"  '{example_text}'\")\n",
    "print(f\"\\nTokens ({len(tokens)} tokens):\")\n",
    "print(f\"  {tokens}\")\n",
    "print(f\"\\nNote: '##' prefix indicates subword continuation (WordPiece tokenization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all projects\n",
    "text_embeddings = text_model.encode(projects_with_yours, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"  - {text_embeddings.shape[0]} projects\")\n",
    "print(f\"  - {text_embeddings.shape[1]} dimensions per embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with PCA/t-SNE\n",
    "\n",
    "Since we have a small dataset (16 points), we'll use **PCA** or **t-SNE** for visualization. UMAP works better with larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "text_2d_pca = pca.fit_transform(text_embeddings)\n",
    "\n",
    "# Also try t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)  # Low perplexity for small dataset\n",
    "text_2d_tsne = tsne.fit_transform(text_embeddings)\n",
    "\n",
    "# Create color mapping\n",
    "unique_fields = list(set(fields_with_yours))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_fields)))\n",
    "field_to_color = {field: colors[i] for i, field in enumerate(unique_fields)}\n",
    "point_colors = [field_to_color[f] for f in fields_with_yours]\n",
    "\n",
    "# Plot both\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PCA plot\n",
    "for field in unique_fields:\n",
    "    mask = [f == field for f in fields_with_yours]\n",
    "    axes[0].scatter(text_2d_pca[mask, 0], text_2d_pca[mask, 1], \n",
    "                    c=[field_to_color[field]], label=field, s=100, alpha=0.7)\n",
    "axes[0].set_title('PCA Visualization')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# t-SNE plot\n",
    "for field in unique_fields:\n",
    "    mask = [f == field for f in fields_with_yours]\n",
    "    axes[1].scatter(text_2d_tsne[mask, 0], text_2d_tsne[mask, 1], \n",
    "                    c=[field_to_color[field]], label=field, s=100, alpha=0.7)\n",
    "axes[1].set_title('t-SNE Visualization')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQuestion: Do projects from the same field cluster together?\")\n",
    "print(\"Question: Where does your project appear?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search\n",
    "\n",
    "Find the most similar projects using cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(text_embeddings)\n",
    "\n",
    "# Find most similar project to YOUR project (last one)\n",
    "your_similarities = similarity_matrix[-1, :-1]  # Exclude self-similarity\n",
    "most_similar_idx = np.argmax(your_similarities)\n",
    "\n",
    "print(\"Your project:\")\n",
    "print(f\"  '{projects_with_yours[-1]}'\")\n",
    "print(f\"\\nMost similar project (similarity: {your_similarities[most_similar_idx]:.3f}):\")\n",
    "print(f\"  '{projects[most_similar_idx]}'\")\n",
    "print(f\"  Field: {fields[most_similar_idx]}\")\n",
    "\n",
    "print(f\"\\nTop 3 similar projects:\")\n",
    "top_3_idx = np.argsort(your_similarities)[-3:][::-1]\n",
    "for i, idx in enumerate(top_3_idx):\n",
    "    print(f\"  {i+1}. [{fields[idx]}] {projects[idx][:60]}... (sim: {your_similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Try a Different Model\n",
    "\n",
    "You can try **SciBERT** (trained on scientific papers) to see if it gives different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Try SciBERT\n",
    "# Uncomment the lines below to try a different model\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# \n",
    "# scibert_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "# scibert_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "# \n",
    "# # Get embeddings (using [CLS] token)\n",
    "# def get_scibert_embedding(text):\n",
    "#     inputs = scibert_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = scibert_model(**inputs)\n",
    "#     return outputs.last_hidden_state[:, 0, :].numpy()  # [CLS] token\n",
    "# \n",
    "# scibert_embeddings = np.vstack([get_scibert_embedding(p) for p in projects_with_yours])\n",
    "# print(f\"SciBERT embeddings shape: {scibert_embeddings.shape}\")\n",
    "\n",
    "print(\"Uncomment the code above to try SciBERT!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section B: Molecular Embeddings (Chemistry)\n",
    "\n",
    "Molecules are represented as **SMILES** (Simplified Molecular Input Line Entry System) strings. These are text representations of molecular structure.\n",
    "\n",
    "### Understanding SMILES\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|--------|\n",
    "| C, N, O, S | Atoms (carbon, nitrogen, oxygen, sulfur) |\n",
    "| c, n, o | Aromatic atoms (lowercase) |\n",
    "| = | Double bond |\n",
    "| # | Triple bond |\n",
    "| () | Branch |\n",
    "| [] | Explicit atom properties |\n",
    "| 1,2,3... | Ring closures |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example molecules with their SMILES and names\n",
    "molecules = {\n",
    "    # Simple molecules\n",
    "    \"Water\": \"O\",\n",
    "    \"Methane\": \"C\",\n",
    "    \"Ethanol\": \"CCO\",\n",
    "    \"Acetic acid\": \"CC(=O)O\",\n",
    "    \n",
    "    # Drugs - Pain/Inflammation\n",
    "    \"Aspirin\": \"CC(=O)Oc1ccccc1C(=O)O\",\n",
    "    \"Ibuprofen\": \"CC(C)Cc1ccc(cc1)C(C)C(=O)O\",\n",
    "    \"Acetaminophen\": \"CC(=O)Nc1ccc(O)cc1\",\n",
    "    \n",
    "    # Drugs - Antibiotics\n",
    "    \"Penicillin G\": \"CC1(C)SC2C(NC(=O)Cc3ccccc3)C(=O)N2C1C(=O)O\",\n",
    "    \"Amoxicillin\": \"CC1(C)SC2C(NC(=O)C(N)c3ccc(O)cc3)C(=O)N2C1C(=O)O\",\n",
    "    \n",
    "    # Drugs - CNS\n",
    "    \"Caffeine\": \"Cn1cnc2c1c(=O)n(c(=O)n2C)C\",\n",
    "    \"Diazepam\": \"CN1C(=O)CN=C(c2ccccc2)c3cc(Cl)ccc13\",\n",
    "    \n",
    "    # Drugs - Cardiovascular\n",
    "    \"Atorvastatin\": \"CC(C)c1c(C(=O)Nc2ccccc2)c(c(c3ccc(F)cc3)n1CC(O)CC(O)CC(=O)O)c4ccccc4\",\n",
    "    \"Lisinopril\": \"NCCCC[C@H](N[C@@H](CCc1ccccc1)C(=O)O)C(=O)N2CCC[C@H]2C(=O)O\",\n",
    "}\n",
    "\n",
    "# Drug categories\n",
    "drug_categories = {\n",
    "    \"Water\": \"Simple\", \"Methane\": \"Simple\", \"Ethanol\": \"Simple\", \"Acetic acid\": \"Simple\",\n",
    "    \"Aspirin\": \"Pain\", \"Ibuprofen\": \"Pain\", \"Acetaminophen\": \"Pain\",\n",
    "    \"Penicillin G\": \"Antibiotic\", \"Amoxicillin\": \"Antibiotic\",\n",
    "    \"Caffeine\": \"CNS\", \"Diazepam\": \"CNS\",\n",
    "    \"Atorvastatin\": \"Cardiovascular\", \"Lisinopril\": \"Cardiovascular\",\n",
    "}\n",
    "\n",
    "print(\"Example SMILES representations:\")\n",
    "for name, smiles in list(molecules.items())[:5]:\n",
    "    print(f\"  {name:15s}: {smiles}\")\n",
    "\n",
    "print(f\"\\nTotal molecules: {len(molecules)}\")\n",
    "print(f\"Categories: {set(drug_categories.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ChemBERTa Model\n",
    "\n",
    "**ChemBERTa** is a transformer model trained on SMILES strings. It understands molecular structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load ChemBERTa\n",
    "chem_tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "chem_model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "chem_model.eval()\n",
    "\n",
    "print(\"Model: ChemBERTa-zinc-base-v1\")\n",
    "print(f\"Vocabulary size: {chem_tokenizer.vocab_size}\")\n",
    "print(f\"Embedding dimension: {chem_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMILES Tokenization\n",
    "\n",
    "See how ChemBERTa tokenizes SMILES strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize aspirin\n",
    "aspirin_smiles = molecules[\"Aspirin\"]\n",
    "tokens = chem_tokenizer.tokenize(aspirin_smiles)\n",
    "\n",
    "print(f\"Molecule: Aspirin\")\n",
    "print(f\"SMILES: {aspirin_smiles}\")\n",
    "print(f\"\\nTokens ({len(tokens)}):\")\n",
    "print(f\"  {tokens}\")\n",
    "print(f\"\\nNote: The tokenizer learns subword units from SMILES patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Molecular Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_embedding(smiles, tokenizer, model):\n",
    "    \"\"\"Get embedding for a SMILES string using [CLS] token.\"\"\"\n",
    "    inputs = tokenizer(smiles, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use [CLS] token embedding (first token)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "\n",
    "# Generate embeddings for all molecules\n",
    "mol_names = list(molecules.keys())\n",
    "mol_smiles = list(molecules.values())\n",
    "mol_categories = [drug_categories[name] for name in mol_names]\n",
    "\n",
    "mol_embeddings = np.array([get_molecule_embedding(s, chem_tokenizer, chem_model) for s in mol_smiles])\n",
    "\n",
    "print(f\"Molecular embeddings shape: {mol_embeddings.shape}\")\n",
    "print(f\"  - {mol_embeddings.shape[0]} molecules\")\n",
    "print(f\"  - {mol_embeddings.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# Reduce to 2D with UMAP\n",
    "reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, random_state=42)\n",
    "mol_2d = reducer.fit_transform(mol_embeddings)\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Color by category\n",
    "unique_categories = list(set(mol_categories))\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(unique_categories)))\n",
    "cat_to_color = {cat: colors[i] for i, cat in enumerate(unique_categories)}\n",
    "\n",
    "for cat in unique_categories:\n",
    "    mask = [c == cat for c in mol_categories]\n",
    "    ax.scatter(mol_2d[mask, 0], mol_2d[mask, 1], \n",
    "               c=[cat_to_color[cat]], label=cat, s=150, alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "for i, name in enumerate(mol_names):\n",
    "    ax.annotate(name, (mol_2d[i, 0], mol_2d[i, 1]), fontsize=8, alpha=0.8)\n",
    "\n",
    "ax.set_title('Molecular Embeddings (ChemBERTa + UMAP)', fontsize=14)\n",
    "ax.set_xlabel('UMAP 1')\n",
    "ax.set_ylabel('UMAP 2')\n",
    "ax.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQuestion: Do similar drugs (same category) cluster together?\")\n",
    "print(\"Question: What do you notice about the pain relievers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Molecular Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar molecule to Aspirin\n",
    "query_mol = \"Aspirin\"\n",
    "query_idx = mol_names.index(query_mol)\n",
    "query_embedding = mol_embeddings[query_idx]\n",
    "\n",
    "# Compute similarities\n",
    "similarities = cosine_similarity([query_embedding], mol_embeddings)[0]\n",
    "similarities[query_idx] = -1  # Exclude self\n",
    "\n",
    "# Top 3 similar\n",
    "top_3_idx = np.argsort(similarities)[-3:][::-1]\n",
    "\n",
    "print(f\"Query molecule: {query_mol}\")\n",
    "print(f\"  SMILES: {molecules[query_mol]}\")\n",
    "print(f\"  Category: {drug_categories[query_mol]}\")\n",
    "\n",
    "print(f\"\\nMost similar molecules:\")\n",
    "for i, idx in enumerate(top_3_idx):\n",
    "    name = mol_names[idx]\n",
    "    print(f\"  {i+1}. {name} ({drug_categories[name]}) - similarity: {similarities[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section C: Protein Embeddings (Biology)\n",
    "\n",
    "Proteins are sequences of **amino acids** (20 standard types). Each amino acid is represented by a single letter.\n",
    "\n",
    "| Letter | Amino Acid | Property |\n",
    "|--------|------------|----------|\n",
    "| A | Alanine | Hydrophobic |\n",
    "| C | Cysteine | Forms disulfide bonds |\n",
    "| D | Aspartic acid | Negative charge |\n",
    "| E | Glutamic acid | Negative charge |\n",
    "| K | Lysine | Positive charge |\n",
    "| R | Arginine | Positive charge |\n",
    "| ... | ... | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example protein sequences from different families\n",
    "proteins = {\n",
    "    # Hemoglobin family (oxygen transport)\n",
    "    \"Hemoglobin_alpha\": \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSH\",\n",
    "    \"Hemoglobin_beta\": \"MVHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLST\",\n",
    "    \"Myoglobin\": \"MGLSDGEWQLVLNVWGKVEADIPGHGQEVLIRLFKGHPETLEKFDKFKHLK\",\n",
    "    \n",
    "    # Kinase family (phosphorylation)\n",
    "    \"PKA_catalytic\": \"MGNAAAAKKGSEQESVKEFLAKAKEDFLKKWENPAQNTAHLDQFERIKTLG\",\n",
    "    \"PKC_alpha\": \"MADVFPGNDSASGPGNTSTGSADPSSAPGSEHCGSAGSGSPTGSPGSSGGP\",\n",
    "    \"CDK2\": \"MENFQKVEKIGEGTYGVVYKARNKLTGEVVALKKIRLDTETEGVPSTAIRE\",\n",
    "    \n",
    "    # Protease family (protein cleavage)\n",
    "    \"Trypsin\": \"IVGGYTCGANTVPYQVSLNSGYHFCGGSLINSQWVVSAAHCYKSGIQVRLG\",\n",
    "    \"Chymotrypsin\": \"CGVPAIQPVLSGLSRIVNGEEAVPGSWPWQVSLQDKTGFHFCGGSLINENWVVTAAHCGVTTSDVVVAGEFDQGSSSEKIQKLKIAKVFKNS\",\n",
    "    \"Elastase\": \"VVGGTEAQRNSWPSQISLQYRSGSSWAHTCGGTLIRQNWVMTAAHCVDRELTFRVVVGEHNLNQNDGTEQYVGVQKIVVHPYWNTDDVAAGYDIALLRLAQSVTLNSYVQLGVLPRAGTILANNSPCYITGWGLTRTNGQLAQTLQQAYLPTVDYAICSSSSYWGSTVKNSMVCAGGDGVRSGCQGDSGGPLHCLVNGQYAVHGVTSFVSRLGCNVTRKPTVFTRVSAYISWINNVIASN\",\n",
    "    \n",
    "    # Transporter family\n",
    "    \"GLUT1\": \"MEPSSKKLTGRLMLAVGGAVLGSLQFGYNTGVINAPQKVIEEFYNQTWVHR\",\n",
    "    \"Aquaporin1\": \"MASEFKKKLFWRAVVAEFLATTLFVFISIGSALGFKYPVGNNQTAVQDNVKVSLAFGLSIATLAQSVGHISGAHLNPAVTLGLLLSCQISIFRALMYIIAQCVGAIVATAILSGITSSLTGNSLGRNDLADGVNSGQGLGIEIIGTLQLVLCVLATTDRRRRDLGGSAPLAIGLSVALGHLLAIDYTGCGINPARSFGSAVITHNFSNHWIFWVGPFIGGALAGLIYDFLLFPRLKSISV\",\n",
    "}\n",
    "\n",
    "protein_families = {\n",
    "    \"Hemoglobin_alpha\": \"Globin\", \"Hemoglobin_beta\": \"Globin\", \"Myoglobin\": \"Globin\",\n",
    "    \"PKA_catalytic\": \"Kinase\", \"PKC_alpha\": \"Kinase\", \"CDK2\": \"Kinase\",\n",
    "    \"Trypsin\": \"Protease\", \"Chymotrypsin\": \"Protease\", \"Elastase\": \"Protease\",\n",
    "    \"GLUT1\": \"Transporter\", \"Aquaporin1\": \"Transporter\",\n",
    "}\n",
    "\n",
    "print(\"Example protein sequences (first 50 amino acids):\")\n",
    "for name, seq in list(proteins.items())[:3]:\n",
    "    print(f\"  {name:20s}: {seq[:50]}...\")\n",
    "\n",
    "print(f\"\\nTotal proteins: {len(proteins)}\")\n",
    "print(f\"Families: {set(protein_families.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ESM-2 Model\n",
    "\n",
    "**ESM-2** (Evolutionary Scale Modeling) is a state-of-the-art protein language model from Meta AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm\n",
    "\n",
    "# Load ESM-2 (small version for speed)\n",
    "esm_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()  # 8M parameter version\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "esm_model.eval()\n",
    "\n",
    "print(\"Model: ESM-2 (esm2_t6_8M_UR50D)\")\n",
    "print(f\"Parameters: 8 million\")\n",
    "print(f\"Embedding dimension: {esm_model.embed_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Tokenization\n",
    "\n",
    "Proteins are tokenized at the amino acid level (each letter = one token):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tokenization\n",
    "example_seq = proteins[\"Hemoglobin_alpha\"][:20]\n",
    "print(f\"Sequence: {example_seq}\")\n",
    "print(f\"\\nTokens (each amino acid is a token):\")\n",
    "print(f\"  {list(example_seq)}\")\n",
    "print(f\"\\nUnlike text, protein tokenization is typically character-level (one amino acid = one token)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Protein Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_embedding(name, sequence, model, batch_converter):\n",
    "    \"\"\"Get mean embedding for a protein sequence.\"\"\"\n",
    "    data = [(name, sequence)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[6])  # Last layer\n",
    "    \n",
    "    # Get mean embedding (excluding special tokens)\n",
    "    token_embeddings = results[\"representations\"][6]\n",
    "    # Mean over sequence length (excluding BOS/EOS)\n",
    "    mean_embedding = token_embeddings[0, 1:-1, :].mean(dim=0).numpy()\n",
    "    return mean_embedding\n",
    "\n",
    "# Generate embeddings\n",
    "protein_names = list(proteins.keys())\n",
    "protein_seqs = list(proteins.values())\n",
    "protein_fams = [protein_families[name] for name in protein_names]\n",
    "\n",
    "print(\"Generating protein embeddings...\")\n",
    "protein_embeddings = np.array([\n",
    "    get_protein_embedding(name, seq, esm_model, batch_converter) \n",
    "    for name, seq in zip(protein_names, protein_seqs)\n",
    "])\n",
    "\n",
    "print(f\"Protein embeddings shape: {protein_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Protein Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization\n",
    "reducer = umap.UMAP(n_neighbors=3, min_dist=0.1, random_state=42)\n",
    "protein_2d = reducer.fit_transform(protein_embeddings)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "unique_fams = list(set(protein_fams))\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(unique_fams)))\n",
    "fam_to_color = {fam: colors[i] for i, fam in enumerate(unique_fams)}\n",
    "\n",
    "for fam in unique_fams:\n",
    "    mask = [f == fam for f in protein_fams]\n",
    "    ax.scatter(protein_2d[mask, 0], protein_2d[mask, 1],\n",
    "               c=[fam_to_color[fam]], label=fam, s=150, alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "for i, name in enumerate(protein_names):\n",
    "    ax.annotate(name, (protein_2d[i, 0], protein_2d[i, 1]), fontsize=8, alpha=0.8)\n",
    "\n",
    "ax.set_title('Protein Embeddings (ESM-2 + UMAP)', fontsize=14)\n",
    "ax.set_xlabel('UMAP 1')\n",
    "ax.set_ylabel('UMAP 2')\n",
    "ax.legend(title='Family', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQuestion: Do proteins from the same family cluster together?\")\n",
    "print(\"Question: Which families are most similar to each other?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section D: DNA Embeddings (Genomics)\n",
    "\n",
    "DNA is a sequence of 4 nucleotides: **A** (Adenine), **T** (Thymine), **G** (Guanine), **C** (Cytosine).\n",
    "\n",
    "### K-mer Tokenization\n",
    "\n",
    "DNA models often use **k-mers** (subsequences of length k) as tokens:\n",
    "- Sequence: `ATCGATCG`\n",
    "- 3-mers: `ATC`, `TCG`, `CGA`, `GAT`, `ATC`, `TCG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DNA sequences (promoter regions)\n",
    "dna_sequences = {\n",
    "    # Housekeeping gene promoters (always active)\n",
    "    \"GAPDH_promoter\": \"GCCCGCCCCCGCCCGCCCGCCGCCGCCGCCGCCGCCGCCGCCCGCCCGCGCGCCCGCCATGGGGAAGGTGAAGGTCGGAGTCAACGGATTTGGTCGTAT\",\n",
    "    \"Actin_promoter\": \"CCCCCGCCCCGCGCCCCGCCCCGCGCCCCGCCCCGCGCCCCGCCCCGCGCCGCGCGCCATGGATGATGATATCGCCGCGCTCGTCGTCGACAACGGCTC\",\n",
    "    \"Tubulin_promoter\": \"GCGCCGCCCCCGCCGCCGCCGCCGCCGCCGCCGCGCGCGCGCGCGCGCGCGCGCGCATGCGTGAGTGCATCTCCATCCACGTTGGCCAGGCTGGTGTC\",\n",
    "    \n",
    "    # Immune response promoters\n",
    "    \"IL6_promoter\": \"AATAAATAAATAAATAAATGCCCCTCAGCTTGACTCACCTGAGACGTGCAGAGCTGGCAGAAGAAAGCAGCAAAGAGGCACTGGCAGAAAACAACCT\",\n",
    "    \"TNF_promoter\": \"AAAAAATATTTATATATTTATATATATGCGCGCCGCGCCGCGCCGCGCCGCGCCGCAGGCAGGCAGGCAGGCAGGCAGGCTGAGCTGAGCTGAGCTGA\",\n",
    "    \"IFN_promoter\": \"AATAAAATAAATTTTTTATAATTTTAATTTTAATTTTAAATTGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGC\",\n",
    "    \n",
    "    # Cell cycle promoters\n",
    "    \"CyclinD_promoter\": \"GCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCATGAACTACCTGGACCGCTTCCTGTCGCTGGAGCCCGTGAA\",\n",
    "    \"CDK1_promoter\": \"CCGCCGCCGCCGCCGCCGCCGCCGCCGCCGCCGCCGCCGCCGCCGCCGCCGCCGCCATGGAAGATTATACCAAAATAGAGAAAATTGGAGAAGGTAC\",\n",
    "    \"p53_promoter\": \"ATATATATATATATATATATATAGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCATGGAGGAGCCGCAGTCAGATCCTAGCGTCGAG\",\n",
    "}\n",
    "\n",
    "dna_categories = {\n",
    "    \"GAPDH_promoter\": \"Housekeeping\", \"Actin_promoter\": \"Housekeeping\", \"Tubulin_promoter\": \"Housekeeping\",\n",
    "    \"IL6_promoter\": \"Immune\", \"TNF_promoter\": \"Immune\", \"IFN_promoter\": \"Immune\",\n",
    "    \"CyclinD_promoter\": \"Cell_cycle\", \"CDK1_promoter\": \"Cell_cycle\", \"p53_promoter\": \"Cell_cycle\",\n",
    "}\n",
    "\n",
    "print(\"Example DNA sequences (first 50 nucleotides):\")\n",
    "for name, seq in list(dna_sequences.items())[:3]:\n",
    "    print(f\"  {name:18s}: {seq[:50]}...\")\n",
    "\n",
    "print(f\"\\nTotal sequences: {len(dna_sequences)}\")\n",
    "print(f\"Categories: {set(dna_categories.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mer Tokenization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmers(sequence, k=6):\n",
    "    \"\"\"Convert sequence to k-mers.\"\"\"\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "# Example\n",
    "example_dna = \"ATCGATCGATCG\"\n",
    "kmers_3 = get_kmers(example_dna, k=3)\n",
    "kmers_6 = get_kmers(example_dna, k=6)\n",
    "\n",
    "print(f\"Sequence: {example_dna}\")\n",
    "print(f\"\\n3-mers: {kmers_3}\")\n",
    "print(f\"\\n6-mers: {kmers_6}\")\n",
    "print(f\"\\nDNABERT uses 6-mers (4^6 = 4096 possible tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DNABERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load DNABERT-2 (or fallback to DNABERT)\n",
    "try:\n",
    "    dna_tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "    dna_model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "    print(\"Model: DNABERT-2-117M\")\n",
    "except:\n",
    "    # Fallback: use a simpler DNA model or generic BERT with k-mers\n",
    "    print(\"DNABERT-2 not available, using fallback approach...\")\n",
    "    dna_tokenizer = None\n",
    "    dna_model = None\n",
    "\n",
    "if dna_model is not None:\n",
    "    dna_model.eval()\n",
    "    print(f\"Embedding dimension: {dna_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DNA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dna_embedding(sequence, tokenizer, model):\n",
    "    \"\"\"Get embedding for a DNA sequence.\"\"\"\n",
    "    if tokenizer is None or model is None:\n",
    "        # Fallback: use simple k-mer frequency vector\n",
    "        from collections import Counter\n",
    "        kmers = get_kmers(sequence, k=4)\n",
    "        counts = Counter(kmers)\n",
    "        # Create a fixed-size vector based on common 4-mers\n",
    "        all_4mers = [''.join(p) for p in __import__('itertools').product('ACGT', repeat=4)]\n",
    "        return np.array([counts.get(kmer, 0) for kmer in all_4mers], dtype=float)\n",
    "    \n",
    "    inputs = tokenizer(sequence, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "\n",
    "# Generate embeddings\n",
    "dna_names = list(dna_sequences.keys())\n",
    "dna_seqs = list(dna_sequences.values())\n",
    "dna_cats = [dna_categories[name] for name in dna_names]\n",
    "\n",
    "print(\"Generating DNA embeddings...\")\n",
    "dna_embeddings = np.array([get_dna_embedding(seq, dna_tokenizer, dna_model) for seq in dna_seqs])\n",
    "\n",
    "print(f\"DNA embeddings shape: {dna_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize DNA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization\n",
    "reducer = umap.UMAP(n_neighbors=3, min_dist=0.1, random_state=42)\n",
    "dna_2d = reducer.fit_transform(dna_embeddings)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "unique_cats = list(set(dna_cats))\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(unique_cats)))\n",
    "cat_to_color = {cat: colors[i] for i, cat in enumerate(unique_cats)}\n",
    "\n",
    "for cat in unique_cats:\n",
    "    mask = [c == cat for c in dna_cats]\n",
    "    ax.scatter(dna_2d[mask, 0], dna_2d[mask, 1],\n",
    "               c=[cat_to_color[cat]], label=cat, s=150, alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "for i, name in enumerate(dna_names):\n",
    "    short_name = name.replace('_promoter', '')\n",
    "    ax.annotate(short_name, (dna_2d[i, 0], dna_2d[i, 1]), fontsize=8, alpha=0.8)\n",
    "\n",
    "ax.set_title('DNA Embeddings (DNABERT + UMAP)', fontsize=14)\n",
    "ax.set_xlabel('UMAP 1')\n",
    "ax.set_ylabel('UMAP 2')\n",
    "ax.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQuestion: Do promoters with similar functions cluster together?\")\n",
    "print(\"Question: What makes housekeeping genes different from immune genes?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 Summary: Comparing Domains\n",
    "\n",
    "| Domain | Data Format | Tokenization | Model | Vocabulary |\n",
    "|--------|-------------|--------------|-------|------------|\n",
    "| Text | Natural language | WordPiece/BPE | Sentence-BERT | ~30k tokens |\n",
    "| Molecules | SMILES strings | SMILES tokenizer | ChemBERTa | ~600 tokens |\n",
    "| Proteins | Amino acid sequence | Per-residue | ESM-2 | 20 amino acids |\n",
    "| DNA | Nucleotide sequence | k-mers | DNABERT | 4 nucleotides (4^k k-mers) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Classification with Embeddings + MLP\n",
    "\n",
    "Now we'll use molecular embeddings as features for a real classification task: **predicting Blood-Brain Barrier Penetration (BBBP)**.\n",
    "\n",
    "This is a critical property for CNS drug development - can the drug cross the blood-brain barrier to reach the brain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BBBP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load BBBP dataset from MoleculeNet (via DeepChem or direct URL)\n",
    "bbbp_url = \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\"\n",
    "\n",
    "try:\n",
    "    bbbp_df = pd.read_csv(bbbp_url)\n",
    "    print(f\"Loaded BBBP dataset: {len(bbbp_df)} molecules\")\n",
    "except:\n",
    "    print(\"Could not load from URL, creating sample dataset...\")\n",
    "    # Create a sample dataset\n",
    "    sample_data = {\n",
    "        'smiles': [\n",
    "            'CC(C)Cc1ccc(cc1)C(C)C(=O)O',  # Ibuprofen - penetrates\n",
    "            'CC(=O)Nc1ccc(O)cc1',  # Acetaminophen - penetrates\n",
    "            'CC(=O)Oc1ccccc1C(=O)O',  # Aspirin - penetrates\n",
    "            'Cn1cnc2c1c(=O)n(c(=O)n2C)C',  # Caffeine - penetrates\n",
    "            'CC1(C)SC2C(NC(=O)Cc3ccccc3)C(=O)N2C1C(=O)O',  # Penicillin - does not penetrate\n",
    "        ] * 40,\n",
    "        'p_np': [1, 1, 1, 1, 0] * 40\n",
    "    }\n",
    "    bbbp_df = pd.DataFrame(sample_data)\n",
    "\n",
    "print(f\"\\nDataset shape: {bbbp_df.shape}\")\n",
    "print(f\"\\nColumns: {bbbp_df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(bbbp_df['p_np'].value_counts())\n",
    "print(\"  1 = Penetrates BBB\")\n",
    "print(\"  0 = Does not penetrate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings for BBBP Dataset\n",
    "\n",
    "We'll use a subset for speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset for speed (full dataset would take too long)\n",
    "n_samples = min(200, len(bbbp_df))\n",
    "bbbp_subset = bbbp_df.sample(n=n_samples, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Using {n_samples} molecules for classification\")\n",
    "print(f\"Label distribution in subset:\")\n",
    "print(bbbp_subset['p_np'].value_counts())\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"\\nGenerating embeddings (this may take a minute)...\")\n",
    "bbbp_embeddings = []\n",
    "valid_indices = []\n",
    "\n",
    "for i, smiles in enumerate(bbbp_subset['smiles']):\n",
    "    try:\n",
    "        emb = get_molecule_embedding(smiles, chem_tokenizer, chem_model)\n",
    "        bbbp_embeddings.append(emb)\n",
    "        valid_indices.append(i)\n",
    "    except:\n",
    "        continue  # Skip invalid SMILES\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Processed {i + 1}/{n_samples}\")\n",
    "\n",
    "bbbp_embeddings = np.array(bbbp_embeddings)\n",
    "bbbp_labels = bbbp_subset.iloc[valid_indices]['p_np'].values\n",
    "\n",
    "print(f\"\\nFinal embeddings shape: {bbbp_embeddings.shape}\")\n",
    "print(f\"Labels shape: {bbbp_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data and Prepare for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    bbbp_embeddings, bbbp_labels, test_size=0.2, random_state=42, stratify=bbbp_labels\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.FloatTensor(X_train)\n",
    "X_test_t = torch.FloatTensor(X_test)\n",
    "y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "y_test_t = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "print(f\"Training set: {X_train_t.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_t.shape[0]} samples\")\n",
    "print(f\"Input dimension: {X_train_t.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"Simple MLP for binary classification on embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create model\n",
    "input_dim = X_train_t.shape[1]\n",
    "model = MLPClassifier(input_dim=input_dim, hidden_dim=128)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred_train = model(X_train_t)\n",
    "    loss = criterion(y_pred_train, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_test = model(X_test_t)\n",
    "        test_loss = criterion(y_pred_test, y_test_t)\n",
    "        test_losses.append(test_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Train Loss = {loss.item():.4f}, Test Loss = {test_loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_proba = model(X_test_t).numpy()\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\" CLASSIFICATION RESULTS\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2%}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No penetration', 'Penetrates BBB']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Experiment with the MLP\n",
    "\n",
    "Try modifying the classifier to improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different architectures\n",
    "# Try changing:\n",
    "# - hidden_dim: 64, 128, 256\n",
    "# - Number of layers\n",
    "# - Dropout rate\n",
    "# - Learning rate\n",
    "\n",
    "class ImprovedMLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):  # <-- Modify hidden_dim!\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # <-- Modify dropout!\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            # Add more layers here if you want\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Train and evaluate your improved model\n",
    "# model_v2 = ImprovedMLPClassifier(input_dim)\n",
    "# ... (training code)\n",
    "\n",
    "print(\"Modify the class above and train your improved model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this practical, you learned:\n",
    "\n",
    "## Part 1: Embedding Exploration\n",
    "1. **Text embeddings**: Sentence-BERT captures semantic meaning of scientific projects\n",
    "2. **Molecular embeddings**: ChemBERTa encodes chemical structure from SMILES\n",
    "3. **Protein embeddings**: ESM-2 captures evolutionary and structural information\n",
    "4. **DNA embeddings**: DNABERT understands genomic sequences via k-mers\n",
    "\n",
    "## Part 2: Classification\n",
    "5. **Embeddings as features**: Pre-trained embeddings can be used as input to downstream tasks\n",
    "6. **MLP classifier**: Simple neural network on top of embeddings for classification\n",
    "7. **BBBP prediction**: Real-world drug discovery application\n",
    "\n",
    "## Key Takeaways\n",
    "- Different domains require different tokenization strategies\n",
    "- Pre-trained embeddings capture domain-specific knowledge\n",
    "- Visualization (PCA, t-SNE, UMAP) helps understand embedding spaces\n",
    "- Embeddings enable transfer learning for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
