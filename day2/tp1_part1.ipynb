{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# TP1 - Part 1: Tokenization & Embeddings\n",
    "\n",
    "**Day 2 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day2/tp1_part1.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this practical, you will understand:\n",
    "\n",
    "1. **The complete pipeline**: Text → Tokens → Token IDs → Embeddings\n",
    "2. **What embeddings are**: Dense vector representations where similar things are close\n",
    "3. **Why pre-trained models matter**: Leveraging knowledge from massive datasets\n",
    "4. **How to use embeddings**: Similarity search and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Pipeline - From Text to Vectors\n",
    "\n",
    "Machine Learning models only understand numbers. To use text data (or molecules, proteins, DNA), we need to convert it to numbers.\n",
    "\n",
    "```\n",
    "┌─────────────┐    ┌──────────────┐    ┌──────────────┐    ┌──────────────────┐\n",
    "│  Raw Text   │ →  │   Tokens     │ →  │  Token IDs   │ →  │    Embeddings    │\n",
    "│  (string)   │    │  (subwords)  │    │  (integers)  │    │ (dense vectors)  │\n",
    "└─────────────┘    └──────────────┘    └──────────────┘    └──────────────────┘\n",
    "\n",
    "\"The cat sat\"  →  [\"The\", \"cat\", \"sat\"] →  [464, 3797, 3332]  →  [[0.1, -0.2, ...], \n",
    "                                                                   [0.3, 0.5, ...],\n",
    "                                                                   [-0.1, 0.4, ...]]\n",
    "```\n",
    "\n",
    "In this practical, we'll explore **tokenization** (how text is split) and then focus on **embeddings** (how tokens become meaningful vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "!pip install -q transformers sentence-transformers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from aiforscience import (\n",
    "    visualize_tokens,\n",
    "    plot_similarity_matrix,\n",
    "    plot_embeddings_pca,\n",
    "    semantic_search,\n",
    "    print_search_results,\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Tokenization: Breaking Text into Pieces\n",
    "\n",
    "Before we can process text, we need to **tokenize** it - break it into units.\n",
    "\n",
    "### Three Tokenization Strategies\n",
    "\n",
    "| Strategy | How it works | Example: \"unhappiness\" | Pros/Cons |\n",
    "|----------|--------------|------------------------|-----------|\n",
    "| **Character** | Split into characters | `['u','n','h','a','p','p','i','n','e','s','s']` | ✓ Small vocab, ✗ Very long sequences |\n",
    "| **Word** | Split by spaces/punctuation | `['unhappiness']` | ✓ Meaningful units, ✗ Huge vocab, can't handle new words |\n",
    "| **Subword** | Split into common subparts | `['un', 'happiness']` | ✓ Balanced! Handles new words, reasonable vocab |\n",
    "\n",
    "**Modern models use subword tokenization** (like BPE - Byte Pair Encoding) because:\n",
    "- Frequent words stay whole: \"the\", \"cat\"\n",
    "- Rare words are split: \"transformers\" → \"transform\" + \"ers\"\n",
    "- Can handle any word, even typos: \"looooong\" → \"l\" + \"oo\" + \"oo\" + \"ong\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dg11mktu6h4",
   "metadata": {},
   "source": [
    "## Tokenizers Should Preserve Data Structure\n",
    "\n",
    "A good tokenizer splits data into **meaningful units** that preserve its structure.\n",
    "\n",
    "### Natural Language Has Structure\n",
    "\n",
    "Even \"simple\" text has structure a tokenizer must handle:\n",
    "\n",
    "| Structure | Examples | Tokenizer choices |\n",
    "|-----------|----------|-------------------|\n",
    "| **Case** | \"Cat\" vs \"cat\" vs \"CAT\" | Keep case? Lowercase all? |\n",
    "| **Alphabets** | Latin, Cyrillic (Привет), Chinese (你好), Arabic (مرحبا) | Which scripts to support? |\n",
    "| **Punctuation** | \"don't\", \"e-mail\", \"Ph.D.\" | Split on punctuation? Keep together? |\n",
    "| **Morphology** | \"un-believe-able\", \"run-ning\" | Recognize prefixes/suffixes? |\n",
    "| **Spaces** | English uses spaces, Chinese doesn't | How to find word boundaries? |\n",
    "\n",
    "Different tokenizers make different choices. For example:\n",
    "- **BERT uncased** lowercases everything: \"DNA\" → \"dna\"\n",
    "- **GPT-2** preserves case: \"DNA\" stays \"DNA\"\n",
    "\n",
    "Let's compare how different tokenizers handle the same text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# GPT-2: case-sensitive, BPE tokenization\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# BERT uncased: lowercases everything, WordPiece tokenization\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# TODO: Choose your own tokenizer from https://huggingface.co/models\n",
    "# Examples: \"mistralai/Mistral-7B-v0.1\", \"deepseek-ai/DeepSeek-V3.2\"\n",
    "your_tokenizer = # <-- Modify this!\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKENIZER COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Model':<25} {'Vocab Size':<15} {'Case':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'GPT-2':<25} {len(tokenizer_gpt2):,}          {'Preserved':<15}\")\n",
    "print(f\"{'BERT uncased':<25} {len(tokenizer_bert):,}          {'Lowercased':<15}\")\n",
    "print(f\"{'Your choice':<25} {len(your_tokenizer):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fmej404vfk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenizers on text with different structures\n",
    "text_examples = [\n",
    "    \"The CRISPR-Cas9 system edits DNA.\",  # Uppercase acronyms\n",
    "    \"COVID-19 vaccine efficacy study\",     # Numbers and hyphens\n",
    "    \"Dr. Smith's research paper\",          # Punctuation and possessives\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOKENIZING TEXT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for text in text_examples:\n",
    "    tokens_gpt2 = tokenizer_gpt2.tokenize(text)\n",
    "    tokens_bert = tokenizer_bert.tokenize(text)\n",
    "    tokens_yours = your_tokenizer.tokenize(text)\n",
    "    \n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"  GPT-2  ({len(tokens_gpt2):2} tokens): {tokens_gpt2}\")\n",
    "    print(f\"  BERT   ({len(tokens_bert):2} tokens): {tokens_bert}\")\n",
    "    print(f\"  Yours  ({len(tokens_yours):2} tokens): {tokens_yours}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yi22dj4l6c",
   "metadata": {},
   "source": [
    "### Question: Analyzing Tokenizer Outputs\n",
    "\n",
    "Look at the outputs above and think about:\n",
    "\n",
    "1. **Case sensitivity**: How does BERT handle \"CRISPR\" and \"DNA\"? What information is lost?\n",
    "\n",
    "2. **Special characters**: How do the tokenizers handle hyphens in \"CRISPR-Cas9\" and \"COVID-19\"?\n",
    "\n",
    "3. **Subword splits**: Notice `Ġ` (GPT-2) and `##` (BERT) - these mark word boundaries. Why encode this?\n",
    "\n",
    "4. **Trade-offs**: BERT uncased has a smaller vocabulary but loses case. When might this matter for scientific text?\n",
    "\n",
    "5. **For your research**: What text structure is important in your domain? Would lowercasing hurt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9u664fy7t2q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "text = \"The CRISPR-Cas9 system enables precise DNA editing.\"\n",
    "\n",
    "print(f\"Text: '{text}'\\n\")\n",
    "\n",
    "print(\"GPT-2:\")\n",
    "visualize_tokens(text, tokenizer_gpt2)\n",
    "\n",
    "print(\"\\nBERT uncased:\")\n",
    "visualize_tokens(text, tokenizer_bert)\n",
    "\n",
    "print(\"\\nYours:\")\n",
    "visualize_tokens(text, your_tokenizer)\n",
    "\n",
    "print(f\"\\nToken counts: GPT-2={len(tokenizer_gpt2.tokenize(text))}, BERT={len(tokenizer_bert.tokenize(text))}, Yours={len(your_tokenizer.tokenize(text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985517e5-7d96-4dd4-a535-215216eb802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own text!\n",
    "your_text = \"Your scientific text here\"  # <-- Modify this!\n",
    "\n",
    "print(f\"Your text: '{your_text}'\\n\")\n",
    "\n",
    "print(\"GPT-2:\")\n",
    "visualize_tokens(your_text, tokenizer_gpt2)\n",
    "\n",
    "print(\"\\nBERT uncased:\")\n",
    "visualize_tokens(your_text, tokenizer_bert)\n",
    "\n",
    "print(\"\\nYours:\")\n",
    "visualize_tokens(your_text, your_tokenizer)\n",
    "\n",
    "print(f\"\\nToken counts: GPT-2={len(tokenizer_gpt2.tokenize(your_text))}, BERT={len(tokenizer_bert.tokenize(your_text))}, Yours={len(your_tokenizer.tokenize(your_text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## From Tokens to Token IDs\n",
    "\n",
    "Each tokenizer has a fixed **vocabulary** - a dictionary mapping tokens to integer IDs:\n",
    "\n",
    "| Model | Vocabulary Size | Tokenization |\n",
    "|-------|-----------------|--------------|\n",
    "| GPT-2 | ~50,000 tokens | BPE |\n",
    "| BERT uncased | ~30,000 tokens | WordPiece |\n",
    "\n",
    "When you tokenize, each token gets a unique ID from the vocabulary:\n",
    "```\n",
    "\"Machine\"  → 33423\n",
    "\"learning\" → 4673\n",
    "```\n",
    "\n",
    "**The Problem: Token IDs are just arbitrary integers!**\n",
    "\n",
    "Is token 33423 similar to token 33424? We have no idea - they're just numbers in a lookup table.\n",
    "\n",
    "**This is where embeddings come in →**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Embeddings - Meaning Becomes Geometry\n",
    "\n",
    "The key insight of modern NLP:\n",
    "\n",
    "> **Convert tokens into vectors where similar things are close together.**\n",
    "\n",
    "```\n",
    "\"King\"   → [0.2, -0.4, 0.8, 0.1, ...] (384 numbers)\n",
    "\"Queen\"  → [0.3, -0.3, 0.7, 0.2, ...] (384 numbers)  ← Close to \"King\"!\n",
    "\"Banana\" → [-0.5, 0.9, -0.2, 0.6, ...] (384 numbers)  ← Far from both\n",
    "```\n",
    "\n",
    "This is the magic of **embeddings**: meaning becomes geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## The Naive Approach: One-Hot Encoding\n",
    "\n",
    "The simplest way to represent words as vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding: each word is a vector with one \"1\" and rest \"0\"s\n",
    "one_hot = {\n",
    "    \"cat\":  np.array([1, 0, 0, 0, 0]),\n",
    "    \"dog\":  np.array([0, 1, 0, 0, 0]),\n",
    "    \"fish\": np.array([0, 0, 1, 0, 0]),\n",
    "    \"bird\": np.array([0, 0, 0, 1, 0]),\n",
    "    \"tree\": np.array([0, 0, 0, 0, 1]),\n",
    "}\n",
    "\n",
    "print(\"One-hot representations:\")\n",
    "for word, vec in one_hot.items():\n",
    "    print(f\"  {word:6} → {vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the distance between words?\n",
    "cat, dog, tree = one_hot[\"cat\"], one_hot[\"dog\"], one_hot[\"tree\"]\n",
    "\n",
    "print(f\"Distance 'cat' ↔ 'dog':  {np.linalg.norm(cat - dog):.2f}\")\n",
    "print(f\"Distance 'cat' ↔ 'tree': {np.linalg.norm(cat - tree):.2f}\")\n",
    "print()\n",
    "print(\"Problem: ALL words are equally distant from each other!\")\n",
    "print(\"We lose the semantic relationship: cat and dog are both animals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## The Better Approach: Dense Embeddings\n",
    "\n",
    "Instead of sparse one-hot vectors, we use **dense vectors** where each dimension captures some aspect of meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual example of learned embeddings\n",
    "# (Real embeddings have hundreds of dimensions, learned from data)\n",
    "#\n",
    "#                [is_animal, has_fur, can_fly, is_pet]\n",
    "embeddings = {\n",
    "    \"cat\":   np.array([0.9, 0.9, 0.8, 0.9]),\n",
    "    \"dog\":   np.array([0.9, 0.9, 0.9, 0.9]),\n",
    "    \"fish\":  np.array([0.9, 0.8, 0.7, 0.0]),\n",
    "    \"bird\":  np.array([0.9, 0.8, 0.9, 0.3]),\n",
    "    \"tree\":  np.array([0.9, 0.0, 0.0, 0.0]),\n",
    "}\n",
    "\n",
    "# Now distances reflect semantic similarity!\n",
    "cat, dog, tree = embeddings[\"cat\"], embeddings[\"dog\"], embeddings[\"tree\"]\n",
    "\n",
    "print(\"With dense embeddings:\")\n",
    "print(f\"  Distance 'cat' ↔ 'dog':  {np.linalg.norm(cat - dog):.2f}  (close - both pets!)\")\n",
    "print(f\"  Distance 'cat' ↔ 'tree': {np.linalg.norm(cat - tree):.2f}  (far - different things)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Looking at the conceptual embeddings above:\n",
    "1. Why are \"cat\" and \"dog\" close in this embedding space?\n",
    "2. What other words would you expect to be close to \"bird\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Cosine Similarity - Measuring Closeness\n",
    "\n",
    "To measure similarity between embeddings, we use **cosine similarity**:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- Measures the **angle** between vectors (not distance)\n",
    "- Range: -1 (opposite) to +1 (identical direction)\n",
    "- Scale-invariant: `[1, 2, 3]` and `[2, 4, 6]` have similarity = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    \"\"\"Cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compare all pairs\n",
    "words = list(embeddings.keys())\n",
    "print(\"Cosine similarities:\\n\")\n",
    "\n",
    "for i, w1 in enumerate(words):\n",
    "    for w2 in words[i+1:]:\n",
    "        sim = cosine_sim(embeddings[w1], embeddings[w2])\n",
    "        bar = \"█\" * int(max(0, sim) * 20)\n",
    "        print(f\"  {w1:5} ↔ {w2:5}: {sim:+.2f}  {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: How Models Learn Embeddings\n",
    "\n",
    "## Token IDs Are Categorical, Not Ordinal\n",
    "\n",
    "Remember: token IDs are **arbitrary integers**. Token 2300 is not \"less than\" token 6500 - they're just different categories in a lookup table.\n",
    "\n",
    "```\n",
    "\"cat\"      → 3797    (just an ID, no meaning)\n",
    "\"dog\"      → 3826    (close number ≠ close meaning!)\n",
    "\"computer\" → 3644    (between cat and dog numerically, but unrelated semantically)\n",
    "```\n",
    "\n",
    "**In models, we treat token IDs as categorical data.** During training (classification, translation, reconstruction...), the model learns a **dense vector representation** for each category.\n",
    "\n",
    "## The Embedding Layer: `nn.Embedding`\n",
    "\n",
    "In PyTorch, `nn.Embedding` is simply a **learnable lookup table**:\n",
    "- Input: token ID (integer)\n",
    "- Output: dense vector (learned during training)\n",
    "\n",
    "```\n",
    "nn.Embedding(vocab_size=50000, embedding_dim=128)\n",
    "         ↓\n",
    "Token ID 3797 → [0.12, -0.34, 0.56, ...] (128 numbers)\n",
    "```\n",
    "\n",
    "Initially, these vectors are **random**. Through training, the model adjusts them so that tokens used in similar contexts get similar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create an embedding layer (like in real models)\n",
    "vocab_size = 50000   # Number of possible tokens\n",
    "embedding_dim = 128  # Size of each embedding vector\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Look up embeddings for some token IDs\n",
    "token_ids = torch.tensor([3797, 3826, 3644])  # \"cat\", \"dog\", \"computer\" (hypothetical)\n",
    "\n",
    "# Get embeddings (random at initialization!)\n",
    "embeddings = embedding_layer(token_ids)\n",
    "\n",
    "print(f\"Embedding layer: {vocab_size:,} tokens → {embedding_dim}D vectors\")\n",
    "print(f\"\\nToken IDs: {token_ids.tolist()}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"\\nEmbedding for token 3797 (first 10 values):\")\n",
    "print(f\"  {embeddings[0, :10].detach().numpy().round(3)}\")\n",
    "print(f\"\\n⚠️  These are RANDOM - no meaning yet!\")\n",
    "print(f\"   The model learns meaningful embeddings during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7mniknnjul",
   "metadata": {},
   "source": [
    "## Leveraging Pre-trained Models\n",
    "\n",
    "Training meaningful embeddings requires data, compute, and time. Instead, you can use **pre-trained models** that have already learned rich representations from large datasets.\n",
    "\n",
    "## From Token Embeddings to Sequence Embeddings\n",
    "\n",
    "Pre-trained models give embeddings for each **token**. To embed a whole **sequence** (sentence, document), a common approach is **mean pooling**:\n",
    "\n",
    "```\n",
    "\"The cat sat\" → tokens: [\"The\", \"cat\", \"sat\"]\n",
    "             → embeddings: [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n",
    "             → mean: [0.3, 0.4]  ← single vector for the sequence\n",
    "```\n",
    "\n",
    "This is what **Sentence Transformers** do (with some refinements).\n",
    "\n",
    "## What Can You Do With Embeddings?\n",
    "\n",
    "1. **Representation & visualization**: Understand your data structure\n",
    "2. **Search**: Find similar items without keyword matching (see Part 6)\n",
    "3. **Train a classifier**: Use embeddings as features for a smaller model (see TP1 Part 3)\n",
    "\n",
    "Let's load a pre-trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3rg244zkph9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained model (trained on 1+ billion sentence pairs)\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# TODO: Choose your own embedding model from https://huggingface.co/models?library=sentence-transformers\n",
    "# Examples: \"sentence-transformers/all-mpnet-base-v2\", \"BAAI/bge-small-en-v1.5\"\n",
    "your_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')  # <-- Modify this!\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"\\nYour model loaded!\")\n",
    "print(f\"Your model embedding dimension: {your_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed some sentences\n",
    "sentences = [\n",
    "    \"The cat is sleeping on the couch.\",\n",
    "    \"A dog is resting on the sofa.\",\n",
    "    \"Machine learning transforms data into insights.\",\n",
    "    \"Deep learning models can recognize images.\",\n",
    "    \"The weather is sunny today.\",\n",
    "]\n",
    "\n",
    "# Compare both models\n",
    "embeddings = model.encode(sentences)\n",
    "embeddings_yours = your_model.encode(sentences)\n",
    "\n",
    "print(f\"Embedded {len(sentences)} sentences\")\n",
    "print(f\"\\nDefault model: each sentence → {embeddings.shape[1]} numbers\")\n",
    "print(f\"Your model:    each sentence → {embeddings_yours.shape[1]} numbers\")\n",
    "print(f\"\\nFirst embedding from default model (first 8 values):\")\n",
    "print(f\"  {embeddings[0][:8]}...\")\n",
    "print(f\"\\nFirst embedding from your model (first 8 values):\")\n",
    "print(f\"  {embeddings_yours[0][:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Visualizing Similarity\n",
    "\n",
    "Let's see if the model understands that semantically similar sentences should have similar embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrices for both models\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "sim_matrix_yours = cosine_similarity(embeddings_yours)\n",
    "\n",
    "# Create short labels\n",
    "labels = [s[:25] + \"...\" if len(s) > 25 else s for s in sentences]\n",
    "\n",
    "# Default model\n",
    "fig = plot_similarity_matrix(sim_matrix, labels, title=\"Default Model (all-MiniLM-L6-v2)\")\n",
    "plt.show()\n",
    "\n",
    "# Your model\n",
    "fig = plot_similarity_matrix(sim_matrix_yours, labels, title=\"Your Model\", cmap='Greens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Look at the similarity matrix:\n",
    "1. Which two sentences are most similar? Why?\n",
    "2. Why is \"The weather is sunny today\" different from all others?\n",
    "3. Notice that \"cat/couch\" and \"dog/sofa\" are similar - the model understands synonyms!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Visualizing the Embedding Space\n",
    "\n",
    "Embeddings have 384 dimensions. We can project them to 2D using **PCA** to visualize clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More diverse sentences\n",
    "sentences_diverse = [\n",
    "    # Animals\n",
    "    \"The cat is sleeping.\",\n",
    "    \"Dogs are loyal companions.\",\n",
    "    \"Fish swim in the ocean.\",\n",
    "    # Technology\n",
    "    \"Computers process data quickly.\",\n",
    "    \"Software engineers write code.\",\n",
    "    \"Artificial intelligence is advancing.\",\n",
    "    # Nature\n",
    "    \"Mountains are covered in snow.\",\n",
    "    \"The forest is full of trees.\",\n",
    "    \"Rivers flow to the sea.\",\n",
    "]\n",
    "\n",
    "categories = [\"Animals\"] * 3 + [\"Technology\"] * 3 + [\"Nature\"] * 3\n",
    "\n",
    "# Compute embeddings with both models\n",
    "emb = model.encode(sentences_diverse)\n",
    "emb_yours = your_model.encode(sentences_diverse)\n",
    "\n",
    "print(f\"Embedded {len(sentences_diverse)} sentences\")\n",
    "print(f\"Default model: {emb.shape[1]}D embeddings\")\n",
    "print(f\"Your model: {emb_yours.shape[1]}D embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with PCA - Default model\n",
    "short_labels = [s[:18] + \"...\" if len(s) > 18 else s for s in sentences_diverse]\n",
    "\n",
    "fig = plot_embeddings_pca(emb, labels=short_labels, categories=categories, \n",
    "                          title=\"Default Model - Embedding Space (PCA)\", annotate=True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize with PCA - Your model\n",
    "fig = plot_embeddings_pca(emb_yours, labels=short_labels, categories=categories, \n",
    "                          title=\"Your Model - Embedding Space (PCA)\", annotate=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "1. Do sentences from the same category cluster together in both models?\n",
    "2. How do the clusters differ between the default model and your chosen model?\n",
    "3. PCA typically explains ~30-40% of variance. What does this tell us about the embedding space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Application - Semantic Search\n",
    "\n",
    "One powerful application: **find similar items without keyword matching**.\n",
    "\n",
    "Unlike keyword search, semantic search understands meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"database\" of scientific topics\n",
    "database = [\n",
    "    \"CRISPR gene editing technology for treating genetic diseases\",\n",
    "    \"Protein folding prediction using deep learning models\",\n",
    "    \"Climate change impact on marine ecosystems\",\n",
    "    \"Quantum computing algorithms for optimization problems\",\n",
    "    \"Drug discovery using molecular simulations\",\n",
    "    \"Machine learning for analyzing medical images\",\n",
    "    \"Renewable energy storage in batteries\",\n",
    "    \"Neuroimaging techniques for brain research\",\n",
    "]\n",
    "\n",
    "# Pre-compute embeddings for the database with both models\n",
    "db_embeddings = model.encode(database)\n",
    "db_embeddings_yours = your_model.encode(database)\n",
    "\n",
    "print(f\"Database: {len(database)} items\")\n",
    "print(f\"Default model: {db_embeddings.shape[1]}D embeddings\")\n",
    "print(f\"Your model: {db_embeddings_yours.shape[1]}D embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with both models!\n",
    "query = \"How can AI help with biology?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEFAULT MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "results = semantic_search(query, database, db_embeddings, model, top_k=3)\n",
    "print_search_results(query, results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"YOUR MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "results_yours = semantic_search(query, database, db_embeddings_yours, your_model, top_k=3)\n",
    "print_search_results(query, results_yours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Exercise: Try Your Own Search\n",
    "\n",
    "Modify the query below. Try queries related to your research!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different queries!\n",
    "my_query = \"neural networks for medical diagnosis\"  # <-- Modify this!\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DEFAULT MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "results = semantic_search(my_query, database, db_embeddings, model, top_k=3)\n",
    "print_search_results(my_query, results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"YOUR MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "results_yours = semantic_search(my_query, database, db_embeddings_yours, your_model, top_k=3)\n",
    "print_search_results(my_query, results_yours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "1. Try a query that doesn't use any exact words from the database. Does it still work?\n",
    "2. Do the two models return the same results? Why might they differ?\n",
    "3. Why does semantic search work without keyword matching?\n",
    "4. Can you think of cases where it might fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The pipeline**: Text → Tokens → Token IDs → **Embeddings**\n",
    "\n",
    "2. **Embeddings** are dense vectors where similar things are close\n",
    "   - One-hot: all words equidistant (bad)\n",
    "   - Dense embeddings: capture semantic similarity (good)\n",
    "\n",
    "3. **Pre-trained models** let you leverage knowledge from massive datasets\n",
    "   - Even with small data, you benefit from billions of training examples\n",
    "   - This is the power of **transfer learning**\n",
    "\n",
    "4. **Cosine similarity** measures how close embeddings are\n",
    "\n",
    "5. **Applications**: semantic search, clustering, classification, visualization\n",
    "\n",
    "## The Key Insight\n",
    "\n",
    "> **Meaning becomes geometry.** With good embeddings, reasoning about meaning becomes reasoning about distances and directions in vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "Before moving to Part 2, think about:\n",
    "\n",
    "1. **In your research domain**, what data could be embedded? (text, sequences, structures?)\n",
    "\n",
    "2. **What would \"similarity\" mean** for your data? What should be close in embedding space?\n",
    "\n",
    "3. **How could pre-trained models help** your research? Do you have limited data? Limited compute?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
