{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# TP1 - Part 1: Tokenization & Embeddings\n",
    "\n",
    "**Day 2 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day2/tp1_part1.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this practical, you will understand:\n",
    "\n",
    "1. **The complete pipeline**: Text → Tokens → Token IDs → Embeddings\n",
    "2. **What embeddings are**: Dense vector representations where similar things are close\n",
    "3. **Why pre-trained models matter**: Leveraging knowledge from massive datasets\n",
    "4. **How to use embeddings**: Similarity search and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Pipeline - From Text to Vectors\n",
    "\n",
    "Neural networks only understand numbers. To process text (or molecules, proteins, DNA), we need to convert it to numbers.\n",
    "\n",
    "```\n",
    "┌─────────────┐    ┌──────────────┐    ┌──────────────┐    ┌──────────────────┐\n",
    "│  Raw Text   │ →  │   Tokens     │ →  │  Token IDs   │ →  │    Embeddings    │\n",
    "│  (string)   │    │  (subwords)  │    │  (integers)  │    │ (dense vectors)  │\n",
    "└─────────────┘    └──────────────┘    └──────────────┘    └──────────────────┘\n",
    "\n",
    "\"The cat sat\"  →  [\"The\", \"cat\", \"sat\"] →  [464, 3797, 3332]  →  [[0.1, -0.2, ...], \n",
    "                                                                   [0.3, 0.5, ...],\n",
    "                                                                   [-0.1, 0.4, ...]]\n",
    "```\n",
    "\n",
    "In this practical, we'll explore **tokenization** (how text is split) and then focus on **embeddings** (how tokens become meaningful vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "!pip install -q transformers sentence-transformers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from aiforscience import (\n",
    "    visualize_tokens,\n",
    "    plot_similarity_matrix,\n",
    "    semantic_search,\n",
    "    print_search_results,\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Tokenization: Breaking Text into Pieces\n",
    "\n",
    "Before we can process text, we need to **tokenize** it - break it into smaller units.\n",
    "\n",
    "### Three Tokenization Strategies\n",
    "\n",
    "| Strategy | How it works | Example: \"unhappiness\" | Pros/Cons |\n",
    "|----------|--------------|------------------------|-----------|\n",
    "| **Character** | Split into characters | `['u','n','h','a','p','p','i','n','e','s','s']` | ✓ Small vocab, ✗ Very long sequences |\n",
    "| **Word** | Split by spaces/punctuation | `['unhappiness']` | ✓ Meaningful units, ✗ Huge vocab, can't handle new words |\n",
    "| **Subword** | Split into common subparts | `['un', 'happiness']` | ✓ Balanced! Handles new words, reasonable vocab |\n",
    "\n",
    "**Modern models use subword tokenization** (like BPE - Byte Pair Encoding) because:\n",
    "- Frequent words stay whole: \"the\", \"cat\"\n",
    "- Rare words are split: \"transformers\" → \"transform\" + \"ers\"\n",
    "- Can handle any word, even typos: \"looooong\" → \"l\" + \"oo\" + \"oo\" + \"ong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load two different tokenizers to compare\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TOKENIZER COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n{'Model':<20} {'Vocabulary Size':<20}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'GPT-2':<20} {len(tokenizer_gpt2):,}\")\n",
    "print(f\"{'BERT':<20} {len(tokenizer_bert):,}\")\n",
    "print(f\"\\nGPT-2 has ~20,000 more tokens than BERT!\")\n",
    "print(\"Different models make different tokenization choices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fmej404vfk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare how the SAME text is tokenized by different models\n",
    "examples = [\n",
    "    \"Machine learning transforms scientific research.\",\n",
    "    \"photosynthesis\",\n",
    "    \"CRISPR-Cas9\",\n",
    "    \"COVID-19 vaccine\",\n",
    "]\n",
    "\n",
    "print(\"How different tokenizers split the SAME text:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for text in examples:\n",
    "    tokens_gpt2 = tokenizer_gpt2.tokenize(text)\n",
    "    tokens_bert = tokenizer_bert.tokenize(text)\n",
    "    \n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"  GPT-2 ({len(tokens_gpt2)} tokens): {tokens_gpt2}\")\n",
    "    print(f\"  BERT  ({len(tokens_bert)} tokens): {tokens_bert}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yi22dj4l6c",
   "metadata": {},
   "source": [
    "### Question: Tokenization Choices\n",
    "\n",
    "Look at the outputs above and think about:\n",
    "\n",
    "1. **Why do GPT-2 and BERT tokenize the same text differently?** (Hint: they were trained on different data with different goals)\n",
    "\n",
    "2. **BERT lowercases everything** (notice \"machine\" vs \"Machine\"). What are the trade-offs?\n",
    "   - Advantage: \"Machine\" and \"machine\" become the same token\n",
    "   - Disadvantage: We lose information (acronyms like \"DNA\" become \"dna\")\n",
    "\n",
    "3. **For scientific text**, which tokenizer seems to handle domain terms better? Why might this matter for your research?\n",
    "\n",
    "4. **Token count matters!** Models have a maximum context length (e.g., 512 or 4096 tokens). If your text is tokenized into more pieces, you can fit less content. How might this affect working with scientific papers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9u664fy7t2q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of tokenization\n",
    "text = \"The CRISPR-Cas9 system enables precise DNA editing.\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 4))\n",
    "\n",
    "# GPT-2\n",
    "plt.sca(axes[0])\n",
    "visualize_tokens(text, tokenizer_gpt2)\n",
    "axes[0].set_title(\"GPT-2 Tokenization\", fontsize=11, fontweight='bold')\n",
    "\n",
    "# BERT\n",
    "plt.sca(axes[1])\n",
    "visualize_tokens(text, tokenizer_bert)\n",
    "axes[1].set_title(\"BERT Tokenization\", fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGPT-2: {len(tokenizer_gpt2.tokenize(text))} tokens\")\n",
    "print(f\"BERT:  {len(tokenizer_bert.tokenize(text))} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## From Tokens to Token IDs\n",
    "\n",
    "Each tokenizer has a fixed **vocabulary** - a dictionary mapping tokens to integer IDs:\n",
    "\n",
    "| Model | Vocabulary Size |\n",
    "|-------|-----------------|\n",
    "| GPT-2 | 50,257 tokens |\n",
    "| BERT | 30,522 tokens |\n",
    "\n",
    "When you tokenize text, each token gets a unique ID from the vocabulary:\n",
    "```\n",
    "\"Machine\"  → 33423\n",
    "\"learning\" → 4673\n",
    "\" research\" → 2267\n",
    "```\n",
    "\n",
    "**The Problem: Token IDs are just arbitrary integers!**\n",
    "\n",
    "Is token 33423 similar to token 33424? We have no idea - they're just numbers in a lookup table. The number itself carries no meaning.\n",
    "\n",
    "**This is where embeddings come in →**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Embeddings - Meaning Becomes Geometry\n",
    "\n",
    "The key insight of modern NLP:\n",
    "\n",
    "> **Convert tokens into vectors where similar things are close together.**\n",
    "\n",
    "```\n",
    "\"King\"   → [0.2, -0.4, 0.8, 0.1, ...] (384 numbers)\n",
    "\"Queen\"  → [0.3, -0.3, 0.7, 0.2, ...] (384 numbers)  ← Close to \"King\"!\n",
    "\"Banana\" → [-0.5, 0.9, -0.2, 0.6, ...] (384 numbers)  ← Far from both\n",
    "```\n",
    "\n",
    "This is the magic of **embeddings**: meaning becomes geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## The Naive Approach: One-Hot Encoding\n",
    "\n",
    "The simplest way to represent words as vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding: each word is a vector with one \"1\" and rest \"0\"s\n",
    "one_hot = {\n",
    "    \"cat\":  np.array([1, 0, 0, 0, 0]),\n",
    "    \"dog\":  np.array([0, 1, 0, 0, 0]),\n",
    "    \"fish\": np.array([0, 0, 1, 0, 0]),\n",
    "    \"bird\": np.array([0, 0, 0, 1, 0]),\n",
    "    \"tree\": np.array([0, 0, 0, 0, 1]),\n",
    "}\n",
    "\n",
    "print(\"One-hot representations:\")\n",
    "for word, vec in one_hot.items():\n",
    "    print(f\"  {word:6} → {vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the distance between words?\n",
    "cat, dog, tree = one_hot[\"cat\"], one_hot[\"dog\"], one_hot[\"tree\"]\n",
    "\n",
    "print(f\"Distance 'cat' ↔ 'dog':  {np.linalg.norm(cat - dog):.2f}\")\n",
    "print(f\"Distance 'cat' ↔ 'tree': {np.linalg.norm(cat - tree):.2f}\")\n",
    "print()\n",
    "print(\"Problem: ALL words are equally distant from each other!\")\n",
    "print(\"We lose the semantic relationship: cat and dog are both animals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## The Better Approach: Dense Embeddings\n",
    "\n",
    "Instead of sparse one-hot vectors, we use **dense vectors** where each dimension captures some aspect of meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual example of learned embeddings\n",
    "# (Real embeddings have hundreds of dimensions, learned from data)\n",
    "#\n",
    "#                [is_animal, has_fur, can_fly, is_pet]\n",
    "embeddings = {\n",
    "    \"cat\":   np.array([0.9, 0.8, 0.0, 0.7]),\n",
    "    \"dog\":   np.array([0.9, 0.9, 0.0, 0.9]),\n",
    "    \"fish\":  np.array([0.7, 0.0, 0.0, 0.4]),\n",
    "    \"bird\":  np.array([0.8, 0.0, 0.9, 0.5]),\n",
    "    \"tree\":  np.array([0.0, 0.0, 0.0, 0.0]),\n",
    "}\n",
    "\n",
    "# Now distances reflect semantic similarity!\n",
    "cat, dog, tree = embeddings[\"cat\"], embeddings[\"dog\"], embeddings[\"tree\"]\n",
    "\n",
    "print(\"With dense embeddings:\")\n",
    "print(f\"  Distance 'cat' ↔ 'dog':  {np.linalg.norm(cat - dog):.2f}  (close - both pets!)\")\n",
    "print(f\"  Distance 'cat' ↔ 'tree': {np.linalg.norm(cat - tree):.2f}  (far - different things)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Looking at the conceptual embeddings above:\n",
    "1. Why are \"cat\" and \"dog\" close in this embedding space?\n",
    "2. What other words would you expect to be close to \"bird\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Cosine Similarity - Measuring Closeness\n",
    "\n",
    "To measure similarity between embeddings, we use **cosine similarity**:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- Measures the **angle** between vectors (not distance)\n",
    "- Range: -1 (opposite) to +1 (identical direction)\n",
    "- Scale-invariant: `[1, 2, 3]` and `[2, 4, 6]` have similarity = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    \"\"\"Cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compare all pairs\n",
    "words = list(embeddings.keys())\n",
    "print(\"Cosine similarities:\\n\")\n",
    "\n",
    "for i, w1 in enumerate(words):\n",
    "    for w2 in words[i+1:]:\n",
    "        sim = cosine_sim(embeddings[w1], embeddings[w2])\n",
    "        bar = \"█\" * int(max(0, sim) * 20)\n",
    "        print(f\"  {w1:5} ↔ {w2:5}: {sim:+.2f}  {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: The Power of Pre-trained Models\n",
    "\n",
    "The conceptual embeddings above were hand-crafted. In practice, we use models that **learned embeddings from massive datasets**.\n",
    "\n",
    "## Why This Matters for Scientists\n",
    "\n",
    "| Your Situation | Pre-trained Model Advantage |\n",
    "|----------------|----------------------------|\n",
    "| Small dataset (100s of samples) | Model learned from billions of examples |\n",
    "| Limited compute | Training already done for you |\n",
    "| Domain-specific task | General language understanding transfers |\n",
    "\n",
    "**Key insight**: You can leverage the knowledge captured by models trained on massive datasets, even if your own dataset is small.\n",
    "\n",
    "Let's use a real model: `all-MiniLM-L6-v2` (trained on 1+ billion sentence pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"\\nThis model was trained on over 1 billion sentence pairs.\")\n",
    "print(\"You get to use all that learning for free!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed some sentences\n",
    "sentences = [\n",
    "    \"The cat is sleeping on the couch.\",\n",
    "    \"A dog is resting on the sofa.\",\n",
    "    \"Machine learning transforms data into insights.\",\n",
    "    \"Deep learning models can recognize images.\",\n",
    "    \"The weather is sunny today.\",\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(f\"Embedded {len(sentences)} sentences\")\n",
    "print(f\"Each sentence → {embeddings.shape[1]} numbers\")\n",
    "print(f\"\\nFirst embedding (first 8 values):\")\n",
    "print(f\"  {embeddings[0][:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Visualizing Similarity\n",
    "\n",
    "Let's see if the model understands that semantically similar sentences should have similar embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Create short labels\n",
    "labels = [s[:25] + \"...\" if len(s) > 25 else s for s in sentences]\n",
    "\n",
    "fig = plot_similarity_matrix(sim_matrix, labels, title=\"Sentence Similarities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Look at the similarity matrix:\n",
    "1. Which two sentences are most similar? Why?\n",
    "2. Why is \"The weather is sunny today\" different from all others?\n",
    "3. Notice that \"cat/couch\" and \"dog/sofa\" are similar - the model understands synonyms!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Visualizing the Embedding Space\n",
    "\n",
    "Embeddings have 384 dimensions. We can project them to 2D using **PCA** to visualize clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More diverse sentences\n",
    "sentences_diverse = [\n",
    "    # Animals\n",
    "    \"The cat is sleeping.\",\n",
    "    \"Dogs are loyal companions.\",\n",
    "    \"Fish swim in the ocean.\",\n",
    "    # Technology\n",
    "    \"Computers process data quickly.\",\n",
    "    \"Software engineers write code.\",\n",
    "    \"Artificial intelligence is advancing.\",\n",
    "    # Nature\n",
    "    \"Mountains are covered in snow.\",\n",
    "    \"The forest is full of trees.\",\n",
    "    \"Rivers flow to the sea.\",\n",
    "]\n",
    "\n",
    "categories = [\"Animals\"] * 3 + [\"Technology\"] * 3 + [\"Nature\"] * 3\n",
    "\n",
    "# Compute embeddings and reduce to 2D\n",
    "emb = model.encode(sentences_diverse)\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(emb)\n",
    "\n",
    "print(f\"Reduced {emb.shape[1]}D → 2D\")\n",
    "print(f\"Variance explained: {sum(pca.explained_variance_ratio_)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "colors = {'Animals': 'red', 'Technology': 'blue', 'Nature': 'green'}\n",
    "\n",
    "for cat in set(categories):\n",
    "    mask = [c == cat for c in categories]\n",
    "    ax.scatter(emb_2d[mask, 0], emb_2d[mask, 1], \n",
    "               c=colors[cat], label=cat, s=120, alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "for i, sent in enumerate(sentences_diverse):\n",
    "    short = sent[:18] + \"...\" if len(sent) > 18 else sent\n",
    "    ax.annotate(short, (emb_2d[i, 0], emb_2d[i, 1]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('PCA 1', fontsize=11)\n",
    "ax.set_ylabel('PCA 2', fontsize=11)\n",
    "ax.set_title('Embedding Space (2D projection)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "1. Do sentences from the same category cluster together?\n",
    "2. PCA only explains ~30-40% of variance. What does this tell us about the embedding space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Application - Semantic Search\n",
    "\n",
    "One powerful application: **find similar items without keyword matching**.\n",
    "\n",
    "Unlike keyword search, semantic search understands meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"database\" of scientific topics\n",
    "database = [\n",
    "    \"CRISPR gene editing technology for treating genetic diseases\",\n",
    "    \"Protein folding prediction using deep learning models\",\n",
    "    \"Climate change impact on marine ecosystems\",\n",
    "    \"Quantum computing algorithms for optimization problems\",\n",
    "    \"Drug discovery using molecular simulations\",\n",
    "    \"Machine learning for analyzing medical images\",\n",
    "    \"Renewable energy storage in batteries\",\n",
    "    \"Neuroimaging techniques for brain research\",\n",
    "]\n",
    "\n",
    "# Pre-compute embeddings for the database\n",
    "db_embeddings = model.encode(database)\n",
    "\n",
    "print(f\"Database: {len(database)} items, each embedded as {db_embeddings.shape[1]}D vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search!\n",
    "query = \"How can AI help with biology?\"\n",
    "\n",
    "results = semantic_search(query, database, db_embeddings, model, top_k=3)\n",
    "print_search_results(query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Exercise: Try Your Own Search\n",
    "\n",
    "Modify the query below. Try queries related to your research!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different queries!\n",
    "my_query = \"neural networks for medical diagnosis\"  # <-- Modify this!\n",
    "\n",
    "results = semantic_search(my_query, database, db_embeddings, model, top_k=3)\n",
    "print_search_results(my_query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "1. Try a query that doesn't use any exact words from the database. Does it still work?\n",
    "2. Why does semantic search work without keyword matching?\n",
    "3. Can you think of cases where it might fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The pipeline**: Text → Tokens → Token IDs → **Embeddings**\n",
    "\n",
    "2. **Embeddings** are dense vectors where similar things are close\n",
    "   - One-hot: all words equidistant (bad)\n",
    "   - Dense embeddings: capture semantic similarity (good)\n",
    "\n",
    "3. **Pre-trained models** let you leverage knowledge from massive datasets\n",
    "   - Even with small data, you benefit from billions of training examples\n",
    "   - This is the power of **transfer learning**\n",
    "\n",
    "4. **Cosine similarity** measures how close embeddings are\n",
    "\n",
    "5. **Applications**: semantic search, clustering, classification, visualization\n",
    "\n",
    "## The Key Insight\n",
    "\n",
    "> **Meaning becomes geometry.** With good embeddings, reasoning about meaning becomes reasoning about distances and directions in vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "Before moving to Part 2, think about:\n",
    "\n",
    "1. **In your research domain**, what data could be embedded? (text, sequences, structures?)\n",
    "\n",
    "2. **What would \"similarity\" mean** for your data? What should be close in embedding space?\n",
    "\n",
    "3. **How could pre-trained models help** your research? Do you have limited data? Limited compute?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
