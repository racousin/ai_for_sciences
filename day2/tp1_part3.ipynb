{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 - Part 3: Will Your Drug Reach the Brain?\n",
    "\n",
    "**Day 2 - AI for Sciences Summer School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day2/tp1_part3.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## The Scenario\n",
    "\n",
    "You are a computational chemist at a pharmaceutical company working on **Alzheimer's disease**. Your team has designed a promising new molecule called **AZD-2024** that might inhibit the formation of amyloid plaques in the brain.\n",
    "\n",
    "But before spending months on synthesis and animal testing, your manager asks:\n",
    "\n",
    "> *\"Can this molecule even reach the brain? If it can't cross the blood-brain barrier, the whole project is dead before it starts.\"*\n",
    "\n",
    "You decide to build an AI model to predict BBB permeability **before** any wet lab experiment.\n",
    "\n",
    "---\n",
    "\n",
    "## The Blood-Brain Barrier Challenge\n",
    "\n",
    "The **blood-brain barrier (BBB)** is a highly selective membrane that protects the brain from pathogens and toxins. Unfortunately, it also blocks ~98% of small-molecule drugs.\n",
    "\n",
    "```\n",
    "                          BLOOD VESSEL\n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "         Drug molecules floating in bloodstream\n",
    "              ðŸ’Š  ðŸ’Š  ðŸ’Š  ðŸ’Š  ðŸ’Š  ðŸ’Š\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "         BLOOD-BRAIN BARRIER (tight junctions)\n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "              ðŸ’Š              ðŸ’Š       â† Only some get through!\n",
    "                    BRAIN TISSUE\n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "For Alzheimer's drugs, BBB permeability is **essential** - the drug MUST reach the brain to work.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How to use **pre-trained embeddings as features** for classification\n",
    "2. How to build a classifier **starting simple**, then improving\n",
    "3. How to **evaluate** a classifier beyond accuracy\n",
    "4. How to **apply your model** to predict new molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Strategy\n",
    "\n",
    "## Transfer Learning with Embeddings\n",
    "\n",
    "We don't have millions of BBB-labeled molecules to train a model from scratch. But we CAN:\n",
    "\n",
    "1. Use **ChemBERTa** (trained on 77M molecules) to convert SMILES â†’ embeddings\n",
    "2. Train a **classifier** on top of these embeddings\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   SMILES    â”‚ â†’  â”‚   ChemBERTa    â”‚ â†’  â”‚  768-dim    â”‚ â†’  â”‚  Classifier  â”‚ â†’ Permeable?\n",
    "â”‚   string    â”‚    â”‚   (FROZEN)     â”‚    â”‚  embedding  â”‚    â”‚  (TRAINED)   â”‚    Yes / No\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   Pre-trained on                           What architecture\n",
    "                   77M molecules                            should we use?\n",
    "```\n",
    "\n",
    "**The question:** What classifier architecture should we use? Let's start with the **simplest possible model** and see how far it gets us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -q transformers torch pandas matplotlib scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Prepare the Data\n",
    "\n",
    "We'll use the **BBBP dataset** (Blood-Brain Barrier Penetration) containing ~2,000 molecules with experimentally measured BBB permeability.\n",
    "\n",
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# For Colab, download data if needed\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(\"Downloading data files...\")\n",
    "    !mkdir -p data\n",
    "    !wget -q -O data/molecules_bbbp.csv https://raw.githubusercontent.com/racousin/ai_for_sciences/main/day2/data/molecules_bbbp.csv\n",
    "    print(\"Data downloaded!\")\n",
    "else:\n",
    "    print(f\"Data directory found: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, \"molecules_bbbp.csv\"))\n",
    "\n",
    "print(f\"Dataset: {df.shape[0]} molecules with known BBB permeability\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['label_name'].value_counts())\n",
    "\n",
    "# Calculate baseline\n",
    "baseline_acc = df['label'].mean()\n",
    "print(f\"\\nâ†’ Baseline accuracy (always predict 'permeable'): {baseline_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Molecular Embeddings with ChemBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load ChemBERTa\n",
    "print(\"Loading ChemBERTa...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "chembert = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "chembert.eval()\n",
    "chembert.to(device)\n",
    "\n",
    "print(f\"ChemBERTa loaded! Embedding dimension: {chembert.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(smiles_list, batch_size=32):\n",
    "    \"\"\"Convert SMILES strings to embeddings using ChemBERTa.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(smiles_list), batch_size):\n",
    "        batch = smiles_list[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", \n",
    "                          padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = chembert(**inputs)\n",
    "            batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_emb)\n",
    "        \n",
    "        if (i // batch_size) % 20 == 0:\n",
    "            print(f\"  Processed {min(i+batch_size, len(smiles_list)):,}/{len(smiles_list):,}\")\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Compute embeddings\n",
    "print(\"Computing embeddings...\\n\")\n",
    "X = compute_embeddings(df['SMILES'].tolist())\n",
    "y = df['label'].values\n",
    "\n",
    "print(f\"\\nEmbedding matrix: {X.shape[0]} molecules Ã— {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert to PyTorch\n",
    "X_train_t = torch.FloatTensor(X_train)\n",
    "y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "X_test_t = torch.FloatTensor(X_test)\n",
    "y_test_t = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=64)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} molecules\")\n",
    "print(f\"Test set: {len(X_test)} molecules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: The Simplest Classifier\n",
    "\n",
    "Let's start with the **simplest possible model**: a single linear layer followed by sigmoid.\n",
    "\n",
    "```\n",
    "Input (768) â”€â”€â†’ Linear(1) â”€â”€â†’ Sigmoid â”€â”€â†’ Probability\n",
    "```\n",
    "\n",
    "This is essentially **logistic regression** - it learns a weighted sum of the 768 embedding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"The simplest possible classifier: just one linear layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=768):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "# Create model\n",
    "simple_model = SimpleClassifier(input_dim=768)\n",
    "simple_model.to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in simple_model.parameters())\n",
    "print(f\"Simple model: {n_params:,} parameters\")\n",
    "print(f\"  - 768 weights (one per embedding dimension)\")\n",
    "print(f\"  - 1 bias term\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, n_epochs=30, lr=0.001):\n",
    "    \"\"\"Train a model and return training history.\"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Train metrics\n",
    "            train_pred = model(X_train_t.to(device)).cpu()\n",
    "            train_acc = ((train_pred > 0.5).float() == y_train_t).float().mean().item()\n",
    "            \n",
    "            # Test metrics\n",
    "            test_pred = model(X_test_t.to(device)).cpu()\n",
    "            test_loss = criterion(test_pred, y_test_t).item()\n",
    "            test_acc = ((test_pred > 0.5).float() == y_test_t).float().mean().item()\n",
    "        \n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:>3}: Train Acc = {train_acc:.1%}, Test Acc = {test_acc:.1%}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the simple model\n",
    "print(\"Training simple model (Linear + Sigmoid)...\\n\")\n",
    "simple_history = train_model(simple_model, train_loader, test_loader, n_epochs=30)\n",
    "\n",
    "print(f\"\\nFinal test accuracy: {simple_history['test_acc'][-1]:.1%}\")\n",
    "print(f\"Baseline (always permeable): {baseline_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate a model and display results.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_proba = model(X_test_t.to(device)).cpu().numpy().flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {model_name} - Results\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"\\nAccuracy: {acc:.1%}  (Baseline: {baseline_acc:.1%})\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                  Predicted\")\n",
    "    print(f\"                  Neg    Pos\")\n",
    "    print(f\"  Actual Neg     {tn:4d}   {fp:4d}   (False Positives: {fp})\")\n",
    "    print(f\"  Actual Pos     {fn:4d}   {tp:4d}   (False Negatives: {fn})\")\n",
    "    \n",
    "    # Precision and Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    print(f\"\\nPrecision: {precision:.1%}  (of predicted permeable, how many actually are)\")\n",
    "    print(f\"Recall:    {recall:.1%}  (of actual permeable, how many we found)\")\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, \n",
    "            'fp': fp, 'fn': fn, 'y_pred_proba': y_pred_proba}\n",
    "\n",
    "simple_results = evaluate_model(simple_model, \"Simple Model (Linear + Sigmoid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Analyzing the Simple Model\n",
    "\n",
    "Look at the results above:\n",
    "\n",
    "1. How much better is the simple model compared to the baseline?\n",
    "2. How many **false negatives** does the model make? (drugs we wrongly reject)\n",
    "3. Do you think a more complex model could do better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Build a Better Model\n",
    "\n",
    "The simple linear model has limitations:\n",
    "- It can only learn **linear combinations** of features\n",
    "- It cannot capture **non-linear patterns** in the data\n",
    "\n",
    "## Exercise: Create an Improved Classifier\n",
    "\n",
    "Your task: Build a **Multi-Layer Perceptron (MLP)** with:\n",
    "- At least one hidden layer\n",
    "- Non-linear activation functions (ReLU)\n",
    "- Optionally: Dropout for regularization\n",
    "\n",
    "```\n",
    "Example architecture:\n",
    "Input (768) â†’ Linear(256) â†’ ReLU â†’ Linear(64) â†’ ReLU â†’ Linear(1) â†’ Sigmoid\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedClassifier(nn.Module):\n",
    "    \"\"\"TODO: Build a better classifier with hidden layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Replace this with a better architecture!\n",
    "        # Hint: Use nn.Sequential with multiple Linear layers,\n",
    "        # ReLU activations, and optionally Dropout\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),  # <-- Modify this!\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create and check your model\n",
    "improved_model = ImprovedClassifier(input_dim=768)\n",
    "improved_model.to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in improved_model.parameters())\n",
    "print(f\"Your model has {n_params:,} parameters\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(improved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your improved model\n",
    "print(\"Training your improved model...\\n\")\n",
    "improved_history = train_model(improved_model, train_loader, test_loader, n_epochs=50)\n",
    "\n",
    "print(f\"\\nYour model test accuracy: {improved_history['test_acc'][-1]:.1%}\")\n",
    "print(f\"Simple model test accuracy: {simple_history['test_acc'][-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your improved model\n",
    "improved_results = evaluate_model(improved_model, \"Your Improved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"                    MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Simple':<15} {'Improved':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Accuracy':<20} {simple_results['accuracy']:<15.1%} {improved_results['accuracy']:<15.1%}\")\n",
    "print(f\"{'Precision':<20} {simple_results['precision']:<15.1%} {improved_results['precision']:<15.1%}\")\n",
    "print(f\"{'Recall':<20} {simple_results['recall']:<15.1%} {improved_results['recall']:<15.1%}\")\n",
    "print(f\"{'False Positives':<20} {simple_results['fp']:<15} {improved_results['fp']:<15}\")\n",
    "print(f\"{'False Negatives':<20} {simple_results['fn']:<15} {improved_results['fn']:<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Baseline':<20} {baseline_acc:<15.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(simple_history['test_acc'], label='Simple Model', linewidth=2, linestyle='--')\n",
    "axes[0].plot(improved_history['test_acc'], label='Improved Model', linewidth=2)\n",
    "axes[0].axhline(y=baseline_acc, color='red', linestyle=':', label=f'Baseline ({baseline_acc:.1%})', alpha=0.7)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Test Accuracy Comparison', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(simple_history['test_loss'], label='Simple Model', linewidth=2, linestyle='--')\n",
    "axes[1].plot(improved_history['test_loss'], label='Improved Model', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Test Loss')\n",
    "axes[1].set_title('Test Loss Comparison', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Model Comparison\n",
    "\n",
    "1. Did your improved model beat the simple model? By how much?\n",
    "2. Did adding more layers help? What about dropout?\n",
    "3. Is there a sign of overfitting in either model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Understanding the Errors\n",
    "\n",
    "In drug discovery, **not all errors are equal**.\n",
    "\n",
    "- **False Positive**: We predict the drug crosses BBB, but it doesn't\n",
    "  - *Consequence*: Wasted experiments on drugs that won't reach the brain\n",
    "\n",
    "- **False Negative**: We predict the drug is blocked, but it actually crosses\n",
    "  - *Consequence*: We might discard a life-saving drug candidate!\n",
    "\n",
    "## Adjusting the Decision Threshold\n",
    "\n",
    "By default, we predict \"permeable\" if probability > 0.5. We can change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model for this analysis\n",
    "best_model = improved_model if improved_results['accuracy'] > simple_results['accuracy'] else simple_model\n",
    "y_proba = improved_results['y_pred_proba'] if improved_results['accuracy'] > simple_results['accuracy'] else simple_results['y_pred_proba']\n",
    "\n",
    "# Try different thresholds\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "print(\"Impact of Decision Threshold:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Threshold':>10} {'Accuracy':>12} {'Precision':>12} {'Recall':>12} {'False Neg':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_t = (y_proba > thresh).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred_t)\n",
    "    tp = ((y_pred_t == 1) & (y_test == 1)).sum()\n",
    "    fp = ((y_pred_t == 1) & (y_test == 0)).sum()\n",
    "    fn = ((y_pred_t == 0) & (y_test == 1)).sum()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    marker = \" â† default\" if thresh == 0.5 else \"\"\n",
    "    print(f\"{thresh:>10.1f} {acc:>12.1%} {precision:>12.1%} {recall:>12.1%} {fn:>12}{marker}\")\n",
    "\n",
    "print(\"\\nâ†’ Lower threshold = fewer false negatives (don't miss good drugs)\")\n",
    "print(\"â†’ Higher threshold = fewer false positives (don't waste on bad drugs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Threshold Selection\n",
    "\n",
    "In early-stage drug discovery for Alzheimer's, which errors are more costly?\n",
    "\n",
    "1. If we want to **not miss any potential drug** (minimize false negatives), what threshold should we use?\n",
    "2. What's the trade-off when we lower the threshold?\n",
    "3. How would you choose the threshold for a real drug discovery project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Predict Your Drug Candidate\n",
    "\n",
    "Now let's use our model for its intended purpose: **predicting whether new molecules can cross the BBB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bbb(smiles, model, name=\"Unknown\"):\n",
    "    \"\"\"Predict BBB permeability for a molecule.\"\"\"\n",
    "    inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emb = chembert(**inputs).last_hidden_state[:, 0, :]\n",
    "        prob = model(emb).cpu().item()\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our candidate molecule (Donepezil - an approved Alzheimer's drug)\n",
    "candidate_smiles = \"COc1cc2CC(CC2cc1OC)C(=O)N1CCc2ccccc2C1\"\n",
    "candidate_name = \"AZD-2024 (Donepezil analog)\"\n",
    "\n",
    "prob = predict_bbb(candidate_smiles, best_model, candidate_name)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"          BBB PERMEABILITY PREDICTION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMolecule: {candidate_name}\")\n",
    "print(f\"SMILES:   {candidate_smiles}\")\n",
    "print(f\"\\nPredicted probability: {prob:.1%}\")\n",
    "print(f\"\\nVerdict: {'PERMEABLE - Likely to cross BBB' if prob > 0.5 else 'IMPERMEABLE - Likely blocked'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if prob > 0.5:\n",
    "    print(\"\\nâ†’ Recommendation: Proceed with synthesis and testing.\")\n",
    "else:\n",
    "    print(\"\\nâ†’ Recommendation: Consider structural modifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Known Drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real drugs to test\n",
    "test_drugs = [\n",
    "    (\"Caffeine\", \"Cn1cnc2c1c(=O)n(c(=O)n2C)C\", \"SHOULD cross (CNS stimulant)\"),\n",
    "    (\"Diazepam\", \"CN1C(=O)CN=C(c2ccccc2)c3cc(Cl)ccc13\", \"SHOULD cross (sedative)\"),\n",
    "    (\"Ibuprofen\", \"CC(C)Cc1ccc(cc1)C(C)C(=O)O\", \"Should NOT cross (peripheral)\"),\n",
    "    (\"Penicillin G\", \"CC1(C)SC2C(NC(=O)Cc3ccccc3)C(=O)N2C1C(=O)O\", \"Should NOT cross (antibiotic)\"),\n",
    "]\n",
    "\n",
    "print(\"Predictions for Known Drugs:\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Drug':<15} {'Probability':>12} {'Prediction':<15} {'Expected'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for name, smiles, expected in test_drugs:\n",
    "    prob = predict_bbb(smiles, best_model, name)\n",
    "    pred = \"Permeable\" if prob > 0.5 else \"Impermeable\"\n",
    "    print(f\"{name:<15} {prob:>12.1%} {pred:<15} {expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Your Own Molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with your own molecule!\n",
    "my_smiles = \"CCO\"  # <-- Ethanol\n",
    "my_name = \"My molecule\"\n",
    "\n",
    "prob = predict_bbb(my_smiles, best_model, my_name)\n",
    "print(f\"\\n{my_name}: {prob:.1%} probability of crossing BBB\")\n",
    "print(f\"Prediction: {'Permeable' if prob > 0.5 else 'Impermeable'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What We Built\n",
    "\n",
    "```\n",
    "SMILES â†’ ChemBERTa (frozen) â†’ 768-dim embedding â†’ Classifier (trained) â†’ BBB Prediction\n",
    "```\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Start simple**: A linear classifier is a good baseline to beat\n",
    "\n",
    "2. **Transfer learning works**: ChemBERTa's embeddings encode useful chemistry - we just need a small classifier on top\n",
    "\n",
    "3. **Evaluation matters**: Accuracy isn't everything - consider which errors are costly for your application\n",
    "\n",
    "4. **Threshold tuning**: You can trade off precision vs recall based on your needs\n",
    "\n",
    "## This Pattern Works Everywhere\n",
    "\n",
    "| Domain | Pre-trained Model | Classification Task |\n",
    "|--------|-------------------|--------------------|\n",
    "| Molecules | ChemBERTa | Toxicity, solubility, BBB |\n",
    "| Proteins | ESM-2 | Function, stability |\n",
    "| DNA | DNABERT-2 | Promoter detection |\n",
    "| Text | SciBERT | Topic classification |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **In your research**, what classification problems could benefit from pre-trained embeddings?\n",
    "\n",
    "2. **What pre-trained model** exists for your data type?\n",
    "\n",
    "3. **Which errors** would be more costly in your application?\n",
    "\n",
    "4. **When might you need** to fine-tune the embedding model itself, instead of just training a classifier on top?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Save Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your improved model\n",
    "torch.save(improved_model.state_dict(), 'bbb_classifier.pt')\n",
    "print(\"Model saved to 'bbb_classifier.pt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
