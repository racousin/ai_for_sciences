{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2: Mathematical Problem Solving with LLMs\n",
    "\n",
    "**Day 2 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day2/tp2_bonus.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this practical, you will understand:\n",
    "\n",
    "1. **Context Engineering**: How prompt design affects LLM performance\n",
    "2. **Prompting Strategies**: Zero-shot, few-shot, and chain-of-thought\n",
    "3. **Fine-tuning with LoRA**: Adapting models efficiently with limited resources\n",
    "4. **Evaluation**: Measuring accuracy on mathematical reasoning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "!pip install -q transformers torch peft accelerate bitsandbytes\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Math Problems Dataset\n",
    "\n",
    "We have 900 math problems across different categories: arithmetic, algebra, geometry, fractions, percentages, and word problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from local file (or download if running on Colab)\n",
    "import os\n",
    "\n",
    "data_path = 'data/maths.csv'\n",
    "if not os.path.exists(data_path):\n",
    "    # Download from GitHub if not available locally\n",
    "    !mkdir -p data\n",
    "    !wget -q -O data/maths.csv https://raw.githubusercontent.com/racousin/ai_for_sciences/main/day2/data/maths.csv\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "print(f\"Dataset size: {len(data)} problems\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(data['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some example problems\n",
    "print(\"Sample problems:\\n\")\n",
    "for i in range(5):\n",
    "    row = data.iloc[i]\n",
    "    print(f\"[{row['category']}] {row['problem']}\")\n",
    "    print(f\"  Answer: {row['solution']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test set (100 problems) for evaluation\n",
    "test_data = data.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "train_data = data.drop(test_data.index).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train set: {len(train_data)} problems (for few-shot examples & fine-tuning)\")\n",
    "print(f\"Test set: {len(test_data)} problems (for evaluation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Utility Functions\n",
    "\n",
    "We need functions to extract numerical answers from model outputs and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(text):\n",
    "    \"\"\"Extract answer from JSON embedded in model response.\"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    \n",
    "    # Find JSON object containing \"answer\" in the text\n",
    "    match = re.search(r'\\{[^{}]*\"answer\"\\s*:\\s*[^{}]*\\}', text)\n",
    "    if match:\n",
    "        try:\n",
    "            data = json.loads(match.group())\n",
    "            return float(data[\"answer\"])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# Test the extraction function\n",
    "test_cases = [\n",
    "    '{\"thought\": \"5 + 3 = 8\", \"answer\": 8}',\n",
    "    '{\"thought\": \"The result is negative\", \"answer\": -15.5}',\n",
    "    'Solution: {\"thought\": \"calculating\", \"answer\": 42} more text',\n",
    "    'Some explanation\\n{\"thought\": \"...\", \"answer\": 100}\\nProblem: next',\n",
    "    'No JSON here',\n",
    "]\n",
    "\n",
    "print(\"Number extraction tests:\")\n",
    "for s in test_cases:\n",
    "    result = extract_number(s)\n",
    "    print(f\"  {s[:50]:50} -> {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, ground_truth, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Calculate accuracy with tolerance for floating point comparisons.\n",
    "    \n",
    "    Two values are considered equal if:\n",
    "    - They round to the same value at 2 decimal places, OR\n",
    "    - Their absolute difference is <= tolerance\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for pred, truth in zip(predictions, ground_truth):\n",
    "        if pred is None:\n",
    "            continue\n",
    "        if round(pred, 2) == round(truth, 2):\n",
    "            correct += 1\n",
    "        elif abs(pred - truth) <= tolerance:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Load Pre-trained Model\n",
    "\n",
    "We'll use a small instruction-tuned model that can run on limited hardware. Qwen2-0.5B-Instruct is a good choice: small enough for Colab, but capable of following instructions and doing basic math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small instruction-tuned model (works on Colab free tier)\n",
    "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set padding token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.0f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt, model, tokenizer, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate an answer using the specified prompt.\n",
    "    Uses greedy decoding for deterministic, reproducible outputs.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy decoding for reproducible outputs\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the prompt from the response\n",
    "    response = response[len(prompt):].strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Context Engineering - Prompting Strategies\n",
    "\n",
    "**Context engineering** (or prompt engineering) is the art of designing prompts that help LLMs perform better. The same model can produce very different results depending on how you ask!\n",
    "\n",
    "We'll explore three main strategies:\n",
    "\n",
    "1. **Zero-shot**: Just ask the question directly\n",
    "2. **Few-shot**: Provide examples before asking\n",
    "3. **Chain-of-Thought (CoT)**: Ask the model to reason step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1: Zero-shot Prompting\n",
    "\n",
    "The simplest approach: just ask the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zero_shot_prompt(problem):\n",
    "    \"\"\"Simple prompt - ask for JSON output.\"\"\"\n",
    "    return f'''Solve this math problem. Reply with JSON: {{\"thought\": \"your reasoning\", \"answer\": number}}\n",
    "\n",
    "Problem: {problem}'''\n",
    "\n",
    "# Test on one problem\n",
    "test_problem = test_data.iloc[0]\n",
    "prompt = make_zero_shot_prompt(test_problem['problem'])\n",
    "\n",
    "print(\"ZERO-SHOT PROMPT:\")\n",
    "print(\"-\" * 50)\n",
    "print(prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = generate_answer(prompt, model, tokenizer)\n",
    "extracted = extract_number(response)\n",
    "\n",
    "print(f\"\\nModel response: {response[:150]}{'...' if len(response) > 150 else ''}\")\n",
    "print(f\"Extracted answer: {extracted}\")\n",
    "print(f\"Correct answer: {test_problem['solution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: Few-shot Prompting\n",
    "\n",
    "Provide examples to show the model what format we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_few_shot_prompt(problem, n_examples=3):\n",
    "    \"\"\"Include examples showing the expected JSON format with real reasoning.\"\"\"\n",
    "    # Hand-crafted examples with actual step-by-step reasoning\n",
    "    examples = [\n",
    "        ('What is 15% of 80?', '{\"thought\": \"15% = 0.15, so 0.15 × 80 = 12\", \"answer\": 12}'),\n",
    "        ('Calculate 48 + 37', '{\"thought\": \"48 + 37 = 85\", \"answer\": 85}'),\n",
    "        ('What is the area of a rectangle with length 6 and width 4?', \n",
    "         '{\"thought\": \"Area = length × width = 6 × 4 = 24\", \"answer\": 24}'),\n",
    "    ]\n",
    "    \n",
    "    examples_text = \"\\n\\n\".join([f'Problem: {p}\\n{a}' for p, a in examples[:n_examples]])\n",
    "    return f'''Solve math problems. Reply with JSON: {{\"thought\": \"your reasoning\", \"answer\": number}}\n",
    "\n",
    "{examples_text}\n",
    "\n",
    "Problem: {problem}'''\n",
    "\n",
    "# Test on the same problem\n",
    "prompt = make_few_shot_prompt(test_problem['problem'])\n",
    "\n",
    "print(\"FEW-SHOT PROMPT:\")\n",
    "print(\"-\" * 50)\n",
    "print(prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = generate_answer(prompt, model, tokenizer)\n",
    "extracted = extract_number(response)\n",
    "\n",
    "print(f\"\\nModel response: {response[:150]}{'...' if len(response) > 150 else ''}\")\n",
    "print(f\"Extracted answer: {extracted}\")\n",
    "print(f\"Correct answer: {test_problem['solution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3: Chain-of-Thought Prompting\n",
    "\n",
    "Ask the model to reason step by step. This often improves performance on math problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cot_prompt(problem):\n",
    "    \"\"\"Chain-of-thought: ask for detailed reasoning in JSON.\"\"\"\n",
    "    return f'''Solve this math problem step by step. Reply with JSON: {{\"thought\": \"detailed step-by-step reasoning\", \"answer\": number}}\n",
    "\n",
    "Problem: {problem}'''\n",
    "\n",
    "# Test on the same problem\n",
    "prompt = make_cot_prompt(test_problem['problem'])\n",
    "\n",
    "print(\"CHAIN-OF-THOUGHT PROMPT:\")\n",
    "print(\"-\" * 50)\n",
    "print(prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = generate_answer(prompt, model, tokenizer, max_new_tokens=100)\n",
    "extracted = extract_number(response)\n",
    "\n",
    "print(f\"\\nModel response: {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
    "print(f\"Extracted answer: {extracted}\")\n",
    "print(f\"Correct answer: {test_problem['solution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare All Strategies\n",
    "\n",
    "Let's evaluate each strategy on a subset of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strategy(strategy_name, prompt_fn, test_subset, model, tokenizer, max_new_tokens=50):\n",
    "    \"\"\"Evaluate a prompting strategy on test data.\"\"\"\n",
    "    predictions = []\n",
    "    ground_truth = test_subset['solution'].tolist()\n",
    "    \n",
    "    for idx, row in test_subset.iterrows():\n",
    "        prompt = prompt_fn(row['problem'])\n",
    "        response = generate_answer(prompt, model, tokenizer, max_new_tokens)\n",
    "        pred = extract_number(response)\n",
    "        predictions.append(pred if pred is not None else 0.0)\n",
    "    \n",
    "    accuracy = compute_accuracy(predictions, ground_truth)\n",
    "    return accuracy, predictions\n",
    "\n",
    "# Evaluate on a small subset (20 problems) for speed\n",
    "eval_subset = test_data.head(20)\n",
    "\n",
    "print(\"Evaluating prompting strategies on 20 test problems...\\n\")\n",
    "\n",
    "strategies = {\n",
    "    'Zero-shot': make_zero_shot_prompt,\n",
    "    'Few-shot (3 examples)': make_few_shot_prompt,\n",
    "    'Chain-of-Thought': make_cot_prompt,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, prompt_fn in strategies.items():\n",
    "    print(f\"Testing {name}...\", end=\" \")\n",
    "    max_tokens = 100 if name == 'Chain-of-Thought' else 50\n",
    "    acc, preds = evaluate_strategy(name, prompt_fn, eval_subset, model, tokenizer, max_tokens)\n",
    "    results[name] = acc\n",
    "    print(f\"Accuracy: {acc:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "for name, acc in results.items():\n",
    "    bar = \"*\" * int(acc * 20)\n",
    "    print(f\"{name:25} {acc:6.1%}  {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "1. Which prompting strategy performed best? Why do you think that is?\n",
    "2. Qwen2-0.5B is a small model (500M parameters). How might results differ with larger models?\n",
    "3. Can you think of other prompting strategies that might help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Fine-tuning with LoRA\n",
    "\n",
    "When prompting isn't enough, we can **fine-tune** the model on our specific task.\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is an efficient fine-tuning technique that:\n",
    "- Freezes the original model weights\n",
    "- Adds small trainable matrices to specific layers\n",
    "- Reduces memory usage by 10-100x compared to full fine-tuning\n",
    "- Can be trained on consumer hardware\n",
    "\n",
    "```\n",
    "Original weight matrix W (frozen)\n",
    "         +\n",
    "LoRA matrices: A @ B (trainable, low-rank)\n",
    "         =\n",
    "Adapted weights: W + A @ B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before fine-tuning (important for Colab!)\n",
    "import gc\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared, ready for fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,                    # Rank of the update matrices (lower = fewer parameters)\n",
    "    lora_alpha=32,          # Scaling factor\n",
    "    lora_dropout=0.1,       # Dropout for regularization\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Attention layers to adapt\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the base model for fine-tuning\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nParameter comparison:\")\n",
    "print(f\"  Total parameters: {total_params / 1e6:.0f}M\")\n",
    "print(f\"  Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(f\"\\nWe're only training {trainable_params / total_params:.2%} of the model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "We need to format our math problems for causal language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        # Format as JSON output\n",
    "        text = f'Problem: {row[\"problem\"]}\\n{{\"thought\": \"solving\", \"answer\": {row[\"solution\"]}}}'\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': input_ids.clone()  # For causal LM, labels = input_ids\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "# Note: We use only 800 examples here for speed. For better results, try using\n",
    "# more data by adding your own math problems!\n",
    "train_subset = train_data.head(800)\n",
    "train_dataset = MathDataset(train_subset, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Training setup\n",
    "optimizer = AdamW(lora_model.parameters(), lr=1e-4)\n",
    "num_epochs = 3\n",
    "\n",
    "lora_model.train()\n",
    "losses = []\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = lora_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(losses, alpha=0.7)\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('LoRA Fine-tuning Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.eval()\n",
    "\n",
    "print(\"Comparing Prompting Strategies vs LoRA Fine-tuning\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get best prompting result from Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prompting = max(results.items(), key=lambda x: x[1])\n",
    "print(f\"\\nBest prompting strategy: {best_prompting[0]} ({best_prompting[1]:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate fine-tuned model (zero-shot, since it learned the task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_subset = test_data.head(20)\n",
    "print(\"Evaluating LoRA model (zero-shot)...\", end=\" \")\n",
    "lora_acc, _ = evaluate_strategy(\"LoRA\", make_zero_shot_prompt, eval_subset, lora_model, tokenizer)\n",
    "print(f\"Accuracy: {lora_acc:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"  Best prompting ({best_prompting[0]}): {best_prompting[1]:.1%}\")\n",
    "print(f\"  LoRA fine-tuned (zero-shot):  {lora_acc:.1%}\")\n",
    "if lora_acc > best_prompting[1]:\n",
    "    print(f\"  Improvement: +{lora_acc - best_prompting[1]:.1%}\")\n",
    "else:\n",
    "    print(f\"  (Fine-tuning may need more data or epochs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base model vs fine-tuned model\n",
    "lora_model.eval()\n",
    "\n",
    "print(\"Comparing Base Model vs LoRA Fine-tuned Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate both on same test subset\n",
    "eval_subset = test_data.head(20)\n",
    "\n",
    "# Base model (few-shot, since it performed best)\n",
    "print(\"\\nEvaluating base model (few-shot)...\", end=\" \")\n",
    "base_acc, _ = evaluate_strategy(\"Base\", make_few_shot_prompt, eval_subset, model, tokenizer)\n",
    "print(f\"Accuracy: {base_acc:.1%}\")\n",
    "\n",
    "# Fine-tuned model (zero-shot, since it learned the task)\n",
    "print(\"Evaluating LoRA model (zero-shot)...\", end=\" \")\n",
    "lora_acc, _ = evaluate_strategy(\"LoRA\", make_zero_shot_prompt, eval_subset, lora_model, tokenizer)\n",
    "print(f\"Accuracy: {lora_acc:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"  Base model (few-shot):     {base_acc:.1%}\")\n",
    "print(f\"  LoRA fine-tuned (zero-shot): {lora_acc:.1%}\")\n",
    "if lora_acc > base_acc:\n",
    "    print(f\"  Improvement: +{lora_acc - base_acc:.1%}\")\n",
    "else:\n",
    "    print(f\"  (Fine-tuning may need more data or epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "1. How does the fine-tuned model compare to the prompting approaches?\n",
    "2. Why might fine-tuning help (or not help) for this task?\n",
    "3. What are the trade-offs between prompting vs fine-tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Exercise - Improve the Results\n",
    "\n",
    "Now it's your turn! Try to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Better Prompts\n",
    "\n",
    "Design a better prompt template. Consider:\n",
    "- More specific instructions\n",
    "- Different number of few-shot examples\n",
    "- Category-specific examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your own prompt template\n",
    "def make_custom_prompt(problem):\n",
    "    \"\"\"Your custom prompt strategy. Use JSON format: {\"thought\": \"...\", \"answer\": X}\"\"\"\n",
    "    # <-- Modify this!\n",
    "    prompt = f'''Solve this math problem. Reply with JSON: {{\"thought\": \"your reasoning\", \"answer\": number}}\n",
    "\n",
    "Problem: {problem}'''\n",
    "    return prompt\n",
    "\n",
    "# Test your prompt\n",
    "test_problem = test_data.iloc[0]\n",
    "prompt = make_custom_prompt(test_problem['problem'])\n",
    "print(\"Your prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "response = generate_answer(prompt, model, tokenizer)\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Extracted: {extract_number(response)}\")\n",
    "print(f\"Correct: {test_problem['solution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Experiment with LoRA Parameters\n",
    "\n",
    "Try different LoRA configurations:\n",
    "- `r`: 4, 8, 16 (higher = more parameters)\n",
    "- `lora_alpha`: 16, 32, 64\n",
    "- More training epochs\n",
    "- Different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different LoRA configurations\n",
    "# Hint: Reload the base model and try different settings\n",
    "\n",
    "# Example:\n",
    "# new_lora_config = LoraConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM,\n",
    "#     r=16,  # <-- Try different values\n",
    "#     lora_alpha=64,\n",
    "#     lora_dropout=0.05,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],  # Attention layers for Qwen2\n",
    "# )\n",
    "\n",
    "print(\"Experiment with different configurations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Context Engineering (Prompting)\n",
    "- **Zero-shot**: Simple but often insufficient for complex tasks\n",
    "- **Few-shot**: Providing examples helps the model understand the task format\n",
    "- **Chain-of-Thought**: Step-by-step reasoning improves math performance\n",
    "- The same model can perform very differently based on how you prompt it\n",
    "\n",
    "### Fine-tuning with LoRA\n",
    "- **LoRA** adds small trainable matrices while keeping base model frozen\n",
    "- Much more efficient than full fine-tuning (trains ~0.1-1% of parameters)\n",
    "- Can be done on consumer hardware\n",
    "- Trade-off: requires training data and compute, but can outperform prompting\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "| Approach | When to Use |\n",
    "|----------|-------------|\n",
    "| Zero-shot | Quick experiments, simple tasks |\n",
    "| Few-shot | Have a few examples, need better format adherence |\n",
    "| Chain-of-Thought | Reasoning tasks (math, logic) |\n",
    "| LoRA Fine-tuning | Have training data, need maximum performance |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **In your research domain**, what tasks might benefit from context engineering vs fine-tuning?\n",
    "\n",
    "2. **What kind of examples** would you include in few-shot prompts for your domain?\n",
    "\n",
    "3. **If you were to fine-tune** a model for your research, what data would you use?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
