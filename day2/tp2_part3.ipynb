{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TP2 - Part 3: Classification with Embeddings\n",
        "\n",
        "**Day 2 - AI for Sciences Winter School**\n",
        "\n",
        "**Instructor:** Raphael Cousin\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day2/tp2_part3.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## The Task: Predicting Blood-Brain Barrier Permeability\n",
        "\n",
        "The **blood-brain barrier (BBB)** is a selective barrier that protects the brain from toxins in the bloodstream. For drug development, it's crucial to know whether a molecule can cross this barrier:\n",
        "\n",
        "- **BBB-permeable**: Drug can enter the brain (needed for CNS drugs)\n",
        "- **BBB-impermeable**: Drug cannot enter (needed for peripherally-acting drugs)\n",
        "\n",
        "**Our Goal**: Build a classifier that predicts BBB permeability using molecular embeddings.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. Understand how to **use embeddings as features** for classification\n",
        "2. Train a simple **MLP classifier** on molecular embeddings\n",
        "3. Evaluate model performance and understand what the model learns\n",
        "4. Experiment with different configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: The Classification Pipeline\n",
        "\n",
        "The key insight: **Embeddings transform the problem.**\n",
        "\n",
        "Instead of teaching a model to understand molecular structure from scratch, we:\n",
        "\n",
        "1. Use a **pre-trained model** (ChemBERTa) to convert SMILES ‚Üí embeddings\n",
        "2. Train a **simple classifier** on the embeddings\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   SMILES    ‚îÇ ‚Üí  ‚îÇ   ChemBERTa    ‚îÇ ‚Üí  ‚îÇ  768-dim    ‚îÇ ‚Üí  ‚îÇ   Classifier ‚îÇ\n",
        "‚îÇ   string    ‚îÇ    ‚îÇ  (frozen)      ‚îÇ    ‚îÇ  embedding  ‚îÇ    ‚îÇ   (trained)  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                    Pre-trained                              BBB_permeable\n",
        "                    knowledge                                or BBB_impermeable\n",
        "```\n",
        "\n",
        "This is called **transfer learning**: leveraging knowledge learned from one task (molecular language modeling) for another task (BBB prediction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install -q transformers torch pandas matplotlib scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Preparing the Data\n",
        "\n",
        "## Load the BBBP Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/racousin/ai_for_sciences/main/day2/data/molecules_bbbp.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(df['label_name'].value_counts())\n",
        "print(f\"\\nClass imbalance ratio: {df['label'].mean():.2%} permeable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Question 1\n",
        "\n",
        "Look at the class distribution:\n",
        "\n",
        "1. Are the classes balanced?\n",
        "2. What would be the accuracy of a model that always predicts \"permeable\"?\n",
        "3. Why is this baseline important to know?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Molecular Embeddings\n",
        "\n",
        "We'll use ChemBERTa to convert SMILES strings to embeddings.\n",
        "\n",
        "**Note**: Computing embeddings for the full dataset takes time. In practice, you'd cache these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load ChemBERTa\n",
        "print(\"Loading ChemBERTa...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
        "model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "print(f\"Embedding dimension: {model.config.hidden_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_embeddings(smiles_list, batch_size=32):\n",
        "    \"\"\"Compute embeddings for a list of SMILES strings.\"\"\"\n",
        "    embeddings = []\n",
        "    \n",
        "    for i in range(0, len(smiles_list), batch_size):\n",
        "        batch = smiles_list[i:i+batch_size]\n",
        "        \n",
        "        # Tokenize\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", \n",
        "                          padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Get embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # Use [CLS] token embedding\n",
        "            batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            embeddings.append(batch_emb)\n",
        "        \n",
        "        if (i // batch_size) % 10 == 0:\n",
        "            print(f\"  Processed {min(i+batch_size, len(smiles_list))}/{len(smiles_list)} molecules\")\n",
        "    \n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# Compute embeddings for all molecules\n",
        "print(\"Computing embeddings...\")\n",
        "X = compute_embeddings(df['SMILES'].tolist())\n",
        "y = df['label'].values\n",
        "\n",
        "print(f\"\\nEmbedding matrix shape: {X.shape}\")\n",
        "print(f\"Labels shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Data into Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data: 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTrain class distribution: {y_train.mean():.2%} permeable\")\n",
        "print(f\"Test class distribution: {y_test.mean():.2%} permeable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Building the Classifier\n",
        "\n",
        "We'll build a simple **Multi-Layer Perceptron (MLP)**:\n",
        "\n",
        "```\n",
        "Input (768) ‚Üí Hidden (256) ‚Üí ReLU ‚Üí Hidden (64) ‚Üí ReLU ‚Üí Output (1) ‚Üí Sigmoid\n",
        "```\n",
        "\n",
        "This is a small network that learns to map embeddings to BBB permeability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BBBClassifier(nn.Module):\n",
        "    \"\"\"Simple MLP classifier for BBB permeability prediction.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 4, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Create model\n",
        "input_dim = X_train.shape[1]  # 768\n",
        "classifier = BBBClassifier(input_dim=input_dim, hidden_dim=256, dropout=0.2)\n",
        "classifier.to(device)\n",
        "\n",
        "# Count parameters\n",
        "n_params = sum(p.numel() for p in classifier.parameters())\n",
        "print(f\"Model architecture:\")\n",
        "print(classifier)\n",
        "print(f\"\\nTotal parameters: {n_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Question 2\n",
        "\n",
        "Look at the model architecture:\n",
        "\n",
        "1. Why do we use ReLU activation after hidden layers but Sigmoid at the output?\n",
        "2. What does Dropout do? Why might it help?\n",
        "3. The model has ~215K parameters. ChemBERTa has ~85M. What does this tell us about transfer learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_t = torch.FloatTensor(X_train)\n",
        "y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "X_test_t = torch.FloatTensor(X_test)\n",
        "y_test_t = torch.FloatTensor(y_test).unsqueeze(1)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64  # <-- You can experiment with this!\n",
        "\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: Training the Classifier\n",
        "\n",
        "The training loop:\n",
        "1. **Forward pass**: Compute predictions\n",
        "2. **Compute loss**: Binary Cross-Entropy\n",
        "3. **Backward pass**: Compute gradients\n",
        "4. **Update weights**: Optimizer step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "learning_rate = 0.001  # <-- You can experiment with this!\n",
        "n_epochs = 50          # <-- You can experiment with this!\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Learning rate: {learning_rate}\")\n",
        "print(f\"  Epochs: {n_epochs}\")\n",
        "print(f\"  Batch size: {batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    \"\"\"Evaluate on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            \n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Store predictions\n",
        "            all_preds.extend((y_pred > 0.5).cpu().numpy().flatten())\n",
        "            all_labels.extend(y_batch.cpu().numpy().flatten())\n",
        "    \n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return total_loss / len(loader), accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "train_losses, test_losses = [], []\n",
        "train_accs, test_accs = [], []\n",
        "\n",
        "print(\"Training...\\n\")\n",
        "print(f\"{'Epoch':>6} {'Train Loss':>12} {'Test Loss':>12} {'Train Acc':>12} {'Test Acc':>12}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Train\n",
        "    train_loss = train_epoch(classifier, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_loss_eval, train_acc = evaluate(classifier, train_loader, criterion, device)\n",
        "    test_loss, test_acc = evaluate(classifier, test_loader, criterion, device)\n",
        "    \n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss_eval)\n",
        "    test_losses.append(test_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    test_accs.append(test_acc)\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"{epoch+1:>6} {train_loss_eval:>12.4f} {test_loss:>12.4f} {train_acc:>12.2%} {test_acc:>12.2%}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(train_losses, label='Train', linewidth=2)\n",
        "axes[0].plot(test_losses, label='Test', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0].set_ylabel('Loss', fontsize=11)\n",
        "axes[0].set_title('Training and Test Loss', fontsize=12, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(train_accs, label='Train', linewidth=2)\n",
        "axes[1].plot(test_accs, label='Test', linewidth=2)\n",
        "axes[1].axhline(y=y_train.mean(), color='gray', linestyle='--', \n",
        "                label=f'Baseline ({y_train.mean():.2%})', alpha=0.7)\n",
        "axes[1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
        "axes[1].set_title('Training and Test Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Question 3\n",
        "\n",
        "Look at the training curves:\n",
        "\n",
        "1. Is the model overfitting? (Hint: Compare train vs test curves)\n",
        "2. Has the model converged?\n",
        "3. Is the model better than the baseline (always predicting \"permeable\")?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 5: Evaluating the Model\n",
        "\n",
        "Accuracy alone doesn't tell the full story. Let's look at more detailed metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get final predictions\n",
        "classifier.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_proba = classifier(X_test_t.to(device)).cpu().numpy().flatten()\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(\"=\" * 55)\n",
        "print(classification_report(y_test, y_pred, \n",
        "                            target_names=['BBB_impermeable', 'BBB_permeable']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "im = ax.imshow(cm, cmap='Blues')\n",
        "\n",
        "# Labels\n",
        "labels = ['Impermeable', 'Permeable']\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "\n",
        "# Add values\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, f'{cm[i,j]}', ha='center', va='center', fontsize=14,\n",
        "               color='white' if cm[i,j] > cm.max()/2 else 'black')\n",
        "\n",
        "ax.set_xlabel('Predicted', fontsize=11)\n",
        "ax.set_ylabel('Actual', fontsize=11)\n",
        "ax.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpret\n",
        "print(f\"\\nInterpretation:\")\n",
        "print(f\"  True Negatives (correctly predicted impermeable): {cm[0,0]}\")\n",
        "print(f\"  False Positives (incorrectly predicted permeable): {cm[0,1]}\")\n",
        "print(f\"  False Negatives (incorrectly predicted impermeable): {cm[1,0]}\")\n",
        "print(f\"  True Positives (correctly predicted permeable): {cm[1,1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Question 4\n",
        "\n",
        "Look at the confusion matrix:\n",
        "\n",
        "1. Which type of error is more common: false positives or false negatives?\n",
        "2. In drug discovery, which error might be more costly?\n",
        "   - False positive: Predict drug enters brain when it doesn't\n",
        "   - False negative: Predict drug doesn't enter brain when it does\n",
        "3. How might you adjust the model to reduce one type of error?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 6: Experiments\n",
        "\n",
        "Now it's your turn to experiment! Try modifying the parameters and see how performance changes.\n",
        "\n",
        "## Exercise 1: Change the Architecture\n",
        "\n",
        "What happens if you make the network deeper or shallower?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Experiment with different architectures\n",
        "# Try changing:\n",
        "# - hidden_dim (64, 128, 256, 512)\n",
        "# - dropout (0.0, 0.1, 0.3, 0.5)\n",
        "\n",
        "# Example: Create a smaller network\n",
        "small_classifier = BBBClassifier(\n",
        "    input_dim=input_dim, \n",
        "    hidden_dim=64,       # <-- Smaller hidden layer\n",
        "    dropout=0.1          # <-- Less dropout\n",
        ")\n",
        "\n",
        "n_params_small = sum(p.numel() for p in small_classifier.parameters())\n",
        "print(f\"Original model parameters: {n_params:,}\")\n",
        "print(f\"Small model parameters: {n_params_small:,}\")\n",
        "print(f\"\\nDoes a smaller model perform worse? Try training it!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Adjust the Decision Threshold\n",
        "\n",
        "By default, we predict \"permeable\" if probability > 0.5.\n",
        "\n",
        "What if we change this threshold?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different thresholds\n",
        "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "\n",
        "print(f\"{'Threshold':>10} {'Accuracy':>12} {'Precision':>12} {'Recall':>12}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for thresh in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba > thresh).astype(int)\n",
        "    acc = accuracy_score(y_test, y_pred_thresh)\n",
        "    \n",
        "    # Compute precision and recall for permeable class\n",
        "    tp = ((y_pred_thresh == 1) & (y_test == 1)).sum()\n",
        "    fp = ((y_pred_thresh == 1) & (y_test == 0)).sum()\n",
        "    fn = ((y_pred_thresh == 0) & (y_test == 1)).sum()\n",
        "    \n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    \n",
        "    print(f\"{thresh:>10.1f} {acc:>12.2%} {precision:>12.2%} {recall:>12.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Question 5\n",
        "\n",
        "Look at how threshold affects metrics:\n",
        "\n",
        "1. What happens to precision as threshold increases?\n",
        "2. What happens to recall as threshold increases?\n",
        "3. Why is there a trade-off between precision and recall?\n",
        "4. What threshold would you choose for a drug discovery application?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Compare to a Baseline Model\n",
        "\n",
        "Let's see how our MLP compares to a simple logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train logistic regression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "lr_pred = lr_model.predict(X_test)\n",
        "lr_acc = accuracy_score(y_test, lr_pred)\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"{'Model':<25} {'Accuracy':>12}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"{'Baseline (always 1)':<25} {y_test.mean():>12.2%}\")\n",
        "print(f\"{'Logistic Regression':<25} {lr_acc:>12.2%}\")\n",
        "print(f\"{'MLP (ours)':<25} {test_accs[-1]:>12.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Summary: Key Takeaways\n",
        "\n",
        "## What We Learned\n",
        "\n",
        "1. **Embeddings as features**: Pre-trained models (ChemBERTa) convert molecules to useful representations\n",
        "\n",
        "2. **Transfer learning**: A small classifier on top of frozen embeddings can be very effective\n",
        "\n",
        "3. **Evaluation matters**: Accuracy alone isn't enough - look at precision, recall, confusion matrix\n",
        "\n",
        "4. **Trade-offs**: Threshold, architecture, and hyperparameters all affect performance\n",
        "\n",
        "## Key Insight\n",
        "\n",
        "> **Pre-trained embeddings dramatically simplify the problem.** Instead of learning chemistry from scratch, we leverage knowledge from models trained on millions of molecules.\n",
        "\n",
        "## Practical Applications\n",
        "\n",
        "This same workflow applies to many scientific domains:\n",
        "\n",
        "| Domain | Embedding Model | Classification Task |\n",
        "|--------|-----------------|---------------------|\n",
        "| Molecules | ChemBERTa | Toxicity, solubility, BBB |\n",
        "| Proteins | ESM-2 | Function prediction, localization |\n",
        "| DNA | DNABERT | Promoter detection, modification |\n",
        "| Text | SciBERT | Topic classification, sentiment |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Reflection Questions\n",
        "\n",
        "1. **For your research**, what classification problems could you solve with embeddings?\n",
        "\n",
        "2. **What pre-trained model** would you use, and what classifier would you build?\n",
        "\n",
        "3. **What evaluation metrics** would matter most for your application?\n",
        "\n",
        "4. **What's the limitation** of this approach? When might you need to fine-tune the embedding model itself?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Bonus: Save Your Model\n",
        "\n",
        "If you trained a good model, you can save it for later use!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "torch.save(classifier.state_dict(), 'bbb_classifier.pt')\n",
        "print(\"Model saved to 'bbb_classifier.pt'\")\n",
        "\n",
        "# To load later:\n",
        "# loaded_model = BBBClassifier(input_dim=768)\n",
        "# loaded_model.load_state_dict(torch.load('bbb_classifier.pt'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
