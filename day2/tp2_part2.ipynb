{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 - Part 2: Domain-Specific Embeddings\n",
    "\n",
    "**Day 2 - AI for Sciences Winter School**\n",
    "\n",
    "**Instructor:** Raphael Cousin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/ai_for_sciences/blob/main/day2/tp2_part2.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this practical, we explore **domain-specific embeddings** across four scientific domains. Each domain has specialized pre-trained models that understand the unique \"language\" of that field.\n",
    "\n",
    "| Domain | Data | Model | What captures \"similarity\"? |\n",
    "|--------|------|-------|----------------------------|\n",
    "| **Text** | Scientific abstracts | SciBERT | Semantic meaning, topic similarity |\n",
    "| **Molecules** | SMILES strings | ChemBERTa | Chemical structure, properties |\n",
    "| **Proteins** | Amino acid sequences | ESM-2 | Structural/functional similarity |\n",
    "| **DNA** | Nucleotide sequences | DNABERT-2 | Functional regions, motifs |\n",
    "\n",
    "**Key Questions**:\n",
    "- How do domain-specific models capture domain-specific similarity?\n",
    "- Can embeddings separate meaningful biological classes?\n",
    "- How do tokenization strategies differ across domains?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -q git+https://github.com/racousin/ai_for_sciences.git\n",
    "!pip install -q transformers torch pandas matplotlib scikit-learn umap-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from aiforscience import (\n",
    "    load_student_projects,\n",
    "    get_transformer_embeddings,\n",
    "    compute_similarity_matrix,\n",
    "    find_top_similar_pairs,\n",
    "    find_most_similar,\n",
    "    plot_similarity_matrix,\n",
    "    plot_embeddings_umap,\n",
    "    print_tokenization_example,\n",
    ")\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths - relative to notebook location\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# For Colab, download data if needed\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(\"Downloading data files...\")\n",
    "    !mkdir -p data\n",
    "    !wget -q -O data/student_project.csv https://raw.githubusercontent.com/racousin/ai_for_sciences/main/day2/data/student_project.csv\n",
    "    !wget -q -O data/molecules_bbbp.csv https://raw.githubusercontent.com/racousin/ai_for_sciences/main/day2/data/molecules_bbbp.csv\n",
    "    !wget -q -O data/proteins_pfam.csv https://raw.githubusercontent.com/racousin/ai_for_sciences/main/day2/data/proteins_pfam.csv\n",
    "    !wget -q -O data/dna_histone.csv https://raw.githubusercontent.com/racousin/ai_for_sciences/main/day2/data/dna_histone.csv\n",
    "    print(\"Data downloaded!\")\n",
    "else:\n",
    "    print(f\"Data directory found: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section A: Scientific Text Embeddings\n",
    "\n",
    "## Context\n",
    "\n",
    "Scientific text presents unique challenges for language models:\n",
    "- **Specialized vocabulary**: Terms like \"apoptosis\", \"eigenvalue\", \"chromatography\"\n",
    "- **Domain-specific meanings**: \"Cell\" means different things in biology vs. computer science\n",
    "- **Complex sentence structures**: Scientific writing style differs from general text\n",
    "\n",
    "**SciBERT** was trained on 1.14M papers from [huggingface](https://www.semanticscholar.org/) covering:\n",
    "- 18% Computer Science\n",
    "- 82% Biomedical papers\n",
    "\n",
    "This domain-specific training helps it understand scientific terminology better than general-purpose models.\n",
    "\n",
    "## The Data: Your Fellow Students' Projects\n",
    "\n",
    "We'll embed the research project descriptions from all students in this summer school and explore which projects are semantically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student projects\n",
    "projects = load_student_projects(os.path.join(DATA_DIR, \"student_project.csv\"))\n",
    "\n",
    "print(f\"Loaded {len(projects)} student projects:\\n\")\n",
    "for i, (author, content) in enumerate(projects.items()):\n",
    "    # Show first 100 chars of each project\n",
    "    print(f\"{i+1}. {author}: {content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: SciBERT\n",
    "\n",
    "**Model**: [`allenai/scibert_scivocab_uncased`](https://huggingface.co/allenai/scibert_scivocab_uncased)\n",
    "\n",
    "- **Architecture**: BERT-base (12 layers, 768 hidden dimensions)\n",
    "- **Vocabulary**: 31k tokens from scientific papers (\"scivocab\")\n",
    "- **Training**: Masked language modeling on scientific papers\n",
    "\n",
    "**Alternative to try**: [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) - a general-purpose sentence embedding model, faster but less specialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load SciBERT\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "text_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "text_model.eval()\n",
    "\n",
    "print(f\"Model loaded: allenai/scibert_scivocab_uncased\")\n",
    "print(f\"Hidden size: {text_model.config.hidden_size}\")\n",
    "print(f\"Number of layers: {text_model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Tokenizer\n",
    "\n",
    "Let's see how SciBERT tokenizes scientific text. Its vocabulary was built from scientific papers, so it handles technical terms efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer statistics\n",
    "print(f\"SciBERT Vocabulary Size: {text_tokenizer.vocab_size:,} tokens\")\n",
    "\n",
    "# Example tokenization of scientific terms\n",
    "scientific_terms = [\n",
    "    \"machine learning for drug discovery\",\n",
    "    \"electroencephalography signal analysis\",\n",
    "    \"convolutional neural network architecture\",\n",
    "    \"protein folding and molecular dynamics\"\n",
    "]\n",
    "\n",
    "print(\"\\nTokenization examples:\")\n",
    "for term in scientific_terms:\n",
    "    tokens = text_tokenizer.tokenize(term)\n",
    "    print(f\"\\n  '{term}'\")\n",
    "    print(f\"  -> {tokens} ({len(tokens)} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Project Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get author names and project descriptions\n",
    "authors = list(projects.keys())\n",
    "descriptions = list(projects.values())\n",
    "\n",
    "# Compute embeddings using mean pooling\n",
    "print(f\"Computing embeddings for {len(authors)} projects...\")\n",
    "text_embeddings = get_transformer_embeddings(\n",
    "    descriptions, \n",
    "    text_tokenizer, \n",
    "    text_model, \n",
    "    max_length=512,\n",
    "    pooling='mean'  # Mean pooling over all tokens\n",
    ")\n",
    "\n",
    "print(f\"Embedding shape: {text_embeddings.shape}\")\n",
    "print(f\"-> {len(authors)} projects, each with {text_embeddings.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Project Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "text_sim = compute_similarity_matrix(text_embeddings)\n",
    "\n",
    "# Plot similarity matrix\n",
    "fig = plot_similarity_matrix(text_sim, authors, title='Student Project Similarity (SciBERT)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar project pairs\n",
    "top_pairs = find_top_similar_pairs(text_sim, authors, top_k=10)\n",
    "\n",
    "print(\"Most similar project pairs:\\n\")\n",
    "for sim, author1, author2 in top_pairs:\n",
    "    print(f\"  {sim:.3f}: {author1} <-> {author2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize in 2D using UMAP\n",
    "fig = plot_embeddings_umap(\n",
    "    text_embeddings, \n",
    "    labels=authors, \n",
    "    title='Student Projects in Embedding Space (UMAP)',\n",
    "    annotate=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Explore Your Own Text\n",
    "\n",
    "Add your own research description, an arXiv abstract, or any scientific text to see where it falls in the embedding space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Paste your own text here!\n",
    "# Examples:\n",
    "# - Your research project abstract\n",
    "# - An arXiv paper abstract (copy-paste from https://arxiv.org/)\n",
    "# - A sentence describing a research topic\n",
    "\n",
    "my_text = \"Your research description or abstract here\"  # <-- Modify this!\n",
    "\n",
    "# Compute embedding for your text\n",
    "my_embedding = get_transformer_embeddings(\n",
    "    [my_text], \n",
    "    text_tokenizer, \n",
    "    text_model,\n",
    "    pooling='mean'\n",
    ")\n",
    "\n",
    "# Find most similar projects\n",
    "results = find_most_similar(my_embedding[0], text_embeddings, authors, top_k=5)\n",
    "\n",
    "print(f\"Your text: {my_text[:80]}...\\n\")\n",
    "print(\"Most similar projects:\")\n",
    "for sim, author in results:\n",
    "    print(f\"  {sim:.3f}: {author}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Token Size and Representation\n",
    "\n",
    "Run the cell below to analyze how different scientific terms are tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenization of domain-specific vs common words\n",
    "test_words = [\n",
    "    (\"DNA\", \"Common scientific term\"),\n",
    "    (\"deoxyribonucleic\", \"Full chemical name\"),\n",
    "    (\"CRISPR\", \"Modern technique\"),\n",
    "    (\"transformer\", \"ML architecture\"),\n",
    "    (\"electroencephalography\", \"Medical term\"),\n",
    "    (\"EEG\", \"Abbreviation\"),\n",
    "]\n",
    "\n",
    "print(\"How SciBERT tokenizes different terms:\\n\")\n",
    "for word, description in test_words:\n",
    "    tokens = text_tokenizer.tokenize(word)\n",
    "    print(f\"  {word:25} ({description:20}) -> {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions to consider:**\n",
    "\n",
    "1. Why does \"DNA\" get tokenized as a single token while \"deoxyribonucleic\" is split?\n",
    "2. What does this tell you about the vocabulary's training data?\n",
    "3. How might tokenization affect the model's understanding of rare scientific terms?\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: Try an Alternative Model\n",
    "\n",
    "Uncomment the code below to compare with `sentence-transformers/all-MiniLM-L6-v2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative: Sentence-BERT (general purpose, faster)\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# \n",
    "# alt_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# alt_embeddings = alt_model.encode(descriptions)\n",
    "# \n",
    "# alt_sim = compute_similarity_matrix(alt_embeddings)\n",
    "# fig = plot_similarity_matrix(alt_sim, authors, title='Project Similarity (MiniLM)')\n",
    "# plt.show()\n",
    "# \n",
    "# # Compare: Do the similarity patterns differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section B: Molecular Embeddings\n",
    "\n",
    "## Context: What is SMILES?\n",
    "\n",
    "**SMILES** (Simplified Molecular Input Line Entry System) is a text notation for representing molecules:\n",
    "\n",
    "```\n",
    "Aspirin:     CC(=O)OC1=CC=CC=C1C(=O)O\n",
    "Caffeine:    CN1C=NC2=C1C(=O)N(C(=O)N2C)C\n",
    "Ethanol:     CCO\n",
    "Benzene:     c1ccccc1\n",
    "```\n",
    "\n",
    "**Key SMILES notation**:\n",
    "- **Letters**: Atoms (C=carbon, N=nitrogen, O=oxygen, etc.)\n",
    "- **Numbers**: Ring closures (1, 2, 3...)\n",
    "- **Parentheses**: Branches\n",
    "- **=, #**: Double and triple bonds\n",
    "- **Lowercase**: Aromatic atoms (c, n, o)\n",
    "\n",
    "**ChemBERTa** is a BERT-style model trained on ~77M molecules from the [ZINC database](https://zinc.docking.org/), learning to understand molecular structure from SMILES strings.\n",
    "\n",
    "## The Data: Blood-Brain Barrier Permeability (BBBP)\n",
    "\n",
    "The **blood-brain barrier (BBB)** is a selective membrane protecting the brain. For drugs targeting the central nervous system, BBB permeability is crucial.\n",
    "\n",
    "- **BBB-permeable**: Drug can cross into the brain\n",
    "- **BBB-impermeable**: Drug cannot reach the brain\n",
    "\n",
    "This dataset contains molecules labeled by their BBB permeability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load molecule dataset\n",
    "mol_df = pd.read_csv(os.path.join(DATA_DIR, \"molecules_bbbp.csv\"))\n",
    "\n",
    "print(f\"Dataset shape: {mol_df.shape}\")\n",
    "print(f\"\\nColumns: {list(mol_df.columns)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(mol_df['label_name'].value_counts())\n",
    "\n",
    "print(f\"\\nExample SMILES:\")\n",
    "for idx in [0, 5, 10]:\n",
    "    row = mol_df.iloc[idx]\n",
    "    print(f\"  {row['SMILES'][:50]:50} -> {row['label_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: ChemBERTa\n",
    "\n",
    "**Model**: [`seyonec/ChemBERTa-zinc-base-v1`](https://huggingface.co/seyonec/ChemBERTa-zinc-base-v1)\n",
    "\n",
    "- **Architecture**: RoBERTa-base (12 layers, 768 hidden dimensions)\n",
    "- **Vocabulary**: ~600 tokens for SMILES characters and substructures\n",
    "- **Training**: Masked language modeling on 77M SMILES from ZINC database\n",
    "\n",
    "**Alternative to try**: [`DeepChem/ChemBERTa-77M-MTR`](https://huggingface.co/DeepChem/ChemBERTa-77M-MTR) - trained with multi-task learning on molecular properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ChemBERTa\n",
    "chem_tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "chem_model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "chem_model.eval()\n",
    "\n",
    "print(f\"Model loaded: seyonec/ChemBERTa-zinc-base-v1\")\n",
    "print(f\"Hidden size: {chem_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding SMILES Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChemBERTa tokenizer statistics\n",
    "print(f\"ChemBERTa Vocabulary Size: {chem_tokenizer.vocab_size:,} tokens\")\n",
    "\n",
    "# Example tokenization\n",
    "example_molecules = [\n",
    "    (\"CCO\", \"Ethanol\"),\n",
    "    (\"c1ccccc1\", \"Benzene\"),\n",
    "    (\"CC(=O)OC1=CC=CC=C1C(=O)O\", \"Aspirin\"),\n",
    "]\n",
    "\n",
    "print(\"\\nSMILES tokenization examples:\")\n",
    "for smiles, name in example_molecules:\n",
    "    tokens = chem_tokenizer.tokenize(smiles)\n",
    "    print(f\"\\n  {name} ({smiles})\")\n",
    "    print(f\"  -> {tokens} ({len(tokens)} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Molecular Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample molecules for visualization\n",
    "n_sample = 100\n",
    "mol_sample = mol_df.sample(n=n_sample, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Computing embeddings for {n_sample} molecules...\")\n",
    "mol_embeddings = get_transformer_embeddings(\n",
    "    mol_sample['SMILES'].tolist(),\n",
    "    chem_tokenizer,\n",
    "    chem_model,\n",
    "    pooling='cls'  # Use [CLS] token for molecules\n",
    ")\n",
    "\n",
    "print(f\"Embedding shape: {mol_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Molecular Embedding Space\n",
    "\n",
    "Do BBB-permeable and BBB-impermeable molecules cluster separately in the embedding space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization with class labels\n",
    "fig = plot_embeddings_umap(\n",
    "    mol_embeddings,\n",
    "    categories=mol_sample['label_name'].tolist(),\n",
    "    title='Molecular Embedding Space (ChemBERTa)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Molecular Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix for a smaller subset (for visualization)\n",
    "n_viz = 20\n",
    "mol_viz = mol_sample.head(n_viz)\n",
    "mol_viz_emb = mol_embeddings[:n_viz]\n",
    "\n",
    "mol_sim = compute_similarity_matrix(mol_viz_emb)\n",
    "\n",
    "# Create labels showing SMILES fragment + class\n",
    "mol_labels = [f\"{s[:15]}... ({l[4:]})\" for s, l in \n",
    "              zip(mol_viz['SMILES'], mol_viz['label_name'])]\n",
    "\n",
    "fig = plot_similarity_matrix(mol_sim, mol_labels, \n",
    "                             title='Molecular Similarity (ChemBERTa)',\n",
    "                             figsize=(12, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Molecular Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar molecules to aspirin\n",
    "aspirin = \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
    "\n",
    "# Get aspirin embedding\n",
    "aspirin_emb = get_transformer_embeddings([aspirin], chem_tokenizer, chem_model, pooling='cls')\n",
    "\n",
    "# Find similarities\n",
    "mol_sim_scores = compute_similarity_matrix(\n",
    "    np.vstack([aspirin_emb, mol_embeddings])\n",
    ")[0, 1:]  # First row, excluding self-similarity\n",
    "\n",
    "print(\"Query: Aspirin\")\n",
    "print(f\"SMILES: {aspirin}\\n\")\n",
    "print(\"Most similar molecules in dataset:\")\n",
    "for idx in mol_sim_scores.argsort()[::-1][:5]:\n",
    "    row = mol_sample.iloc[idx]\n",
    "    print(f\"  {mol_sim_scores[idx]:.3f}: {row['SMILES'][:40]}... ({row['label_name']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Molecular Embeddings\n",
    "\n",
    "Looking at the UMAP visualization and similarity matrix:\n",
    "\n",
    "1. Are BBB-permeable and impermeable molecules clearly separated?\n",
    "2. What does this tell us about whether BBB permeability is related to the molecular structure features that ChemBERTa captures?\n",
    "3. Why might there be overlap between the two classes? (Think about what chemical features influence BBB permeability)\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: Try ChemBERTa-77M-MTR\n",
    "\n",
    "Uncomment to compare with the multi-task trained version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative: ChemBERTa trained with multi-task regression\n",
    "# alt_chem_tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "# alt_chem_model = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "# alt_chem_model.eval()\n",
    "# \n",
    "# alt_mol_embeddings = get_transformer_embeddings(\n",
    "#     mol_sample['SMILES'].tolist(),\n",
    "#     alt_chem_tokenizer,\n",
    "#     alt_chem_model,\n",
    "#     pooling='cls'\n",
    "# )\n",
    "# \n",
    "# fig = plot_embeddings_umap(\n",
    "#     alt_mol_embeddings,\n",
    "#     categories=mol_sample['label_name'].tolist(),\n",
    "#     title='Molecular Embedding Space (ChemBERTa-77M-MTR)'\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section C: Protein Embeddings\n",
    "\n",
    "## Context\n",
    "\n",
    "Proteins are sequences of **amino acids** - 20 building blocks denoted by single letters:\n",
    "\n",
    "```\n",
    "A (Alanine)      G (Glycine)      M (Methionine)    S (Serine)\n",
    "C (Cysteine)     H (Histidine)    N (Asparagine)    T (Threonine)\n",
    "D (Aspartate)    I (Isoleucine)   P (Proline)       V (Valine)\n",
    "E (Glutamate)    K (Lysine)       Q (Glutamine)     W (Tryptophan)\n",
    "F (Phenylalanine) L (Leucine)     R (Arginine)      Y (Tyrosine)\n",
    "```\n",
    "\n",
    "Example protein sequence: `MSKIIEYDETARRAIEAGVNTLADAVRVTLGPRGR...`\n",
    "\n",
    "**ESM-2** (Evolutionary Scale Modeling) from Meta AI was trained on **millions of protein sequences** from UniRef. It learns:\n",
    "- Structural information from sequence alone\n",
    "- Evolutionary relationships between proteins\n",
    "- Functional motifs and domains\n",
    "\n",
    "## The Data: Protein Families (Pfam)\n",
    "\n",
    "Proteins are grouped into **families** based on evolutionary relationships and similar functions. Proteins in the same family often:\n",
    "- Share similar 3D structures\n",
    "- Perform related biological functions\n",
    "- Have common evolutionary ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load protein dataset\n",
    "prot_df = pd.read_csv(os.path.join(DATA_DIR, \"proteins_pfam.csv\"))\n",
    "\n",
    "print(f\"Dataset shape: {prot_df.shape}\")\n",
    "print(f\"\\nProtein families:\")\n",
    "print(prot_df['family_name'].value_counts())\n",
    "\n",
    "print(f\"\\nExample sequence (first 60 amino acids):\")\n",
    "print(f\"  {prot_df.iloc[0]['sequence'][:60]}...\")\n",
    "print(f\"  Family: {prot_df.iloc[0]['family_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: ESM-2\n",
    "\n",
    "**Model**: [`facebook/esm2_t6_8M_UR50D`](https://huggingface.co/facebook/esm2_t6_8M_UR50D)\n",
    "\n",
    "- **Architecture**: Transformer (6 layers, 320 hidden dimensions)\n",
    "- **Vocabulary**: 33 tokens (20 amino acids + special tokens)\n",
    "- **Training**: Masked language modeling on UniRef50 (millions of protein sequences)\n",
    "- **Size**: 8M parameters (smallest ESM-2, good for tutorials)\n",
    "\n",
    "**Alternative to try**: [`Rostlab/prot_bert`](https://huggingface.co/Rostlab/prot_bert) - BERT trained on protein sequences with different tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmTokenizer, EsmModel\n",
    "\n",
    "# Load ESM-2\n",
    "esm_tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "esm_model = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "esm_model.eval()\n",
    "\n",
    "print(f\"Model loaded: facebook/esm2_t6_8M_UR50D\")\n",
    "print(f\"Hidden size: {esm_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Protein Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM tokenizer statistics\n",
    "print(f\"ESM-2 Vocabulary Size: {esm_tokenizer.vocab_size} tokens\")\n",
    "print(f\"\\nVocabulary: {list(esm_tokenizer.get_vocab().keys())[:30]}...\")\n",
    "\n",
    "# Example tokenization\n",
    "example_seq = \"MSKIIEYDETARRAIE\"\n",
    "tokens = esm_tokenizer.tokenize(example_seq)\n",
    "print(f\"\\nExample sequence: {example_seq}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"\\nNote: ESM-2 uses single amino acid tokens (no subword tokenization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Protein Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample proteins (balanced across families)\n",
    "n_per_family = 15\n",
    "prot_sample = prot_df.groupby('family_name').head(n_per_family).reset_index(drop=True)\n",
    "\n",
    "print(f\"Computing embeddings for {len(prot_sample)} proteins...\")\n",
    "\n",
    "# Truncate long sequences for speed\n",
    "sequences = [seq[:200] for seq in prot_sample['sequence'].tolist()]\n",
    "\n",
    "prot_embeddings = get_transformer_embeddings(\n",
    "    sequences,\n",
    "    esm_tokenizer,\n",
    "    esm_model,\n",
    "    max_length=200,\n",
    "    pooling='mean'  # Mean pooling for proteins\n",
    ")\n",
    "\n",
    "print(f\"Embedding shape: {prot_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Protein Families\n",
    "\n",
    "Do proteins from the same family cluster together in the embedding space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization with family labels\n",
    "fig = plot_embeddings_umap(\n",
    "    prot_embeddings,\n",
    "    categories=prot_sample['family_name'].tolist(),\n",
    "    title='Protein Embedding Space (ESM-2)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Family Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix for subset\n",
    "n_viz = 20\n",
    "prot_viz = prot_sample.head(n_viz)\n",
    "prot_viz_emb = prot_embeddings[:n_viz]\n",
    "\n",
    "prot_sim = compute_similarity_matrix(prot_viz_emb)\n",
    "\n",
    "# Create labels showing UniProt ID + family\n",
    "prot_labels = [f\"{uid[:8]}...\" for uid in prot_viz['uniprot_id']]\n",
    "\n",
    "fig = plot_similarity_matrix(prot_sim, prot_labels,\n",
    "                             title='Protein Similarity (ESM-2)',\n",
    "                             figsize=(10, 8))\n",
    "plt.show()\n",
    "\n",
    "# Show family for each protein\n",
    "print(\"\\nProtein families:\")\n",
    "for i, (uid, fam) in enumerate(zip(prot_viz['uniprot_id'], prot_viz['family_name'])):\n",
    "    print(f\"  {i}: {uid[:8]}... -> {fam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Protein Family Clustering\n",
    "\n",
    "Looking at the protein embedding visualization:\n",
    "\n",
    "1. Are proteins from the same family clustered together?\n",
    "2. Which families seem most distinct? Most overlapping?\n",
    "3. What biological properties might the embedding capture that cause this clustering? (Consider: sequence patterns, structural features, functional domains)\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: Try ProtBERT\n",
    "\n",
    "Uncomment to compare with ProtBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative: ProtBERT\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# \n",
    "# prot_bert_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "# prot_bert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "# prot_bert_model.eval()\n",
    "# \n",
    "# # ProtBERT expects space-separated amino acids\n",
    "# sequences_spaced = [' '.join(seq[:200]) for seq in prot_sample['sequence'].tolist()]\n",
    "# \n",
    "# alt_prot_embeddings = get_transformer_embeddings(\n",
    "#     sequences_spaced,\n",
    "#     prot_bert_tokenizer,\n",
    "#     prot_bert_model,\n",
    "#     pooling='mean'\n",
    "# )\n",
    "# \n",
    "# fig = plot_embeddings_umap(\n",
    "#     alt_prot_embeddings,\n",
    "#     categories=prot_sample['family_name'].tolist(),\n",
    "#     title='Protein Embedding Space (ProtBERT)'\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section D: DNA Embeddings\n",
    "\n",
    "## Context\n",
    "\n",
    "DNA sequences use 4 nucleotides: **A** (Adenine), **T** (Thymine), **C** (Cytosine), **G** (Guanine)\n",
    "\n",
    "```\n",
    "Example: ATCGATCGATCG...\n",
    "```\n",
    "\n",
    "**Histone modifications** are chemical changes to histone proteins around which DNA wraps. These modifications affect:\n",
    "- Gene expression (turning genes on/off)\n",
    "- DNA accessibility\n",
    "- Chromatin structure\n",
    "\n",
    "**DNABERT-2** learns representations from DNA sequences that capture:\n",
    "- Sequence motifs and patterns\n",
    "- Functional elements (promoters, enhancers, etc.)\n",
    "- Regulatory signals\n",
    "\n",
    "## The Data: Histone Modification Prediction\n",
    "\n",
    "This dataset contains DNA sequences labeled by whether the surrounding histone (H3) is modified or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DNA dataset\n",
    "dna_df = pd.read_csv(os.path.join(DATA_DIR, \"dna_histone.csv\"))\n",
    "\n",
    "print(f\"Dataset shape: {dna_df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(dna_df['label_name'].value_counts())\n",
    "print(f\"\\nSequence length: {len(dna_df.iloc[0]['sequence'])} bp\")\n",
    "print(f\"\\nExample sequence (first 60 nucleotides):\")\n",
    "print(f\"  {dna_df.iloc[0]['sequence'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: DNABERT-2\n",
    "\n",
    "**Model**: [`zhihan1996/DNABERT-2-117M`](https://huggingface.co/zhihan1996/DNABERT-2-117M)\n",
    "\n",
    "- **Architecture**: BERT with Byte-Pair Encoding for DNA\n",
    "- **Vocabulary**: BPE-based, learns subword patterns from DNA\n",
    "- **Training**: Multi-species genome data\n",
    "- **Innovation**: Unlike earlier DNABERT (k-mer based), DNABERT-2 uses BPE which can learn variable-length motifs\n",
    "\n",
    "**Alternative to try**: [`InstaDeepAI/nucleotide-transformer-500m-human-ref`](https://huggingface.co/InstaDeepAI/nucleotide-transformer-500m-human-ref) - larger model focused on human genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DNABERT-2\n",
    "dna_tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "dna_model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "dna_model.eval()\n",
    "\n",
    "print(f\"Model loaded: zhihan1996/DNABERT-2-117M\")\n",
    "print(f\"Hidden size: {dna_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding DNA Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNABERT-2 tokenizer statistics\n",
    "print(f\"DNABERT-2 Vocabulary Size: {dna_tokenizer.vocab_size:,} tokens\")\n",
    "\n",
    "# Example tokenization\n",
    "example_dna = \"ATCGATCGATCGATCGATCG\"\n",
    "tokens = dna_tokenizer.tokenize(example_dna)\n",
    "print(f\"\\nExample DNA: {example_dna}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Compare with a repetitive vs random sequence\n",
    "repetitive = \"ATATATATAT\"\n",
    "random_seq = \"ACTGCTAGCT\"\n",
    "print(f\"\\nRepetitive ({repetitive}): {dna_tokenizer.tokenize(repetitive)}\")\n",
    "print(f\"Random ({random_seq}): {dna_tokenizer.tokenize(random_seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing DNA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DNA sequences\n",
    "n_sample = 100\n",
    "dna_sample = dna_df.sample(n=n_sample, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Computing embeddings for {n_sample} DNA sequences...\")\n",
    "\n",
    "# Truncate for speed\n",
    "sequences = [seq[:200] for seq in dna_sample['sequence'].tolist()]\n",
    "\n",
    "dna_embeddings = get_transformer_embeddings(\n",
    "    sequences,\n",
    "    dna_tokenizer,\n",
    "    dna_model,\n",
    "    max_length=200,\n",
    "    pooling='mean'\n",
    ")\n",
    "\n",
    "print(f\"Embedding shape: {dna_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing DNA Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization with modification labels\n",
    "fig = plot_embeddings_umap(\n",
    "    dna_embeddings,\n",
    "    categories=dna_sample['label_name'].tolist(),\n",
    "    title='DNA Embedding Space (DNABERT-2)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNA Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix for subset\n",
    "n_viz = 20\n",
    "dna_viz = dna_sample.head(n_viz)\n",
    "dna_viz_emb = dna_embeddings[:n_viz]\n",
    "\n",
    "dna_sim = compute_similarity_matrix(dna_viz_emb)\n",
    "\n",
    "# Create labels showing first nucleotides + class\n",
    "dna_labels = [f\"{seq[:8]}...({l.split('_')[1][:3]})\" \n",
    "              for seq, l in zip(dna_viz['sequence'], dna_viz['label_name'])]\n",
    "\n",
    "fig = plot_similarity_matrix(dna_sim, dna_labels,\n",
    "                             title='DNA Sequence Similarity (DNABERT-2)',\n",
    "                             figsize=(10, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: DNA Embeddings\n",
    "\n",
    "Looking at the DNA embedding visualization:\n",
    "\n",
    "1. Are H3-modified and unmodified sequences separable in the embedding space?\n",
    "2. What does this suggest about the relationship between sequence patterns and histone modification?\n",
    "3. Compare this to the molecular embeddings (Section B) - which shows better class separation? What might explain the difference?\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: Try Nucleotide Transformer\n",
    "\n",
    "Uncomment to try a larger DNA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative: Nucleotide Transformer (larger model)\n",
    "# # Note: This model is larger and may be slower\n",
    "# \n",
    "# alt_dna_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"InstaDeepAI/nucleotide-transformer-500m-human-ref\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# alt_dna_model = AutoModel.from_pretrained(\n",
    "#     \"InstaDeepAI/nucleotide-transformer-500m-human-ref\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# alt_dna_model.eval()\n",
    "# \n",
    "# alt_dna_embeddings = get_transformer_embeddings(\n",
    "#     sequences[:50],  # Fewer samples for speed\n",
    "#     alt_dna_tokenizer,\n",
    "#     alt_dna_model,\n",
    "#     pooling='mean'\n",
    "# )\n",
    "# \n",
    "# fig = plot_embeddings_umap(\n",
    "#     alt_dna_embeddings,\n",
    "#     categories=dna_sample['label_name'].tolist()[:50],\n",
    "#     title='DNA Embedding Space (Nucleotide Transformer)'\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: Key Takeaways\n",
    "\n",
    "> **Pre-trained embeddings are powerful features.** Even without fine-tuning, these models capture meaningful relationships in scientific data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **For your research domain**, which pre-trained model might be most useful? Does one exist for your specific data type?\n",
    "\n",
    "2. **What properties** would you want embeddings to capture in your domain? How would you evaluate if a model captures them?\n",
    "\n",
    "3. **What downstream tasks** could you solve with good embeddings? Consider:\n",
    "   - Classification (categorizing samples)\n",
    "   - Clustering (grouping similar samples)\n",
    "   - Retrieval (finding similar samples to a query)\n",
    "   - Visualization (understanding data structure)\n",
    "\n",
    "4. **Tokenization trade-offs**: How does the tokenization strategy affect what the model can learn? Think about:\n",
    "   - Vocabulary size vs. sequence length\n",
    "   - Domain-specific vs. general vocabulary\n",
    "   - Character-level vs. subword vs. word-level"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
